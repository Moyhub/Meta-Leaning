nohup: 忽略输入
2022-01-11 22:40:28:INFO:Finish setting logger...
2022-01-11 22:40:28:INFO:==> Training/Evaluation parameters are:
2022-01-11 22:40:28:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42'
2022-01-11 22:40:28:INFO:	data_fn=1
2022-01-11 22:40:28:INFO:	datatest_fn=1
2022-01-11 22:40:28:INFO:	filter_kernel_size=1
2022-01-11 22:40:28:INFO:	override_data_cache=False
2022-01-11 22:40:28:INFO:	maxRUL=125
2022-01-11 22:40:28:INFO:	low_ratio=0.1
2022-01-11 22:40:28:INFO:	high_ratio=0.99
2022-01-11 22:40:28:INFO:	aug_ratio=150
2022-01-11 22:40:28:INFO:	noise_amplitude=0.01
2022-01-11 22:40:28:INFO:	modeltype='cnn1d'
2022-01-11 22:40:28:INFO:	max_seq_len=550
2022-01-11 22:40:28:INFO:	d_model=128
2022-01-11 22:40:28:INFO:	p_dropout=0.1
2022-01-11 22:40:28:INFO:	n_head=4
2022-01-11 22:40:28:INFO:	n_layer=2
2022-01-11 22:40:28:INFO:	dim_feedforward=512
2022-01-11 22:40:28:INFO:	e_dropout=0.1
2022-01-11 22:40:28:INFO:	activation='relu'
2022-01-11 22:40:28:INFO:	layer_norm=False
2022-01-11 22:40:28:INFO:	support_size=0
2022-01-11 22:40:28:INFO:	inner_steps=1
2022-01-11 22:40:28:INFO:	lr_inner=0.0001
2022-01-11 22:40:28:INFO:	lr_meta=0.001
2022-01-11 22:40:28:INFO:	n_epochs=12
2022-01-11 22:40:28:INFO:	train_batch_size=20
2022-01-11 22:40:28:INFO:	eval_batch_size=1
2022-01-11 22:40:28:INFO:	lr=0.001
2022-01-11 22:40:28:INFO:	weight_decay=0.01
2022-01-11 22:40:28:INFO:	warmup_ratio=0.0
2022-01-11 22:40:28:INFO:	max_grad_norm=5.0
2022-01-11 22:40:28:INFO:	logging_steps=50
2022-01-11 22:40:28:INFO:	seed=42
2022-01-11 22:40:28:INFO:	gpu_id=0
2022-01-11 22:40:28:INFO:	do_train=True
2022-01-11 22:40:28:INFO:	do_eval=False
2022-01-11 22:40:28:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-11 22:40:28:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-11 22:40:28:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-11 22:40:28:INFO:	device=device(type='cuda'))
2022-01-11 22:40:28:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-11 22:40:28:INFO:==> Read data from data/train_FD001.txt...
2022-01-11 22:40:28:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-11 22:40:28:INFO:==> Min_max normalization...
2022-01-11 22:40:28:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-11 22:40:28:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-11 22:40:28:INFO:==> Read data from data/test_FD001.txt...
2022-01-11 22:40:28:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-11 22:40:28:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-11 22:40:28:INFO:	min_rul: 7, max_rul: 145
2022-01-11 22:40:28:INFO:==> Input length ratio of the [TEST] data:
2022-01-11 22:40:28:INFO:	min_ratio = 0.2067
2022-01-11 22:40:28:INFO:	max_ratio = 0.9667
2022-01-11 22:40:28:INFO:==> Min_max normalization...
2022-01-11 22:40:28:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-11 22:40:28:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-11 22:40:32:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-11 22:40:32:INFO:NumExpr defaulting to 8 threads.
2022-01-11 22:40:32:INFO:=============== Scheme: Normal Learning ===============
2022-01-11 22:40:32:INFO:	Num examples = 15000
2022-01-11 22:40:32:INFO:	Num epochs = 12
2022-01-11 22:40:32:INFO:	Batch size = 20
2022-01-11 22:40:32:INFO:	Total optimization steps = 9000
2022-01-11 22:40:41:INFO:==> Group parameters for optimization...
2022-01-11 22:40:41:INFO:    Parameters to update are:
2022-01-11 22:40:41:INFO:	conv1.0.weight
2022-01-11 22:40:41:INFO:	conv2.0.weight
2022-01-11 22:40:41:INFO:	conv3.0.weight
2022-01-11 22:40:41:INFO:	conv4.0.weight
2022-01-11 22:40:41:INFO:	conv5.0.weight
2022-01-11 22:40:41:INFO:	fc_1.0.weight
2022-01-11 22:40:41:INFO:	fc_1.0.bias
2022-01-11 22:40:41:INFO:	fc_2.weight
2022-01-11 22:40:41:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-11 22:40:43:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0017
2022-01-11 22:41:01:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0245
2022-01-11 22:41:20:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0077
2022-01-11 22:41:39:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0070
2022-01-11 22:41:57:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0062
2022-01-11 22:42:16:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0056
2022-01-11 22:42:34:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0048
2022-01-11 22:42:53:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0039
2022-01-11 22:43:12:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0036
2022-01-11 22:43:30:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0037
2022-01-11 22:43:49:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0035
2022-01-11 22:44:07:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0032
2022-01-11 22:44:26:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0028
2022-01-11 22:44:44:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0030
2022-01-11 22:45:03:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0029
2022-01-11 22:45:23:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 22:45:23:INFO:	Num examples = 100
2022-01-11 22:45:23:INFO:	RMSE = 34.0395
2022-01-11 22:45:25:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 22:45:25:INFO:	Num examples = 100
2022-01-11 22:45:25:INFO:	RMSE = 33.3675
2022-01-11 22:45:25:INFO:==> Minimal valid RMSE!
2022-01-11 22:45:25:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-11 22:45:26:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0030
2022-01-11 22:45:44:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0027
2022-01-11 22:46:03:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0028
2022-01-11 22:46:21:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0022
2022-01-11 22:46:40:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0026
2022-01-11 22:46:58:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0023
2022-01-11 22:47:17:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0025
2022-01-11 22:47:36:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0023
2022-01-11 22:47:54:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0024
2022-01-11 22:48:13:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0021
2022-01-11 22:48:31:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0026
2022-01-11 22:48:50:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0020
2022-01-11 22:49:09:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0020
2022-01-11 22:49:27:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0020
2022-01-11 22:49:46:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0017
2022-01-11 22:50:06:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 22:50:06:INFO:	Num examples = 100
2022-01-11 22:50:06:INFO:	RMSE = 31.1376
2022-01-11 22:50:08:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 22:50:08:INFO:	Num examples = 100
2022-01-11 22:50:08:INFO:	RMSE = 24.7693
2022-01-11 22:50:08:INFO:==> Minimal valid RMSE!
2022-01-11 22:50:08:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-11 22:50:08:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0018
2022-01-11 22:50:27:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0019
2022-01-11 22:50:46:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0017
2022-01-11 22:51:04:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0019
2022-01-11 22:51:23:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0019
2022-01-11 22:51:41:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0017
2022-01-11 22:52:00:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0017
2022-01-11 22:52:19:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0017
2022-01-11 22:52:37:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0015
2022-01-11 22:52:56:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0015
2022-01-11 22:53:14:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0016
2022-01-11 22:53:33:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0016
2022-01-11 22:53:51:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0015
2022-01-11 22:54:10:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0014
2022-01-11 22:54:29:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0014
2022-01-11 22:54:49:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 22:54:49:INFO:	Num examples = 100
2022-01-11 22:54:49:INFO:	RMSE = 30.8675
2022-01-11 22:54:51:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 22:54:51:INFO:	Num examples = 100
2022-01-11 22:54:51:INFO:	RMSE = 24.0344
2022-01-11 22:54:51:INFO:==> Minimal valid RMSE!
2022-01-11 22:54:51:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-11 22:54:52:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0014
2022-01-11 22:55:10:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0013
2022-01-11 22:55:29:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0013
2022-01-11 22:55:47:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0014
2022-01-11 22:56:06:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0012
2022-01-11 22:56:24:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0012
2022-01-11 22:56:43:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0012
2022-01-11 22:57:02:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0012
2022-01-11 22:57:20:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0014
2022-01-11 22:57:39:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0012
2022-01-11 22:57:57:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0012
2022-01-11 22:58:16:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0012
2022-01-11 22:58:34:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0013
2022-01-11 22:58:53:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0011
2022-01-11 22:59:11:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0010
2022-01-11 22:59:32:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 22:59:32:INFO:	Num examples = 100
2022-01-11 22:59:32:INFO:	RMSE = 31.2568
2022-01-11 22:59:34:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 22:59:34:INFO:	Num examples = 100
2022-01-11 22:59:34:INFO:	RMSE = 26.1756
2022-01-11 22:59:34:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0011
2022-01-11 22:59:53:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0010
2022-01-11 23:00:11:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0010
2022-01-11 23:00:30:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0009
2022-01-11 23:00:48:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0011
2022-01-11 23:01:07:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0011
2022-01-11 23:01:26:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0010
2022-01-11 23:01:44:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0010
2022-01-11 23:02:03:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0011
2022-01-11 23:02:21:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0010
2022-01-11 23:02:40:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0010
2022-01-11 23:02:58:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0010
2022-01-11 23:03:17:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0010
2022-01-11 23:03:36:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0011
2022-01-11 23:03:54:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0009
2022-01-11 23:04:14:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:04:14:INFO:	Num examples = 100
2022-01-11 23:04:14:INFO:	RMSE = 29.9128
2022-01-11 23:04:17:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:04:17:INFO:	Num examples = 100
2022-01-11 23:04:17:INFO:	RMSE = 25.4110
2022-01-11 23:04:17:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0009
2022-01-11 23:04:36:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0010
2022-01-11 23:04:54:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0009
2022-01-11 23:05:13:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0008
2022-01-11 23:05:31:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0010
2022-01-11 23:05:50:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0009
2022-01-11 23:06:08:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0009
2022-01-11 23:06:27:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0010
2022-01-11 23:06:46:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0008
2022-01-11 23:07:04:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0008
2022-01-11 23:07:23:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0009
2022-01-11 23:07:41:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0007
2022-01-11 23:08:00:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0008
2022-01-11 23:08:18:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0008
2022-01-11 23:08:37:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0009
2022-01-11 23:08:57:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:08:57:INFO:	Num examples = 100
2022-01-11 23:08:57:INFO:	RMSE = 29.8746
2022-01-11 23:08:59:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:08:59:INFO:	Num examples = 100
2022-01-11 23:08:59:INFO:	RMSE = 24.5785
2022-01-11 23:09:00:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0007
2022-01-11 23:09:18:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0008
2022-01-11 23:09:37:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0007
2022-01-11 23:09:56:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0007
2022-01-11 23:10:14:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0007
2022-01-11 23:10:33:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0007
2022-01-11 23:10:51:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0007
2022-01-11 23:11:10:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0007
2022-01-11 23:11:28:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0006
2022-01-11 23:11:47:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0007
2022-01-11 23:12:06:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0007
2022-01-11 23:12:24:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0007
2022-01-11 23:12:43:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0007
2022-01-11 23:13:01:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0007
2022-01-11 23:13:20:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0006
2022-01-11 23:13:40:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:13:40:INFO:	Num examples = 100
2022-01-11 23:13:40:INFO:	RMSE = 31.0820
2022-01-11 23:13:43:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:13:43:INFO:	Num examples = 100
2022-01-11 23:13:43:INFO:	RMSE = 24.0309
2022-01-11 23:13:43:INFO:==> Minimal valid RMSE!
2022-01-11 23:13:43:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-11 23:13:43:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0007
2022-01-11 23:14:02:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0007
2022-01-11 23:14:20:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0006
2022-01-11 23:14:39:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0006
2022-01-11 23:14:57:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0006
2022-01-11 23:15:16:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0005
2022-01-11 23:15:34:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0006
2022-01-11 23:15:53:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0006
2022-01-11 23:16:12:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0005
2022-01-11 23:16:30:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0006
2022-01-11 23:16:49:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0006
2022-01-11 23:17:07:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0005
2022-01-11 23:17:26:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0005
2022-01-11 23:17:44:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0006
2022-01-11 23:18:03:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0005
2022-01-11 23:18:24:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:18:24:INFO:	Num examples = 100
2022-01-11 23:18:24:INFO:	RMSE = 31.5319
2022-01-11 23:18:26:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:18:26:INFO:	Num examples = 100
2022-01-11 23:18:26:INFO:	RMSE = 24.6488
2022-01-11 23:18:26:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0005
2022-01-11 23:18:45:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0005
2022-01-11 23:19:03:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0005
2022-01-11 23:19:22:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0005
2022-01-11 23:19:40:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0005
2022-01-11 23:19:59:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0005
2022-01-11 23:20:18:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0005
2022-01-11 23:20:36:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0004
2022-01-11 23:20:55:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0004
2022-01-11 23:21:13:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0004
2022-01-11 23:21:32:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0005
2022-01-11 23:21:51:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0004
2022-01-11 23:22:09:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0005
2022-01-11 23:22:28:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0004
2022-01-11 23:22:46:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0004
2022-01-11 23:23:07:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:23:07:INFO:	Num examples = 100
2022-01-11 23:23:07:INFO:	RMSE = 31.8778
2022-01-11 23:23:09:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:23:09:INFO:	Num examples = 100
2022-01-11 23:23:09:INFO:	RMSE = 22.7859
2022-01-11 23:23:09:INFO:==> Minimal valid RMSE!
2022-01-11 23:23:09:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-11 23:23:09:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0004
2022-01-11 23:23:28:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0004
2022-01-11 23:23:46:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0004
2022-01-11 23:24:05:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0004
2022-01-11 23:24:23:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0004
2022-01-11 23:24:42:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0004
2022-01-11 23:25:00:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0004
2022-01-11 23:25:19:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0004
2022-01-11 23:25:38:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0004
2022-01-11 23:25:56:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0004
2022-01-11 23:26:15:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0004
2022-01-11 23:26:33:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0004
2022-01-11 23:26:52:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0004
2022-01-11 23:27:10:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0004
2022-01-11 23:27:29:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0003
2022-01-11 23:27:49:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:27:49:INFO:	Num examples = 100
2022-01-11 23:27:49:INFO:	RMSE = 31.8400
2022-01-11 23:27:52:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:27:52:INFO:	Num examples = 100
2022-01-11 23:27:52:INFO:	RMSE = 25.1788
2022-01-11 23:27:52:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0003
2022-01-11 23:28:10:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0003
2022-01-11 23:28:29:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0003
2022-01-11 23:28:48:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0003
2022-01-11 23:29:06:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0003
2022-01-11 23:29:25:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0003
2022-01-11 23:29:43:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0003
2022-01-11 23:30:02:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0003
2022-01-11 23:30:21:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0003
2022-01-11 23:30:39:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0003
2022-01-11 23:30:58:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0003
2022-01-11 23:31:16:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0003
2022-01-11 23:31:35:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0003
2022-01-11 23:31:54:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0004
2022-01-11 23:32:12:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0003
2022-01-11 23:32:32:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:32:32:INFO:	Num examples = 100
2022-01-11 23:32:32:INFO:	RMSE = 31.7549
2022-01-11 23:32:35:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:32:35:INFO:	Num examples = 100
2022-01-11 23:32:35:INFO:	RMSE = 23.6598
2022-01-11 23:32:35:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0003
2022-01-11 23:32:53:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0003
2022-01-11 23:33:12:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0003
2022-01-11 23:33:31:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0003
2022-01-11 23:33:49:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0003
2022-01-11 23:34:08:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0003
2022-01-11 23:34:26:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0003
2022-01-11 23:34:45:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0003
2022-01-11 23:35:03:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0003
2022-01-11 23:35:22:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0003
2022-01-11 23:35:40:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0003
2022-01-11 23:35:59:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0003
2022-01-11 23:36:17:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0003
2022-01-11 23:36:36:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0003
2022-01-11 23:36:54:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0003
2022-01-11 23:37:15:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:37:15:INFO:	Num examples = 100
2022-01-11 23:37:15:INFO:	RMSE = 32.2523
2022-01-11 23:37:17:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:37:17:INFO:	Num examples = 100
2022-01-11 23:37:17:INFO:	RMSE = 23.6336
2022-01-11 23:37:17:INFO:	Output TEST RMSE:	31.8778
2022-01-11 23:37:17:INFO:	VALID RMSEs:	33.3675	24.7693	24.0344	26.1756	25.4110	24.5785	24.0309	24.6488	22.7859	25.1788	23.6598	23.6336
2022-01-11 23:37:17:INFO:	TEST RMSEs:	34.0395	31.1376	30.8675	31.2568	29.9128	29.8746	31.0820	31.5319	31.8778	31.8400	31.7549	32.2523
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-11 23:37:20:INFO:Finish setting logger...
2022-01-11 23:37:20:INFO:==> Training/Evaluation parameters are:
2022-01-11 23:37:20:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667'
2022-01-11 23:37:20:INFO:	data_fn=1
2022-01-11 23:37:20:INFO:	datatest_fn=1
2022-01-11 23:37:20:INFO:	filter_kernel_size=1
2022-01-11 23:37:20:INFO:	override_data_cache=False
2022-01-11 23:37:20:INFO:	maxRUL=125
2022-01-11 23:37:20:INFO:	low_ratio=0.1
2022-01-11 23:37:20:INFO:	high_ratio=0.99
2022-01-11 23:37:20:INFO:	aug_ratio=150
2022-01-11 23:37:20:INFO:	noise_amplitude=0.01
2022-01-11 23:37:20:INFO:	modeltype='cnn1d'
2022-01-11 23:37:20:INFO:	max_seq_len=550
2022-01-11 23:37:20:INFO:	d_model=128
2022-01-11 23:37:20:INFO:	p_dropout=0.1
2022-01-11 23:37:20:INFO:	n_head=4
2022-01-11 23:37:20:INFO:	n_layer=2
2022-01-11 23:37:20:INFO:	dim_feedforward=512
2022-01-11 23:37:20:INFO:	e_dropout=0.1
2022-01-11 23:37:20:INFO:	activation='relu'
2022-01-11 23:37:20:INFO:	layer_norm=False
2022-01-11 23:37:20:INFO:	support_size=0
2022-01-11 23:37:20:INFO:	inner_steps=1
2022-01-11 23:37:20:INFO:	lr_inner=0.0001
2022-01-11 23:37:20:INFO:	lr_meta=0.001
2022-01-11 23:37:20:INFO:	n_epochs=12
2022-01-11 23:37:20:INFO:	train_batch_size=20
2022-01-11 23:37:20:INFO:	eval_batch_size=1
2022-01-11 23:37:20:INFO:	lr=0.001
2022-01-11 23:37:20:INFO:	weight_decay=0.01
2022-01-11 23:37:20:INFO:	warmup_ratio=0.0
2022-01-11 23:37:20:INFO:	max_grad_norm=5.0
2022-01-11 23:37:20:INFO:	logging_steps=50
2022-01-11 23:37:20:INFO:	seed=667
2022-01-11 23:37:20:INFO:	gpu_id=0
2022-01-11 23:37:20:INFO:	do_train=True
2022-01-11 23:37:20:INFO:	do_eval=False
2022-01-11 23:37:20:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-11 23:37:20:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-11 23:37:20:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-11 23:37:20:INFO:	device=device(type='cuda'))
2022-01-11 23:37:20:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-11 23:37:20:INFO:==> Read data from data/train_FD001.txt...
2022-01-11 23:37:20:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-11 23:37:20:INFO:==> Min_max normalization...
2022-01-11 23:37:20:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-11 23:37:20:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-11 23:37:20:INFO:==> Read data from data/test_FD001.txt...
2022-01-11 23:37:20:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-11 23:37:20:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-11 23:37:20:INFO:	min_rul: 7, max_rul: 145
2022-01-11 23:37:20:INFO:==> Input length ratio of the [TEST] data:
2022-01-11 23:37:20:INFO:	min_ratio = 0.2067
2022-01-11 23:37:20:INFO:	max_ratio = 0.9667
2022-01-11 23:37:20:INFO:==> Min_max normalization...
2022-01-11 23:37:20:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-11 23:37:20:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-11 23:37:24:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-11 23:37:24:INFO:NumExpr defaulting to 8 threads.
2022-01-11 23:37:24:INFO:=============== Scheme: Normal Learning ===============
2022-01-11 23:37:24:INFO:	Num examples = 15000
2022-01-11 23:37:24:INFO:	Num epochs = 12
2022-01-11 23:37:24:INFO:	Batch size = 20
2022-01-11 23:37:24:INFO:	Total optimization steps = 9000
2022-01-11 23:37:32:INFO:==> Group parameters for optimization...
2022-01-11 23:37:32:INFO:    Parameters to update are:
2022-01-11 23:37:32:INFO:	conv1.0.weight
2022-01-11 23:37:32:INFO:	conv2.0.weight
2022-01-11 23:37:32:INFO:	conv3.0.weight
2022-01-11 23:37:32:INFO:	conv4.0.weight
2022-01-11 23:37:32:INFO:	conv5.0.weight
2022-01-11 23:37:32:INFO:	fc_1.0.weight
2022-01-11 23:37:32:INFO:	fc_1.0.bias
2022-01-11 23:37:32:INFO:	fc_2.weight
2022-01-11 23:37:32:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-11 23:37:35:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0011
2022-01-11 23:37:53:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0172
2022-01-11 23:38:12:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0073
2022-01-11 23:38:30:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0064
2022-01-11 23:38:49:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0053
2022-01-11 23:39:07:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0049
2022-01-11 23:39:26:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0036
2022-01-11 23:39:44:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0036
2022-01-11 23:40:02:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0031
2022-01-11 23:40:21:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0029
2022-01-11 23:40:40:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0029
2022-01-11 23:40:58:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0029
2022-01-11 23:41:17:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0031
2022-01-11 23:41:35:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0032
2022-01-11 23:41:54:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0026
2022-01-11 23:42:14:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:42:14:INFO:	Num examples = 100
2022-01-11 23:42:14:INFO:	RMSE = 32.1515
2022-01-11 23:42:16:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:42:16:INFO:	Num examples = 100
2022-01-11 23:42:16:INFO:	RMSE = 34.3315
2022-01-11 23:42:16:INFO:==> Minimal valid RMSE!
2022-01-11 23:42:16:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-11 23:42:16:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0026
2022-01-11 23:42:35:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0024
2022-01-11 23:42:54:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0023
2022-01-11 23:43:12:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0024
2022-01-11 23:43:31:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0023
2022-01-11 23:43:49:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0022
2022-01-11 23:44:08:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0021
2022-01-11 23:44:26:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0025
2022-01-11 23:44:45:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0022
2022-01-11 23:45:03:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0030
2022-01-11 23:45:22:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0024
2022-01-11 23:45:40:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0021
2022-01-11 23:45:59:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0026
2022-01-11 23:46:18:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0019
2022-01-11 23:46:36:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0021
2022-01-11 23:46:56:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:46:56:INFO:	Num examples = 100
2022-01-11 23:46:56:INFO:	RMSE = 32.9730
2022-01-11 23:46:58:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:46:58:INFO:	Num examples = 100
2022-01-11 23:46:58:INFO:	RMSE = 26.4313
2022-01-11 23:46:58:INFO:==> Minimal valid RMSE!
2022-01-11 23:46:58:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-11 23:46:59:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0019
2022-01-11 23:47:17:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0023
2022-01-11 23:47:36:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0021
2022-01-11 23:47:55:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0021
2022-01-11 23:48:13:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0022
2022-01-11 23:48:32:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0020
2022-01-11 23:48:50:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0018
2022-01-11 23:49:09:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0020
2022-01-11 23:49:27:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0019
2022-01-11 23:49:46:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0019
2022-01-11 23:50:04:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0021
2022-01-11 23:50:23:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0019
2022-01-11 23:50:41:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0017
2022-01-11 23:51:00:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0022
2022-01-11 23:51:18:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0019
2022-01-11 23:51:39:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:51:39:INFO:	Num examples = 100
2022-01-11 23:51:39:INFO:	RMSE = 33.2890
2022-01-11 23:51:41:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:51:41:INFO:	Num examples = 100
2022-01-11 23:51:41:INFO:	RMSE = 30.8470
2022-01-11 23:51:41:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0020
2022-01-11 23:52:00:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0018
2022-01-11 23:52:18:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0017
2022-01-11 23:52:37:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0016
2022-01-11 23:52:56:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0016
2022-01-11 23:53:14:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0017
2022-01-11 23:53:33:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0018
2022-01-11 23:53:51:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0017
2022-01-11 23:54:09:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0016
2022-01-11 23:54:28:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0015
2022-01-11 23:54:46:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0016
2022-01-11 23:55:05:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0016
2022-01-11 23:55:24:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0015
2022-01-11 23:55:42:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0016
2022-01-11 23:56:01:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0014
2022-01-11 23:56:21:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-11 23:56:21:INFO:	Num examples = 100
2022-01-11 23:56:21:INFO:	RMSE = 32.5811
2022-01-11 23:56:23:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-11 23:56:23:INFO:	Num examples = 100
2022-01-11 23:56:23:INFO:	RMSE = 25.0415
2022-01-11 23:56:23:INFO:==> Minimal valid RMSE!
2022-01-11 23:56:23:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-11 23:56:24:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0015
2022-01-11 23:56:42:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0014
2022-01-11 23:57:01:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0013
2022-01-11 23:57:19:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0014
2022-01-11 23:57:38:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0013
2022-01-11 23:57:56:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0013
2022-01-11 23:58:15:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0013
2022-01-11 23:58:33:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0014
2022-01-11 23:58:52:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0013
2022-01-11 23:59:10:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0013
2022-01-11 23:59:29:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0013
2022-01-11 23:59:48:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0012
2022-01-12 00:00:06:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0012
2022-01-12 00:00:25:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0011
2022-01-12 00:00:43:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0011
2022-01-12 00:01:04:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:01:04:INFO:	Num examples = 100
2022-01-12 00:01:04:INFO:	RMSE = 31.2504
2022-01-12 00:01:06:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:01:06:INFO:	Num examples = 100
2022-01-12 00:01:06:INFO:	RMSE = 26.1662
2022-01-12 00:01:06:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0013
2022-01-12 00:01:25:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0011
2022-01-12 00:01:43:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0011
2022-01-12 00:02:02:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0011
2022-01-12 00:02:20:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0012
2022-01-12 00:02:39:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0011
2022-01-12 00:02:58:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0011
2022-01-12 00:03:16:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0011
2022-01-12 00:03:35:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0011
2022-01-12 00:03:53:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0010
2022-01-12 00:04:12:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0013
2022-01-12 00:04:30:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0011
2022-01-12 00:04:49:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0010
2022-01-12 00:05:07:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0011
2022-01-12 00:05:25:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0010
2022-01-12 00:05:46:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:05:46:INFO:	Num examples = 100
2022-01-12 00:05:46:INFO:	RMSE = 31.3420
2022-01-12 00:05:48:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:05:48:INFO:	Num examples = 100
2022-01-12 00:05:48:INFO:	RMSE = 27.6826
2022-01-12 00:05:48:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0010
2022-01-12 00:06:07:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0010
2022-01-12 00:06:25:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0010
2022-01-12 00:06:44:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0009
2022-01-12 00:07:02:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0010
2022-01-12 00:07:21:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0010
2022-01-12 00:07:40:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0010
2022-01-12 00:07:58:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0009
2022-01-12 00:08:17:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0010
2022-01-12 00:08:35:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0010
2022-01-12 00:08:54:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0009
2022-01-12 00:09:12:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0009
2022-01-12 00:09:31:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0009
2022-01-12 00:09:49:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0008
2022-01-12 00:10:08:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0009
2022-01-12 00:10:28:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:10:28:INFO:	Num examples = 100
2022-01-12 00:10:28:INFO:	RMSE = 31.3335
2022-01-12 00:10:30:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:10:30:INFO:	Num examples = 100
2022-01-12 00:10:30:INFO:	RMSE = 29.1480
2022-01-12 00:10:31:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0009
2022-01-12 00:10:49:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0009
2022-01-12 00:11:08:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0009
2022-01-12 00:11:26:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0009
2022-01-12 00:11:45:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0009
2022-01-12 00:12:04:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0009
2022-01-12 00:12:22:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0008
2022-01-12 00:12:41:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0009
2022-01-12 00:12:59:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0008
2022-01-12 00:13:18:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0009
2022-01-12 00:13:36:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0009
2022-01-12 00:13:55:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0009
2022-01-12 00:14:13:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0009
2022-01-12 00:14:32:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0009
2022-01-12 00:14:50:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0009
2022-01-12 00:15:11:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:15:11:INFO:	Num examples = 100
2022-01-12 00:15:11:INFO:	RMSE = 30.7786
2022-01-12 00:15:13:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:15:13:INFO:	Num examples = 100
2022-01-12 00:15:13:INFO:	RMSE = 28.5608
2022-01-12 00:15:13:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0008
2022-01-12 00:15:32:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0008
2022-01-12 00:15:50:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0008
2022-01-12 00:16:09:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0008
2022-01-12 00:16:28:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0008
2022-01-12 00:16:46:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0008
2022-01-12 00:17:05:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0008
2022-01-12 00:17:23:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0007
2022-01-12 00:17:42:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0007
2022-01-12 00:18:00:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0008
2022-01-12 00:18:19:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0008
2022-01-12 00:18:38:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0008
2022-01-12 00:18:56:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0007
2022-01-12 00:19:15:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0008
2022-01-12 00:19:33:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0007
2022-01-12 00:19:53:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:19:53:INFO:	Num examples = 100
2022-01-12 00:19:53:INFO:	RMSE = 32.4080
2022-01-12 00:19:56:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:19:56:INFO:	Num examples = 100
2022-01-12 00:19:56:INFO:	RMSE = 28.6378
2022-01-12 00:19:56:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0008
2022-01-12 00:20:15:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0007
2022-01-12 00:20:33:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0007
2022-01-12 00:20:52:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0006
2022-01-12 00:21:10:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0007
2022-01-12 00:21:29:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0008
2022-01-12 00:21:47:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0007
2022-01-12 00:22:06:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0007
2022-01-12 00:22:24:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0007
2022-01-12 00:22:43:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0008
2022-01-12 00:23:01:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0006
2022-01-12 00:23:20:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0008
2022-01-12 00:23:39:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0007
2022-01-12 00:23:57:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0007
2022-01-12 00:24:16:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0007
2022-01-12 00:24:36:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:24:36:INFO:	Num examples = 100
2022-01-12 00:24:36:INFO:	RMSE = 32.0662
2022-01-12 00:24:38:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:24:38:INFO:	Num examples = 100
2022-01-12 00:24:38:INFO:	RMSE = 27.9830
2022-01-12 00:24:39:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0007
2022-01-12 00:24:57:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0007
2022-01-12 00:25:16:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0007
2022-01-12 00:25:34:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0007
2022-01-12 00:25:53:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0007
2022-01-12 00:26:11:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0007
2022-01-12 00:26:30:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0007
2022-01-12 00:26:48:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0006
2022-01-12 00:27:07:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0006
2022-01-12 00:27:25:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0007
2022-01-12 00:27:44:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0007
2022-01-12 00:28:03:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0006
2022-01-12 00:28:21:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0007
2022-01-12 00:28:40:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0006
2022-01-12 00:28:58:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0007
2022-01-12 00:29:19:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:29:19:INFO:	Num examples = 100
2022-01-12 00:29:19:INFO:	RMSE = 31.7852
2022-01-12 00:29:21:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:29:21:INFO:	Num examples = 100
2022-01-12 00:29:21:INFO:	RMSE = 28.4003
2022-01-12 00:29:21:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0006
2022-01-12 00:29:40:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0007
2022-01-12 00:29:59:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0007
2022-01-12 00:30:17:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0007
2022-01-12 00:30:36:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0006
2022-01-12 00:30:54:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0007
2022-01-12 00:31:12:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0006
2022-01-12 00:31:31:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0007
2022-01-12 00:31:50:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0006
2022-01-12 00:32:08:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0006
2022-01-12 00:32:27:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0006
2022-01-12 00:32:45:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0006
2022-01-12 00:33:04:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0006
2022-01-12 00:33:22:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0006
2022-01-12 00:33:41:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0006
2022-01-12 00:34:01:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:34:01:INFO:	Num examples = 100
2022-01-12 00:34:01:INFO:	RMSE = 31.6681
2022-01-12 00:34:03:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:34:03:INFO:	Num examples = 100
2022-01-12 00:34:03:INFO:	RMSE = 28.1340
2022-01-12 00:34:03:INFO:	Output TEST RMSE:	32.5811
2022-01-12 00:34:03:INFO:	VALID RMSEs:	34.3315	26.4313	30.8470	25.0415	26.1662	27.6826	29.1480	28.5608	28.6378	27.9830	28.4003	28.1340
2022-01-12 00:34:03:INFO:	TEST RMSEs:	32.1515	32.9730	33.2890	32.5811	31.2504	31.3420	31.3335	30.7786	32.4080	32.0662	31.7852	31.6681
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-12 00:34:06:INFO:Finish setting logger...
2022-01-12 00:34:06:INFO:==> Training/Evaluation parameters are:
2022-01-12 00:34:06:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128'
2022-01-12 00:34:06:INFO:	data_fn=1
2022-01-12 00:34:06:INFO:	datatest_fn=1
2022-01-12 00:34:06:INFO:	filter_kernel_size=1
2022-01-12 00:34:06:INFO:	override_data_cache=False
2022-01-12 00:34:06:INFO:	maxRUL=125
2022-01-12 00:34:06:INFO:	low_ratio=0.1
2022-01-12 00:34:06:INFO:	high_ratio=0.99
2022-01-12 00:34:06:INFO:	aug_ratio=150
2022-01-12 00:34:06:INFO:	noise_amplitude=0.01
2022-01-12 00:34:06:INFO:	modeltype='cnn1d'
2022-01-12 00:34:06:INFO:	max_seq_len=550
2022-01-12 00:34:06:INFO:	d_model=128
2022-01-12 00:34:06:INFO:	p_dropout=0.1
2022-01-12 00:34:06:INFO:	n_head=4
2022-01-12 00:34:06:INFO:	n_layer=2
2022-01-12 00:34:06:INFO:	dim_feedforward=512
2022-01-12 00:34:06:INFO:	e_dropout=0.1
2022-01-12 00:34:06:INFO:	activation='relu'
2022-01-12 00:34:06:INFO:	layer_norm=False
2022-01-12 00:34:06:INFO:	support_size=0
2022-01-12 00:34:06:INFO:	inner_steps=1
2022-01-12 00:34:06:INFO:	lr_inner=0.0001
2022-01-12 00:34:06:INFO:	lr_meta=0.001
2022-01-12 00:34:06:INFO:	n_epochs=12
2022-01-12 00:34:06:INFO:	train_batch_size=20
2022-01-12 00:34:06:INFO:	eval_batch_size=1
2022-01-12 00:34:06:INFO:	lr=0.001
2022-01-12 00:34:06:INFO:	weight_decay=0.01
2022-01-12 00:34:06:INFO:	warmup_ratio=0.0
2022-01-12 00:34:06:INFO:	max_grad_norm=5.0
2022-01-12 00:34:06:INFO:	logging_steps=50
2022-01-12 00:34:06:INFO:	seed=128
2022-01-12 00:34:06:INFO:	gpu_id=0
2022-01-12 00:34:06:INFO:	do_train=True
2022-01-12 00:34:06:INFO:	do_eval=False
2022-01-12 00:34:06:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-12 00:34:06:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-12 00:34:06:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-12 00:34:06:INFO:	device=device(type='cuda'))
2022-01-12 00:34:06:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-12 00:34:06:INFO:==> Read data from data/train_FD001.txt...
2022-01-12 00:34:06:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 00:34:06:INFO:==> Min_max normalization...
2022-01-12 00:34:07:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 00:34:07:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 00:34:07:INFO:==> Read data from data/test_FD001.txt...
2022-01-12 00:34:07:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 00:34:07:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-12 00:34:07:INFO:	min_rul: 7, max_rul: 145
2022-01-12 00:34:07:INFO:==> Input length ratio of the [TEST] data:
2022-01-12 00:34:07:INFO:	min_ratio = 0.2067
2022-01-12 00:34:07:INFO:	max_ratio = 0.9667
2022-01-12 00:34:07:INFO:==> Min_max normalization...
2022-01-12 00:34:07:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 00:34:07:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 00:34:10:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-12 00:34:10:INFO:NumExpr defaulting to 8 threads.
2022-01-12 00:34:10:INFO:=============== Scheme: Normal Learning ===============
2022-01-12 00:34:10:INFO:	Num examples = 15000
2022-01-12 00:34:10:INFO:	Num epochs = 12
2022-01-12 00:34:10:INFO:	Batch size = 20
2022-01-12 00:34:10:INFO:	Total optimization steps = 9000
2022-01-12 00:34:19:INFO:==> Group parameters for optimization...
2022-01-12 00:34:19:INFO:    Parameters to update are:
2022-01-12 00:34:19:INFO:	conv1.0.weight
2022-01-12 00:34:19:INFO:	conv2.0.weight
2022-01-12 00:34:19:INFO:	conv3.0.weight
2022-01-12 00:34:19:INFO:	conv4.0.weight
2022-01-12 00:34:19:INFO:	conv5.0.weight
2022-01-12 00:34:19:INFO:	fc_1.0.weight
2022-01-12 00:34:19:INFO:	fc_1.0.bias
2022-01-12 00:34:19:INFO:	fc_2.weight
2022-01-12 00:34:19:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-12 00:34:21:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0013
2022-01-12 00:34:39:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0252
2022-01-12 00:34:58:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0093
2022-01-12 00:35:16:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0050
2022-01-12 00:35:35:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0038
2022-01-12 00:35:53:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0040
2022-01-12 00:36:12:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0033
2022-01-12 00:36:31:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0032
2022-01-12 00:36:49:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0027
2022-01-12 00:37:08:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0027
2022-01-12 00:37:26:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0024
2022-01-12 00:37:45:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0025
2022-01-12 00:38:03:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0024
2022-01-12 00:38:22:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0026
2022-01-12 00:38:40:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0027
2022-01-12 00:39:01:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:39:01:INFO:	Num examples = 100
2022-01-12 00:39:01:INFO:	RMSE = 31.9210
2022-01-12 00:39:03:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:39:03:INFO:	Num examples = 100
2022-01-12 00:39:03:INFO:	RMSE = 30.7162
2022-01-12 00:39:03:INFO:==> Minimal valid RMSE!
2022-01-12 00:39:03:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-12 00:39:03:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0026
2022-01-12 00:39:22:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0026
2022-01-12 00:39:40:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0023
2022-01-12 00:39:59:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0023
2022-01-12 00:40:17:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0022
2022-01-12 00:40:36:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0024
2022-01-12 00:40:54:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0025
2022-01-12 00:41:13:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0023
2022-01-12 00:41:32:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0021
2022-01-12 00:41:50:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0020
2022-01-12 00:42:09:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0021
2022-01-12 00:42:27:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0021
2022-01-12 00:42:46:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0020
2022-01-12 00:43:04:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0020
2022-01-12 00:43:23:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0021
2022-01-12 00:43:43:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:43:43:INFO:	Num examples = 100
2022-01-12 00:43:43:INFO:	RMSE = 32.6403
2022-01-12 00:43:45:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:43:45:INFO:	Num examples = 100
2022-01-12 00:43:45:INFO:	RMSE = 31.2958
2022-01-12 00:43:46:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0020
2022-01-12 00:44:04:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0023
2022-01-12 00:44:23:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0020
2022-01-12 00:44:41:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0021
2022-01-12 00:45:00:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0019
2022-01-12 00:45:18:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0018
2022-01-12 00:45:37:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0020
2022-01-12 00:45:55:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0026
2022-01-12 00:46:14:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0019
2022-01-12 00:46:33:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0018
2022-01-12 00:46:51:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0019
2022-01-12 00:47:10:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0019
2022-01-12 00:47:28:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0017
2022-01-12 00:47:47:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0019
2022-01-12 00:48:05:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0017
2022-01-12 00:48:26:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:48:26:INFO:	Num examples = 100
2022-01-12 00:48:26:INFO:	RMSE = 30.7177
2022-01-12 00:48:28:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:48:28:INFO:	Num examples = 100
2022-01-12 00:48:28:INFO:	RMSE = 33.2470
2022-01-12 00:48:28:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0016
2022-01-12 00:48:47:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0020
2022-01-12 00:49:06:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0015
2022-01-12 00:49:24:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0016
2022-01-12 00:49:43:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0017
2022-01-12 00:50:01:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0016
2022-01-12 00:50:20:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0018
2022-01-12 00:50:38:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0016
2022-01-12 00:50:57:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0016
2022-01-12 00:51:15:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0017
2022-01-12 00:51:34:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0015
2022-01-12 00:51:52:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0014
2022-01-12 00:52:11:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0015
2022-01-12 00:52:29:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0013
2022-01-12 00:52:48:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0015
2022-01-12 00:53:08:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:53:08:INFO:	Num examples = 100
2022-01-12 00:53:08:INFO:	RMSE = 30.9322
2022-01-12 00:53:10:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:53:10:INFO:	Num examples = 100
2022-01-12 00:53:10:INFO:	RMSE = 31.8127
2022-01-12 00:53:11:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0014
2022-01-12 00:53:29:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0013
2022-01-12 00:53:48:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0013
2022-01-12 00:54:06:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0012
2022-01-12 00:54:25:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0012
2022-01-12 00:54:43:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0012
2022-01-12 00:55:02:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0013
2022-01-12 00:55:20:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0013
2022-01-12 00:55:39:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0014
2022-01-12 00:55:58:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0012
2022-01-12 00:56:16:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0012
2022-01-12 00:56:35:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0011
2022-01-12 00:56:53:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0012
2022-01-12 00:57:12:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0011
2022-01-12 00:57:30:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0012
2022-01-12 00:57:51:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 00:57:51:INFO:	Num examples = 100
2022-01-12 00:57:51:INFO:	RMSE = 31.8077
2022-01-12 00:57:53:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 00:57:53:INFO:	Num examples = 100
2022-01-12 00:57:53:INFO:	RMSE = 31.4109
2022-01-12 00:57:53:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0011
2022-01-12 00:58:12:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0011
2022-01-12 00:58:30:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0010
2022-01-12 00:58:49:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0011
2022-01-12 00:59:07:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0010
2022-01-12 00:59:26:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0011
2022-01-12 00:59:44:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0011
2022-01-12 01:00:03:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0009
2022-01-12 01:00:21:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0009
2022-01-12 01:00:40:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0010
2022-01-12 01:00:58:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0010
2022-01-12 01:01:17:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0010
2022-01-12 01:01:35:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0011
2022-01-12 01:01:54:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0009
2022-01-12 01:02:13:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0010
2022-01-12 01:02:33:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 01:02:33:INFO:	Num examples = 100
2022-01-12 01:02:33:INFO:	RMSE = 30.7844
2022-01-12 01:02:35:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 01:02:35:INFO:	Num examples = 100
2022-01-12 01:02:35:INFO:	RMSE = 28.7352
2022-01-12 01:02:35:INFO:==> Minimal valid RMSE!
2022-01-12 01:02:35:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-0_innerSteps-1_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-12 01:02:35:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0010
2022-01-12 01:02:54:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0009
2022-01-12 01:03:12:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0009
2022-01-12 01:03:31:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0010
2022-01-12 01:03:50:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0009
2022-01-12 01:04:08:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0009
2022-01-12 01:04:27:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0009
2022-01-12 01:04:45:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0009
2022-01-12 01:05:04:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0010
2022-01-12 01:05:22:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0010
2022-01-12 01:05:41:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0009
2022-01-12 01:05:59:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0009
2022-01-12 01:06:18:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0008
2022-01-12 01:06:36:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0009
2022-01-12 01:06:55:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0008
2022-01-12 01:07:15:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 01:07:15:INFO:	Num examples = 100
2022-01-12 01:07:15:INFO:	RMSE = 31.0135
2022-01-12 01:07:17:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 01:07:17:INFO:	Num examples = 100
2022-01-12 01:07:17:INFO:	RMSE = 31.3105
2022-01-12 01:07:18:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0008
2022-01-12 01:07:36:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0009
2022-01-12 01:07:55:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0008
2022-01-12 01:08:13:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0009
2022-01-12 01:08:32:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0008
2022-01-12 01:08:50:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0009
2022-01-12 01:09:09:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0008
2022-01-12 01:09:27:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0009
2022-01-12 01:09:46:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0008
2022-01-12 01:10:05:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0008
2022-01-12 01:10:23:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0008
2022-01-12 01:10:42:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0008
2022-01-12 01:11:00:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0007
2022-01-12 01:11:19:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0009
2022-01-12 01:11:37:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0008
2022-01-12 01:11:58:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 01:11:58:INFO:	Num examples = 100
2022-01-12 01:11:58:INFO:	RMSE = 31.6726
2022-01-12 01:12:00:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 01:12:00:INFO:	Num examples = 100
2022-01-12 01:12:00:INFO:	RMSE = 31.4921
2022-01-12 01:12:00:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0008
2022-01-12 01:12:19:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0008
2022-01-12 01:12:37:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0007
2022-01-12 01:12:55:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0007
2022-01-12 01:13:14:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0008
2022-01-12 01:13:32:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0007
2022-01-12 01:13:51:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0008
2022-01-12 01:14:09:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0008
2022-01-12 01:14:28:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0007
2022-01-12 01:14:46:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0008
2022-01-12 01:15:05:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0008
2022-01-12 01:15:24:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0007
2022-01-12 01:15:42:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0008
2022-01-12 01:16:01:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0007
2022-01-12 01:16:19:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0008
2022-01-12 01:16:39:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 01:16:39:INFO:	Num examples = 100
2022-01-12 01:16:39:INFO:	RMSE = 31.2814
2022-01-12 01:16:42:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 01:16:42:INFO:	Num examples = 100
2022-01-12 01:16:42:INFO:	RMSE = 33.6393
2022-01-12 01:16:42:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0007
2022-01-12 01:17:01:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0008
2022-01-12 01:17:19:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0006
2022-01-12 01:17:38:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0007
2022-01-12 01:17:56:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0007
2022-01-12 01:18:15:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0007
2022-01-12 01:18:33:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0007
2022-01-12 01:18:52:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0007
2022-01-12 01:19:10:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0007
2022-01-12 01:19:29:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0006
2022-01-12 01:19:47:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0007
2022-01-12 01:20:06:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0007
2022-01-12 01:20:24:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0006
2022-01-12 01:20:43:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0007
2022-01-12 01:21:01:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0007
2022-01-12 01:21:22:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 01:21:22:INFO:	Num examples = 100
2022-01-12 01:21:22:INFO:	RMSE = 31.5660
2022-01-12 01:21:24:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 01:21:24:INFO:	Num examples = 100
2022-01-12 01:21:24:INFO:	RMSE = 32.5046
2022-01-12 01:21:24:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0007
2022-01-12 01:21:43:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0007
2022-01-12 01:22:01:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0007
2022-01-12 01:22:20:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0007
2022-01-12 01:22:38:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0006
2022-01-12 01:22:57:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0006
2022-01-12 01:23:16:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0007
2022-01-12 01:23:34:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0007
2022-01-12 01:23:53:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0006
2022-01-12 01:24:11:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0006
2022-01-12 01:24:30:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0006
2022-01-12 01:24:48:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0006
2022-01-12 01:25:07:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0006
2022-01-12 01:25:25:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0006
2022-01-12 01:25:44:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0006
2022-01-12 01:26:04:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 01:26:04:INFO:	Num examples = 100
2022-01-12 01:26:04:INFO:	RMSE = 31.4892
2022-01-12 01:26:06:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 01:26:06:INFO:	Num examples = 100
2022-01-12 01:26:06:INFO:	RMSE = 32.0098
2022-01-12 01:26:07:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0006
2022-01-12 01:26:25:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0006
2022-01-12 01:26:44:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0006
2022-01-12 01:27:02:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0006
2022-01-12 01:27:21:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0005
2022-01-12 01:27:39:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0006
2022-01-12 01:27:58:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0006
2022-01-12 01:28:16:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0006
2022-01-12 01:28:35:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0006
2022-01-12 01:28:53:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0006
2022-01-12 01:29:12:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0006
2022-01-12 01:29:30:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0007
2022-01-12 01:29:49:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0006
2022-01-12 01:30:07:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0006
2022-01-12 01:30:26:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0006
2022-01-12 01:30:46:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 01:30:46:INFO:	Num examples = 100
2022-01-12 01:30:46:INFO:	RMSE = 31.2384
2022-01-12 01:30:48:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 01:30:48:INFO:	Num examples = 100
2022-01-12 01:30:48:INFO:	RMSE = 31.7438
2022-01-12 01:30:48:INFO:	Output TEST RMSE:	30.7844
2022-01-12 01:30:48:INFO:	VALID RMSEs:	30.7162	31.2958	33.2470	31.8127	31.4109	28.7352	31.3105	31.4921	33.6393	32.5046	32.0098	31.7438
2022-01-12 01:30:48:INFO:	TEST RMSEs:	31.9210	32.6403	30.7177	30.9322	31.8077	30.7844	31.0135	31.6726	31.2814	31.5660	31.4892	31.2384
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-12 01:30:51:INFO:Finish setting logger...
2022-01-12 01:30:51:INFO:==> Training/Evaluation parameters are:
2022-01-12 01:30:51:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42'
2022-01-12 01:30:51:INFO:	data_fn=1
2022-01-12 01:30:51:INFO:	datatest_fn=1
2022-01-12 01:30:51:INFO:	filter_kernel_size=1
2022-01-12 01:30:51:INFO:	override_data_cache=False
2022-01-12 01:30:51:INFO:	maxRUL=125
2022-01-12 01:30:51:INFO:	low_ratio=0.1
2022-01-12 01:30:51:INFO:	high_ratio=0.99
2022-01-12 01:30:51:INFO:	aug_ratio=150
2022-01-12 01:30:51:INFO:	noise_amplitude=0.01
2022-01-12 01:30:51:INFO:	modeltype='cnn1d'
2022-01-12 01:30:51:INFO:	max_seq_len=550
2022-01-12 01:30:51:INFO:	d_model=128
2022-01-12 01:30:51:INFO:	p_dropout=0.1
2022-01-12 01:30:51:INFO:	n_head=4
2022-01-12 01:30:51:INFO:	n_layer=2
2022-01-12 01:30:51:INFO:	dim_feedforward=512
2022-01-12 01:30:51:INFO:	e_dropout=0.1
2022-01-12 01:30:51:INFO:	activation='relu'
2022-01-12 01:30:51:INFO:	layer_norm=False
2022-01-12 01:30:51:INFO:	support_size=2
2022-01-12 01:30:51:INFO:	inner_steps=2
2022-01-12 01:30:51:INFO:	lr_inner=0.0001
2022-01-12 01:30:51:INFO:	lr_meta=0.001
2022-01-12 01:30:51:INFO:	n_epochs=12
2022-01-12 01:30:51:INFO:	train_batch_size=20
2022-01-12 01:30:51:INFO:	eval_batch_size=1
2022-01-12 01:30:51:INFO:	lr=0.001
2022-01-12 01:30:51:INFO:	weight_decay=0.01
2022-01-12 01:30:51:INFO:	warmup_ratio=0.0
2022-01-12 01:30:51:INFO:	max_grad_norm=5.0
2022-01-12 01:30:51:INFO:	logging_steps=50
2022-01-12 01:30:51:INFO:	seed=42
2022-01-12 01:30:51:INFO:	gpu_id=0
2022-01-12 01:30:51:INFO:	do_train=True
2022-01-12 01:30:51:INFO:	do_eval=False
2022-01-12 01:30:51:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-12 01:30:51:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-12 01:30:51:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-12 01:30:51:INFO:	device=device(type='cuda'))
2022-01-12 01:30:51:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 01:30:51:INFO:==> Read data from data/train_FD001.txt...
2022-01-12 01:30:51:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 01:30:51:INFO:==> Min_max normalization...
2022-01-12 01:30:51:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 01:30:51:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 01:30:51:INFO:==> Read data from data/test_FD001.txt...
2022-01-12 01:30:51:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 01:30:51:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-12 01:30:51:INFO:	min_rul: 7, max_rul: 145
2022-01-12 01:30:51:INFO:==> Input length ratio of the [TEST] data:
2022-01-12 01:30:51:INFO:	min_ratio = 0.2067
2022-01-12 01:30:51:INFO:	max_ratio = 0.9667
2022-01-12 01:30:51:INFO:==> Min_max normalization...
2022-01-12 01:30:51:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 01:30:51:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 01:30:51:INFO:==> Computing Criterion...
2022-01-12 01:30:51:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-12 01:31:00:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-12 01:31:00:INFO:NumExpr defaulting to 8 threads.
2022-01-12 01:31:00:INFO:=============== Scheme: Meta Learning ===============
2022-01-12 01:31:00:INFO:	Num examples = 15000
2022-01-12 01:31:00:INFO:	Num epochs = 12
2022-01-12 01:31:00:INFO:	Batch size = 20
2022-01-12 01:31:00:INFO:	Total meta optimization steps = 9000
2022-01-12 01:31:00:INFO:	Total inner optimization steps = 18000
2022-01-12 01:31:08:INFO:==> Group parameters for optimization...
2022-01-12 01:31:08:INFO:    Parameters to update are:
2022-01-12 01:31:08:INFO:	conv1.0.weight
2022-01-12 01:31:08:INFO:	conv2.0.weight
2022-01-12 01:31:08:INFO:	conv3.0.weight
2022-01-12 01:31:08:INFO:	conv4.0.weight
2022-01-12 01:31:08:INFO:	conv5.0.weight
2022-01-12 01:31:08:INFO:	fc_1.0.weight
2022-01-12 01:31:08:INFO:	fc_1.0.bias
2022-01-12 01:31:08:INFO:	fc_2.weight
2022-01-12 01:31:08:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-12 01:31:11:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0011
2022-01-12 01:32:26:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0166
2022-01-12 01:33:40:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0068
2022-01-12 01:34:54:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0060
2022-01-12 01:36:09:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0065
2022-01-12 01:37:24:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0052
2022-01-12 01:38:39:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0037
2022-01-12 01:39:53:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0043
2022-01-12 01:41:08:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0031
2022-01-12 01:42:23:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0025
2022-01-12 01:43:38:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0022
2022-01-12 01:44:53:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0023
2022-01-12 01:46:08:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0021
2022-01-12 01:47:23:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0021
2022-01-12 01:48:38:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0018
2022-01-12 01:49:58:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 01:49:58:INFO:	Num examples = 100
2022-01-12 01:49:58:INFO:	RMSE = 32.3072
2022-01-12 01:50:05:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 01:50:05:INFO:	Num examples = 100
2022-01-12 01:50:05:INFO:	RMSE = 25.6635
2022-01-12 01:50:05:INFO:==> Minimal valid RMSE!
2022-01-12 01:50:05:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 01:50:07:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0017
2022-01-12 01:51:22:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0018
2022-01-12 01:52:36:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0015
2022-01-12 01:53:52:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0015
2022-01-12 01:55:07:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0016
2022-01-12 01:56:22:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0014
2022-01-12 01:57:38:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0014
2022-01-12 01:58:53:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0015
2022-01-12 02:00:07:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0013
2022-01-12 02:01:22:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0012
2022-01-12 02:02:37:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0013
2022-01-12 02:03:51:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0012
2022-01-12 02:05:06:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0012
2022-01-12 02:06:22:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0014
2022-01-12 02:07:36:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0013
2022-01-12 02:08:57:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 02:08:57:INFO:	Num examples = 100
2022-01-12 02:08:57:INFO:	RMSE = 34.5220
2022-01-12 02:09:04:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 02:09:04:INFO:	Num examples = 100
2022-01-12 02:09:04:INFO:	RMSE = 22.2175
2022-01-12 02:09:04:INFO:==> Minimal valid RMSE!
2022-01-12 02:09:04:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 02:09:06:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0011
2022-01-12 02:10:20:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0011
2022-01-12 02:11:36:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0011
2022-01-12 02:12:50:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0010
2022-01-12 02:14:06:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0011
2022-01-12 02:15:21:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0011
2022-01-12 02:16:35:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0010
2022-01-12 02:17:50:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0009
2022-01-12 02:19:05:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0010
2022-01-12 02:20:19:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0009
2022-01-12 02:21:34:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0008
2022-01-12 02:22:49:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0009
2022-01-12 02:24:03:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0009
2022-01-12 02:25:18:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0008
2022-01-12 02:26:33:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0009
2022-01-12 02:27:54:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 02:27:54:INFO:	Num examples = 100
2022-01-12 02:27:54:INFO:	RMSE = 32.6524
2022-01-12 02:28:01:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 02:28:01:INFO:	Num examples = 100
2022-01-12 02:28:01:INFO:	RMSE = 18.6177
2022-01-12 02:28:01:INFO:==> Minimal valid RMSE!
2022-01-12 02:28:01:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 02:28:02:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0008
2022-01-12 02:29:16:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0008
2022-01-12 02:30:32:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0008
2022-01-12 02:31:47:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0006
2022-01-12 02:33:02:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0007
2022-01-12 02:34:16:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0006
2022-01-12 02:35:30:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0007
2022-01-12 02:36:45:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0006
2022-01-12 02:38:00:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0007
2022-01-12 02:39:15:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0006
2022-01-12 02:40:29:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0007
2022-01-12 02:41:44:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0006
2022-01-12 02:42:59:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0005
2022-01-12 02:44:14:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0006
2022-01-12 02:45:29:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0005
2022-01-12 02:46:49:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 02:46:49:INFO:	Num examples = 100
2022-01-12 02:46:49:INFO:	RMSE = 33.8899
2022-01-12 02:46:57:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 02:46:57:INFO:	Num examples = 100
2022-01-12 02:46:57:INFO:	RMSE = 17.2506
2022-01-12 02:46:57:INFO:==> Minimal valid RMSE!
2022-01-12 02:46:57:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 02:46:58:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0006
2022-01-12 02:48:13:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0004
2022-01-12 02:49:27:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0005
2022-01-12 02:50:43:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0005
2022-01-12 02:51:57:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0006
2022-01-12 02:53:12:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0005
2022-01-12 02:54:26:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0005
2022-01-12 02:55:41:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0004
2022-01-12 02:56:57:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0005
2022-01-12 02:58:12:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0004
2022-01-12 02:59:26:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0005
2022-01-12 03:00:41:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0005
2022-01-12 03:01:55:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0005
2022-01-12 03:03:09:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0004
2022-01-12 03:04:24:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0004
2022-01-12 03:05:44:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 03:05:44:INFO:	Num examples = 100
2022-01-12 03:05:44:INFO:	RMSE = 35.3255
2022-01-12 03:05:51:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 03:05:51:INFO:	Num examples = 100
2022-01-12 03:05:51:INFO:	RMSE = 16.3494
2022-01-12 03:05:51:INFO:==> Minimal valid RMSE!
2022-01-12 03:05:51:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 03:05:53:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0004
2022-01-12 03:07:08:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0003
2022-01-12 03:08:23:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0004
2022-01-12 03:09:37:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0003
2022-01-12 03:10:52:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0004
2022-01-12 03:12:06:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0004
2022-01-12 03:13:21:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0003
2022-01-12 03:14:35:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0004
2022-01-12 03:15:50:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0003
2022-01-12 03:17:05:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0003
2022-01-12 03:18:19:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0003
2022-01-12 03:19:34:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0003
2022-01-12 03:20:49:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0003
2022-01-12 03:22:04:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0003
2022-01-12 03:23:19:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0003
2022-01-12 03:24:38:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 03:24:38:INFO:	Num examples = 100
2022-01-12 03:24:38:INFO:	RMSE = 35.0468
2022-01-12 03:24:46:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 03:24:46:INFO:	Num examples = 100
2022-01-12 03:24:46:INFO:	RMSE = 18.2115
2022-01-12 03:24:47:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0003
2022-01-12 03:26:02:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0003
2022-01-12 03:27:16:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0003
2022-01-12 03:28:31:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0003
2022-01-12 03:29:46:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0002
2022-01-12 03:31:00:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0003
2022-01-12 03:32:15:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0003
2022-01-12 03:33:29:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0003
2022-01-12 03:34:44:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0003
2022-01-12 03:36:00:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0003
2022-01-12 03:37:15:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0003
2022-01-12 03:38:31:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0003
2022-01-12 03:39:47:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0003
2022-01-12 03:41:01:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0003
2022-01-12 03:42:16:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0002
2022-01-12 03:43:35:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 03:43:35:INFO:	Num examples = 100
2022-01-12 03:43:35:INFO:	RMSE = 34.0237
2022-01-12 03:43:42:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 03:43:42:INFO:	Num examples = 100
2022-01-12 03:43:42:INFO:	RMSE = 16.4133
2022-01-12 03:43:44:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0003
2022-01-12 03:44:58:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0003
2022-01-12 03:46:12:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0002
2022-01-12 03:47:25:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0003
2022-01-12 03:48:38:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0002
2022-01-12 03:49:53:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0002
2022-01-12 03:51:06:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0002
2022-01-12 03:52:21:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0002
2022-01-12 03:53:34:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0002
2022-01-12 03:54:49:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0002
2022-01-12 03:56:03:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0002
2022-01-12 03:57:17:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0002
2022-01-12 03:58:31:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0002
2022-01-12 03:59:46:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0002
2022-01-12 04:01:01:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0002
2022-01-12 04:02:20:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 04:02:20:INFO:	Num examples = 100
2022-01-12 04:02:20:INFO:	RMSE = 34.3598
2022-01-12 04:02:27:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 04:02:27:INFO:	Num examples = 100
2022-01-12 04:02:27:INFO:	RMSE = 17.2430
2022-01-12 04:02:29:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 04:03:43:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 04:04:58:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 04:06:12:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 04:07:25:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 04:08:38:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 04:09:52:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0002
2022-01-12 04:11:05:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 04:12:19:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 04:13:32:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 04:14:46:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 04:15:59:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 04:17:13:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 04:18:27:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 04:19:41:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 04:21:00:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 04:21:00:INFO:	Num examples = 100
2022-01-12 04:21:00:INFO:	RMSE = 34.7358
2022-01-12 04:21:07:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 04:21:07:INFO:	Num examples = 100
2022-01-12 04:21:07:INFO:	RMSE = 17.9787
2022-01-12 04:21:09:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0002
2022-01-12 04:22:22:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 04:23:36:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 04:24:50:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 04:26:05:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 04:27:20:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 04:28:31:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 04:29:45:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 04:30:58:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 04:32:13:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0002
2022-01-12 04:33:27:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0001
2022-01-12 04:34:41:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0001
2022-01-12 04:35:53:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0001
2022-01-12 04:37:07:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0001
2022-01-12 04:38:21:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0002
2022-01-12 04:39:40:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 04:39:40:INFO:	Num examples = 100
2022-01-12 04:39:40:INFO:	RMSE = 34.0802
2022-01-12 04:39:48:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 04:39:48:INFO:	Num examples = 100
2022-01-12 04:39:48:INFO:	RMSE = 19.3703
2022-01-12 04:39:49:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0002
2022-01-12 04:41:03:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0001
2022-01-12 04:42:18:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0001
2022-01-12 04:43:32:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-12 04:44:45:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0001
2022-01-12 04:45:57:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0001
2022-01-12 04:47:11:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0001
2022-01-12 04:48:24:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0001
2022-01-12 04:49:38:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0002
2022-01-12 04:50:50:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0002
2022-01-12 04:52:03:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0001
2022-01-12 04:53:18:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0001
2022-01-12 04:54:32:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0001
2022-01-12 04:55:47:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0001
2022-01-12 04:57:01:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0001
2022-01-12 04:58:19:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 04:58:19:INFO:	Num examples = 100
2022-01-12 04:58:19:INFO:	RMSE = 34.3317
2022-01-12 04:58:27:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 04:58:27:INFO:	Num examples = 100
2022-01-12 04:58:27:INFO:	RMSE = 21.3433
2022-01-12 04:58:28:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0001
2022-01-12 04:59:41:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0001
2022-01-12 05:00:55:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0001
2022-01-12 05:02:09:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0001
2022-01-12 05:03:23:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0001
2022-01-12 05:04:36:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0001
2022-01-12 05:05:49:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0001
2022-01-12 05:07:02:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0001
2022-01-12 05:08:15:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0001
2022-01-12 05:09:27:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0001
2022-01-12 05:10:41:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0001
2022-01-12 05:11:55:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0001
2022-01-12 05:13:08:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0001
2022-01-12 05:14:21:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0001
2022-01-12 05:15:35:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0001
2022-01-12 05:16:55:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 05:16:55:INFO:	Num examples = 100
2022-01-12 05:16:55:INFO:	RMSE = 34.2303
2022-01-12 05:17:02:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 05:17:02:INFO:	Num examples = 100
2022-01-12 05:17:02:INFO:	RMSE = 23.1036
2022-01-12 05:17:02:INFO:	Output TEST RMSE:	35.3255
2022-01-12 05:17:02:INFO:	VALID RMSEs:	25.6635	22.2175	18.6177	17.2506	16.3494	18.2115	16.4133	17.2430	17.9787	19.3703	21.3433	23.1036
2022-01-12 05:17:02:INFO:	TEST RMSEs:	32.3072	34.5220	32.6524	33.8899	35.3255	35.0468	34.0237	34.3598	34.7358	34.0802	34.3317	34.2303
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-12 05:17:06:INFO:Finish setting logger...
2022-01-12 05:17:06:INFO:==> Training/Evaluation parameters are:
2022-01-12 05:17:06:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42'
2022-01-12 05:17:06:INFO:	data_fn=1
2022-01-12 05:17:06:INFO:	datatest_fn=1
2022-01-12 05:17:06:INFO:	filter_kernel_size=1
2022-01-12 05:17:06:INFO:	override_data_cache=False
2022-01-12 05:17:06:INFO:	maxRUL=125
2022-01-12 05:17:06:INFO:	low_ratio=0.1
2022-01-12 05:17:06:INFO:	high_ratio=0.99
2022-01-12 05:17:06:INFO:	aug_ratio=150
2022-01-12 05:17:06:INFO:	noise_amplitude=0.01
2022-01-12 05:17:06:INFO:	modeltype='cnn1d'
2022-01-12 05:17:06:INFO:	max_seq_len=550
2022-01-12 05:17:06:INFO:	d_model=128
2022-01-12 05:17:06:INFO:	p_dropout=0.1
2022-01-12 05:17:06:INFO:	n_head=4
2022-01-12 05:17:06:INFO:	n_layer=2
2022-01-12 05:17:06:INFO:	dim_feedforward=512
2022-01-12 05:17:06:INFO:	e_dropout=0.1
2022-01-12 05:17:06:INFO:	activation='relu'
2022-01-12 05:17:06:INFO:	layer_norm=False
2022-01-12 05:17:06:INFO:	support_size=2
2022-01-12 05:17:06:INFO:	inner_steps=2
2022-01-12 05:17:06:INFO:	lr_inner=0.001
2022-01-12 05:17:06:INFO:	lr_meta=0.001
2022-01-12 05:17:06:INFO:	n_epochs=12
2022-01-12 05:17:06:INFO:	train_batch_size=20
2022-01-12 05:17:06:INFO:	eval_batch_size=1
2022-01-12 05:17:06:INFO:	lr=0.001
2022-01-12 05:17:06:INFO:	weight_decay=0.01
2022-01-12 05:17:06:INFO:	warmup_ratio=0.0
2022-01-12 05:17:06:INFO:	max_grad_norm=5.0
2022-01-12 05:17:06:INFO:	logging_steps=50
2022-01-12 05:17:06:INFO:	seed=42
2022-01-12 05:17:06:INFO:	gpu_id=0
2022-01-12 05:17:06:INFO:	do_train=True
2022-01-12 05:17:06:INFO:	do_eval=False
2022-01-12 05:17:06:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-12 05:17:06:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-12 05:17:06:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-12 05:17:06:INFO:	device=device(type='cuda'))
2022-01-12 05:17:06:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 05:17:06:INFO:==> Read data from data/train_FD001.txt...
2022-01-12 05:17:06:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 05:17:06:INFO:==> Min_max normalization...
2022-01-12 05:17:06:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 05:17:06:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 05:17:06:INFO:==> Read data from data/test_FD001.txt...
2022-01-12 05:17:06:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 05:17:06:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-12 05:17:06:INFO:	min_rul: 7, max_rul: 145
2022-01-12 05:17:06:INFO:==> Input length ratio of the [TEST] data:
2022-01-12 05:17:06:INFO:	min_ratio = 0.2067
2022-01-12 05:17:06:INFO:	max_ratio = 0.9667
2022-01-12 05:17:06:INFO:==> Min_max normalization...
2022-01-12 05:17:06:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 05:17:06:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 05:17:06:INFO:==> Computing Criterion...
2022-01-12 05:17:06:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-12 05:17:13:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-12 05:17:13:INFO:NumExpr defaulting to 8 threads.
2022-01-12 05:17:13:INFO:=============== Scheme: Meta Learning ===============
2022-01-12 05:17:13:INFO:	Num examples = 15000
2022-01-12 05:17:13:INFO:	Num epochs = 12
2022-01-12 05:17:13:INFO:	Batch size = 20
2022-01-12 05:17:13:INFO:	Total meta optimization steps = 9000
2022-01-12 05:17:13:INFO:	Total inner optimization steps = 18000
2022-01-12 05:17:22:INFO:==> Group parameters for optimization...
2022-01-12 05:17:22:INFO:    Parameters to update are:
2022-01-12 05:17:22:INFO:	conv1.0.weight
2022-01-12 05:17:22:INFO:	conv2.0.weight
2022-01-12 05:17:22:INFO:	conv3.0.weight
2022-01-12 05:17:22:INFO:	conv4.0.weight
2022-01-12 05:17:22:INFO:	conv5.0.weight
2022-01-12 05:17:22:INFO:	fc_1.0.weight
2022-01-12 05:17:22:INFO:	fc_1.0.bias
2022-01-12 05:17:22:INFO:	fc_2.weight
2022-01-12 05:17:22:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-12 05:17:25:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0011
2022-01-12 05:18:38:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0166
2022-01-12 05:19:51:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0068
2022-01-12 05:21:05:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0060
2022-01-12 05:22:19:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0065
2022-01-12 05:23:31:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0052
2022-01-12 05:24:44:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0037
2022-01-12 05:25:57:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0043
2022-01-12 05:27:09:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0031
2022-01-12 05:28:23:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0025
2022-01-12 05:29:36:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0022
2022-01-12 05:30:48:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0023
2022-01-12 05:32:02:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0021
2022-01-12 05:33:16:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0021
2022-01-12 05:34:29:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0018
2022-01-12 05:35:47:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 05:35:47:INFO:	Num examples = 100
2022-01-12 05:35:47:INFO:	RMSE = 32.3072
2022-01-12 05:35:54:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 05:35:54:INFO:	Num examples = 100
2022-01-12 05:35:54:INFO:	RMSE = 25.6635
2022-01-12 05:35:54:INFO:==> Minimal valid RMSE!
2022-01-12 05:35:54:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 05:35:56:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0017
2022-01-12 05:37:10:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0018
2022-01-12 05:38:23:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0015
2022-01-12 05:39:36:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0015
2022-01-12 05:40:49:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0016
2022-01-12 05:42:03:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0014
2022-01-12 05:43:17:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0014
2022-01-12 05:44:30:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0015
2022-01-12 05:45:43:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0013
2022-01-12 05:46:56:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0012
2022-01-12 05:48:10:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0013
2022-01-12 05:49:23:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0012
2022-01-12 05:50:34:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0012
2022-01-12 05:51:48:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0014
2022-01-12 05:53:02:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0013
2022-01-12 05:54:21:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 05:54:21:INFO:	Num examples = 100
2022-01-12 05:54:21:INFO:	RMSE = 34.5220
2022-01-12 05:54:28:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 05:54:28:INFO:	Num examples = 100
2022-01-12 05:54:28:INFO:	RMSE = 22.2175
2022-01-12 05:54:28:INFO:==> Minimal valid RMSE!
2022-01-12 05:54:28:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 05:54:30:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0011
2022-01-12 05:55:43:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0011
2022-01-12 05:56:57:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0011
2022-01-12 05:58:11:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0010
2022-01-12 05:59:25:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0011
2022-01-12 06:00:38:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0011
2022-01-12 06:01:51:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0010
2022-01-12 06:03:05:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0009
2022-01-12 06:04:18:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0010
2022-01-12 06:05:32:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0009
2022-01-12 06:06:46:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0008
2022-01-12 06:08:01:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0009
2022-01-12 06:09:15:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0009
2022-01-12 06:10:28:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0008
2022-01-12 06:11:42:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0009
2022-01-12 06:13:01:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 06:13:01:INFO:	Num examples = 100
2022-01-12 06:13:01:INFO:	RMSE = 32.6524
2022-01-12 06:13:09:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 06:13:09:INFO:	Num examples = 100
2022-01-12 06:13:09:INFO:	RMSE = 18.6177
2022-01-12 06:13:09:INFO:==> Minimal valid RMSE!
2022-01-12 06:13:09:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 06:13:10:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0008
2022-01-12 06:14:24:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0008
2022-01-12 06:15:37:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0008
2022-01-12 06:16:50:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0006
2022-01-12 06:18:04:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0007
2022-01-12 06:19:18:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0006
2022-01-12 06:20:32:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0007
2022-01-12 06:21:46:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0006
2022-01-12 06:22:59:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0007
2022-01-12 06:24:14:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0006
2022-01-12 06:25:27:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0007
2022-01-12 06:26:40:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0006
2022-01-12 06:27:53:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0005
2022-01-12 06:29:06:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0006
2022-01-12 06:30:20:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0005
2022-01-12 06:31:39:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 06:31:39:INFO:	Num examples = 100
2022-01-12 06:31:39:INFO:	RMSE = 33.8899
2022-01-12 06:31:46:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 06:31:46:INFO:	Num examples = 100
2022-01-12 06:31:46:INFO:	RMSE = 17.2506
2022-01-12 06:31:46:INFO:==> Minimal valid RMSE!
2022-01-12 06:31:46:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 06:31:48:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0006
2022-01-12 06:33:03:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0004
2022-01-12 06:34:16:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0005
2022-01-12 06:35:30:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0005
2022-01-12 06:36:45:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0006
2022-01-12 06:37:59:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0005
2022-01-12 06:39:13:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0005
2022-01-12 06:40:25:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0004
2022-01-12 06:41:38:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0005
2022-01-12 06:42:52:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0004
2022-01-12 06:44:04:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0005
2022-01-12 06:45:17:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0005
2022-01-12 06:46:29:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0005
2022-01-12 06:47:42:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0004
2022-01-12 06:48:55:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0004
2022-01-12 06:50:14:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 06:50:14:INFO:	Num examples = 100
2022-01-12 06:50:14:INFO:	RMSE = 35.3255
2022-01-12 06:50:21:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 06:50:21:INFO:	Num examples = 100
2022-01-12 06:50:21:INFO:	RMSE = 16.3494
2022-01-12 06:50:21:INFO:==> Minimal valid RMSE!
2022-01-12 06:50:21:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 06:50:23:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0004
2022-01-12 06:51:36:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0003
2022-01-12 06:52:50:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0004
2022-01-12 06:54:03:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0003
2022-01-12 06:55:17:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0004
2022-01-12 06:56:30:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0004
2022-01-12 06:57:43:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0003
2022-01-12 06:58:56:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0004
2022-01-12 07:00:10:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0003
2022-01-12 07:01:24:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0003
2022-01-12 07:02:37:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0003
2022-01-12 07:03:51:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0003
2022-01-12 07:05:04:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0003
2022-01-12 07:06:18:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0003
2022-01-12 07:07:31:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0003
2022-01-12 07:08:51:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 07:08:51:INFO:	Num examples = 100
2022-01-12 07:08:51:INFO:	RMSE = 35.0468
2022-01-12 07:08:58:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 07:08:58:INFO:	Num examples = 100
2022-01-12 07:08:58:INFO:	RMSE = 18.2115
2022-01-12 07:08:59:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0003
2022-01-12 07:10:13:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0003
2022-01-12 07:11:26:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0003
2022-01-12 07:12:39:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0003
2022-01-12 07:13:53:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0002
2022-01-12 07:15:08:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0003
2022-01-12 07:16:20:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0003
2022-01-12 07:17:34:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0003
2022-01-12 07:18:47:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0003
2022-01-12 07:20:00:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0003
2022-01-12 07:21:14:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0003
2022-01-12 07:22:27:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0003
2022-01-12 07:23:41:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0003
2022-01-12 07:24:54:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0003
2022-01-12 07:26:07:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0002
2022-01-12 07:27:25:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 07:27:25:INFO:	Num examples = 100
2022-01-12 07:27:25:INFO:	RMSE = 34.0237
2022-01-12 07:27:33:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 07:27:33:INFO:	Num examples = 100
2022-01-12 07:27:33:INFO:	RMSE = 16.4133
2022-01-12 07:27:34:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0003
2022-01-12 07:28:47:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0003
2022-01-12 07:30:00:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0002
2022-01-12 07:31:12:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0003
2022-01-12 07:32:26:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0002
2022-01-12 07:33:39:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0002
2022-01-12 07:34:52:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0002
2022-01-12 07:36:04:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0002
2022-01-12 07:37:17:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0002
2022-01-12 07:38:30:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0002
2022-01-12 07:39:43:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0002
2022-01-12 07:40:56:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0002
2022-01-12 07:42:09:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0002
2022-01-12 07:43:23:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0002
2022-01-12 07:44:36:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0002
2022-01-12 07:45:54:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 07:45:54:INFO:	Num examples = 100
2022-01-12 07:45:54:INFO:	RMSE = 34.3598
2022-01-12 07:46:02:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 07:46:02:INFO:	Num examples = 100
2022-01-12 07:46:02:INFO:	RMSE = 17.2430
2022-01-12 07:46:03:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 07:47:15:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 07:48:27:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 07:49:40:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 07:50:53:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 07:52:05:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 07:53:17:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0002
2022-01-12 07:54:30:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 07:55:43:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 07:56:56:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 07:58:09:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 07:59:23:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 08:00:36:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 08:01:49:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 08:03:03:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 08:04:22:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 08:04:22:INFO:	Num examples = 100
2022-01-12 08:04:22:INFO:	RMSE = 34.7358
2022-01-12 08:04:29:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 08:04:29:INFO:	Num examples = 100
2022-01-12 08:04:29:INFO:	RMSE = 17.9787
2022-01-12 08:04:31:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0002
2022-01-12 08:05:44:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 08:06:57:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 08:08:10:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 08:09:24:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 08:10:38:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 08:11:52:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 08:13:05:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 08:14:19:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 08:15:33:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0002
2022-01-12 08:16:46:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0001
2022-01-12 08:18:00:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0001
2022-01-12 08:19:14:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0001
2022-01-12 08:20:28:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0001
2022-01-12 08:21:42:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0002
2022-01-12 08:23:02:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 08:23:02:INFO:	Num examples = 100
2022-01-12 08:23:02:INFO:	RMSE = 34.0802
2022-01-12 08:23:09:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 08:23:09:INFO:	Num examples = 100
2022-01-12 08:23:09:INFO:	RMSE = 19.3703
2022-01-12 08:23:10:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0002
2022-01-12 08:24:24:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0001
2022-01-12 08:25:38:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0001
2022-01-12 08:26:51:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-12 08:28:03:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0001
2022-01-12 08:29:17:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0001
2022-01-12 08:30:30:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0001
2022-01-12 08:31:44:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0001
2022-01-12 08:32:57:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0002
2022-01-12 08:34:11:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0002
2022-01-12 08:35:24:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0001
2022-01-12 08:36:39:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0001
2022-01-12 08:37:53:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0001
2022-01-12 08:39:07:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0001
2022-01-12 08:40:21:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0001
2022-01-12 08:41:40:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 08:41:40:INFO:	Num examples = 100
2022-01-12 08:41:40:INFO:	RMSE = 34.3317
2022-01-12 08:41:47:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 08:41:47:INFO:	Num examples = 100
2022-01-12 08:41:47:INFO:	RMSE = 21.3433
2022-01-12 08:41:49:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0001
2022-01-12 08:43:02:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0001
2022-01-12 08:44:16:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0001
2022-01-12 08:45:30:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0001
2022-01-12 08:46:44:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0001
2022-01-12 08:47:57:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0001
2022-01-12 08:49:10:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0001
2022-01-12 08:50:24:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0001
2022-01-12 08:51:37:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0001
2022-01-12 08:52:50:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0001
2022-01-12 08:54:03:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0001
2022-01-12 08:55:16:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0001
2022-01-12 08:56:30:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0001
2022-01-12 08:57:43:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0001
2022-01-12 08:58:56:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0001
2022-01-12 09:00:16:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 09:00:16:INFO:	Num examples = 100
2022-01-12 09:00:16:INFO:	RMSE = 34.2303
2022-01-12 09:00:23:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 09:00:23:INFO:	Num examples = 100
2022-01-12 09:00:23:INFO:	RMSE = 23.1036
2022-01-12 09:00:23:INFO:	Output TEST RMSE:	35.3255
2022-01-12 09:00:23:INFO:	VALID RMSEs:	25.6635	22.2175	18.6177	17.2506	16.3494	18.2115	16.4133	17.2430	17.9787	19.3703	21.3433	23.1036
2022-01-12 09:00:23:INFO:	TEST RMSEs:	32.3072	34.5220	32.6524	33.8899	35.3255	35.0468	34.0237	34.3598	34.7358	34.0802	34.3317	34.2303
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-12 09:00:26:INFO:Finish setting logger...
2022-01-12 09:00:26:INFO:==> Training/Evaluation parameters are:
2022-01-12 09:00:26:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42'
2022-01-12 09:00:26:INFO:	data_fn=1
2022-01-12 09:00:26:INFO:	datatest_fn=1
2022-01-12 09:00:26:INFO:	filter_kernel_size=1
2022-01-12 09:00:26:INFO:	override_data_cache=False
2022-01-12 09:00:26:INFO:	maxRUL=125
2022-01-12 09:00:26:INFO:	low_ratio=0.1
2022-01-12 09:00:26:INFO:	high_ratio=0.99
2022-01-12 09:00:26:INFO:	aug_ratio=150
2022-01-12 09:00:26:INFO:	noise_amplitude=0.01
2022-01-12 09:00:26:INFO:	modeltype='cnn1d'
2022-01-12 09:00:26:INFO:	max_seq_len=550
2022-01-12 09:00:26:INFO:	d_model=128
2022-01-12 09:00:26:INFO:	p_dropout=0.1
2022-01-12 09:00:26:INFO:	n_head=4
2022-01-12 09:00:26:INFO:	n_layer=2
2022-01-12 09:00:26:INFO:	dim_feedforward=512
2022-01-12 09:00:26:INFO:	e_dropout=0.1
2022-01-12 09:00:26:INFO:	activation='relu'
2022-01-12 09:00:26:INFO:	layer_norm=False
2022-01-12 09:00:26:INFO:	support_size=5
2022-01-12 09:00:26:INFO:	inner_steps=2
2022-01-12 09:00:26:INFO:	lr_inner=0.0001
2022-01-12 09:00:26:INFO:	lr_meta=0.001
2022-01-12 09:00:26:INFO:	n_epochs=12
2022-01-12 09:00:26:INFO:	train_batch_size=20
2022-01-12 09:00:26:INFO:	eval_batch_size=1
2022-01-12 09:00:26:INFO:	lr=0.001
2022-01-12 09:00:26:INFO:	weight_decay=0.01
2022-01-12 09:00:26:INFO:	warmup_ratio=0.0
2022-01-12 09:00:26:INFO:	max_grad_norm=5.0
2022-01-12 09:00:26:INFO:	logging_steps=50
2022-01-12 09:00:26:INFO:	seed=42
2022-01-12 09:00:26:INFO:	gpu_id=0
2022-01-12 09:00:26:INFO:	do_train=True
2022-01-12 09:00:26:INFO:	do_eval=False
2022-01-12 09:00:26:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-12 09:00:26:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-12 09:00:26:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-12 09:00:26:INFO:	device=device(type='cuda'))
2022-01-12 09:00:26:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 09:00:26:INFO:==> Read data from data/train_FD001.txt...
2022-01-12 09:00:26:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 09:00:26:INFO:==> Min_max normalization...
2022-01-12 09:00:26:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 09:00:26:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 09:00:26:INFO:==> Read data from data/test_FD001.txt...
2022-01-12 09:00:26:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 09:00:27:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-12 09:00:27:INFO:	min_rul: 7, max_rul: 145
2022-01-12 09:00:27:INFO:==> Input length ratio of the [TEST] data:
2022-01-12 09:00:27:INFO:	min_ratio = 0.2067
2022-01-12 09:00:27:INFO:	max_ratio = 0.9667
2022-01-12 09:00:27:INFO:==> Min_max normalization...
2022-01-12 09:00:27:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 09:00:27:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 09:00:27:INFO:==> Computing Criterion...
2022-01-12 09:00:27:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-12 09:00:40:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-12 09:00:40:INFO:NumExpr defaulting to 8 threads.
2022-01-12 09:00:41:INFO:=============== Scheme: Meta Learning ===============
2022-01-12 09:00:41:INFO:	Num examples = 15000
2022-01-12 09:00:41:INFO:	Num epochs = 12
2022-01-12 09:00:41:INFO:	Batch size = 20
2022-01-12 09:00:41:INFO:	Total meta optimization steps = 9000
2022-01-12 09:00:41:INFO:	Total inner optimization steps = 18000
2022-01-12 09:00:49:INFO:==> Group parameters for optimization...
2022-01-12 09:00:49:INFO:    Parameters to update are:
2022-01-12 09:00:49:INFO:	conv1.0.weight
2022-01-12 09:00:49:INFO:	conv2.0.weight
2022-01-12 09:00:49:INFO:	conv3.0.weight
2022-01-12 09:00:49:INFO:	conv4.0.weight
2022-01-12 09:00:49:INFO:	conv5.0.weight
2022-01-12 09:00:49:INFO:	fc_1.0.weight
2022-01-12 09:00:49:INFO:	fc_1.0.bias
2022-01-12 09:00:49:INFO:	fc_2.weight
2022-01-12 09:00:49:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-12 09:00:52:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0010
2022-01-12 09:02:06:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0470
2022-01-12 09:03:19:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0257
2022-01-12 09:04:33:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0174
2022-01-12 09:05:47:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0090
2022-01-12 09:07:00:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0067
2022-01-12 09:08:13:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0060
2022-01-12 09:09:27:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0056
2022-01-12 09:10:41:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0045
2022-01-12 09:11:56:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0037
2022-01-12 09:13:10:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0034
2022-01-12 09:14:24:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0029
2022-01-12 09:15:38:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0024
2022-01-12 09:16:52:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0022
2022-01-12 09:18:06:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0019
2022-01-12 09:19:26:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 09:19:26:INFO:	Num examples = 100
2022-01-12 09:19:26:INFO:	RMSE = 32.8729
2022-01-12 09:19:34:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 09:19:34:INFO:	Num examples = 100
2022-01-12 09:19:34:INFO:	RMSE = 24.9272
2022-01-12 09:19:34:INFO:==> Minimal valid RMSE!
2022-01-12 09:19:34:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 09:19:35:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0021
2022-01-12 09:20:50:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0020
2022-01-12 09:22:04:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0020
2022-01-12 09:23:18:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0018
2022-01-12 09:24:32:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0017
2022-01-12 09:25:46:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0017
2022-01-12 09:27:01:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0018
2022-01-12 09:28:15:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0016
2022-01-12 09:29:29:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0015
2022-01-12 09:30:42:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0016
2022-01-12 09:31:56:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0015
2022-01-12 09:33:09:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0014
2022-01-12 09:34:23:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0014
2022-01-12 09:35:37:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0014
2022-01-12 09:36:51:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0013
2022-01-12 09:38:11:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 09:38:11:INFO:	Num examples = 100
2022-01-12 09:38:11:INFO:	RMSE = 34.3853
2022-01-12 09:38:18:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 09:38:18:INFO:	Num examples = 100
2022-01-12 09:38:18:INFO:	RMSE = 21.8471
2022-01-12 09:38:18:INFO:==> Minimal valid RMSE!
2022-01-12 09:38:18:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 09:38:19:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0014
2022-01-12 09:39:33:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0014
2022-01-12 09:40:47:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0013
2022-01-12 09:42:01:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0012
2022-01-12 09:43:15:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0013
2022-01-12 09:44:29:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0013
2022-01-12 09:45:44:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0011
2022-01-12 09:46:58:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0014
2022-01-12 09:48:12:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0011
2022-01-12 09:49:26:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0012
2022-01-12 09:50:40:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0012
2022-01-12 09:51:54:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0011
2022-01-12 09:53:08:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0011
2022-01-12 09:54:22:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0011
2022-01-12 09:55:36:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0011
2022-01-12 09:56:56:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 09:56:56:INFO:	Num examples = 100
2022-01-12 09:56:56:INFO:	RMSE = 33.4502
2022-01-12 09:57:03:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 09:57:03:INFO:	Num examples = 100
2022-01-12 09:57:03:INFO:	RMSE = 21.0183
2022-01-12 09:57:03:INFO:==> Minimal valid RMSE!
2022-01-12 09:57:03:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 09:57:04:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0010
2022-01-12 09:58:18:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0009
2022-01-12 09:59:32:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0010
2022-01-12 10:00:46:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0009
2022-01-12 10:02:00:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0009
2022-01-12 10:03:14:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0008
2022-01-12 10:04:28:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0010
2022-01-12 10:05:42:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0009
2022-01-12 10:06:56:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0008
2022-01-12 10:08:10:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0008
2022-01-12 10:09:24:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0008
2022-01-12 10:10:38:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0008
2022-01-12 10:11:52:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0007
2022-01-12 10:13:06:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0006
2022-01-12 10:14:20:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0007
2022-01-12 10:15:40:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 10:15:40:INFO:	Num examples = 100
2022-01-12 10:15:40:INFO:	RMSE = 32.7893
2022-01-12 10:15:47:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 10:15:47:INFO:	Num examples = 100
2022-01-12 10:15:47:INFO:	RMSE = 17.7502
2022-01-12 10:15:47:INFO:==> Minimal valid RMSE!
2022-01-12 10:15:47:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 10:15:49:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0007
2022-01-12 10:17:03:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0006
2022-01-12 10:18:17:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0006
2022-01-12 10:19:31:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0007
2022-01-12 10:20:46:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0006
2022-01-12 10:21:59:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0006
2022-01-12 10:23:14:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0006
2022-01-12 10:24:27:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0006
2022-01-12 10:25:41:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0006
2022-01-12 10:26:55:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0006
2022-01-12 10:28:09:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0005
2022-01-12 10:29:23:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0005
2022-01-12 10:30:38:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0005
2022-01-12 10:31:53:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0005
2022-01-12 10:33:08:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0005
2022-01-12 10:34:29:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 10:34:29:INFO:	Num examples = 100
2022-01-12 10:34:29:INFO:	RMSE = 33.9679
2022-01-12 10:34:37:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 10:34:37:INFO:	Num examples = 100
2022-01-12 10:34:37:INFO:	RMSE = 18.3325
2022-01-12 10:34:38:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0005
2022-01-12 10:35:54:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0004
2022-01-12 10:37:09:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0004
2022-01-12 10:38:24:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0004
2022-01-12 10:39:40:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0004
2022-01-12 10:40:56:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0004
2022-01-12 10:42:11:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0004
2022-01-12 10:43:26:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0004
2022-01-12 10:44:41:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0004
2022-01-12 10:45:55:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0003
2022-01-12 10:47:10:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0004
2022-01-12 10:48:26:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0003
2022-01-12 10:49:41:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0004
2022-01-12 10:50:55:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0004
2022-01-12 10:52:10:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0004
2022-01-12 10:53:30:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 10:53:30:INFO:	Num examples = 100
2022-01-12 10:53:30:INFO:	RMSE = 32.9830
2022-01-12 10:53:37:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 10:53:37:INFO:	Num examples = 100
2022-01-12 10:53:37:INFO:	RMSE = 20.1117
2022-01-12 10:53:39:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0004
2022-01-12 10:54:54:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0003
2022-01-12 10:56:10:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0003
2022-01-12 10:57:25:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0004
2022-01-12 10:58:40:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0003
2022-01-12 10:59:55:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0003
2022-01-12 11:01:10:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0003
2022-01-12 11:02:25:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0003
2022-01-12 11:03:41:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0003
2022-01-12 11:04:56:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0003
2022-01-12 11:06:12:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0003
2022-01-12 11:07:27:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0003
2022-01-12 11:08:42:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0003
2022-01-12 11:09:57:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0003
2022-01-12 11:11:13:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0003
2022-01-12 11:12:34:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 11:12:34:INFO:	Num examples = 100
2022-01-12 11:12:34:INFO:	RMSE = 33.5932
2022-01-12 11:12:41:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 11:12:41:INFO:	Num examples = 100
2022-01-12 11:12:41:INFO:	RMSE = 17.6570
2022-01-12 11:12:41:INFO:==> Minimal valid RMSE!
2022-01-12 11:12:41:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-42...
2022-01-12 11:12:43:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0003
2022-01-12 11:13:58:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0002
2022-01-12 11:15:13:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0002
2022-01-12 11:16:27:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0002
2022-01-12 11:17:43:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0002
2022-01-12 11:18:57:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0002
2022-01-12 11:20:13:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0002
2022-01-12 11:21:28:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0002
2022-01-12 11:22:44:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0002
2022-01-12 11:23:59:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0002
2022-01-12 11:25:14:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0002
2022-01-12 11:26:29:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0002
2022-01-12 11:27:45:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0002
2022-01-12 11:29:01:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0002
2022-01-12 11:30:16:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0002
2022-01-12 11:31:38:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 11:31:38:INFO:	Num examples = 100
2022-01-12 11:31:38:INFO:	RMSE = 32.6725
2022-01-12 11:31:45:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 11:31:45:INFO:	Num examples = 100
2022-01-12 11:31:45:INFO:	RMSE = 19.7463
2022-01-12 11:31:47:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 11:33:02:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 11:34:17:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 11:35:31:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 11:36:47:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 11:38:02:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 11:39:17:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0002
2022-01-12 11:40:32:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 11:41:47:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 11:43:02:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 11:44:17:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 11:45:33:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 11:46:48:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 11:48:04:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 11:49:19:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 11:50:41:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 11:50:41:INFO:	Num examples = 100
2022-01-12 11:50:41:INFO:	RMSE = 32.5342
2022-01-12 11:50:48:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 11:50:48:INFO:	Num examples = 100
2022-01-12 11:50:48:INFO:	RMSE = 19.9903
2022-01-12 11:50:50:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0002
2022-01-12 11:52:05:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 11:53:20:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 11:54:36:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 11:55:51:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 11:57:06:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 11:58:22:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 11:59:38:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 12:00:53:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 12:02:08:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0001
2022-01-12 12:03:24:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0002
2022-01-12 12:04:39:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0002
2022-01-12 12:05:55:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0002
2022-01-12 12:07:10:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0001
2022-01-12 12:08:25:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0002
2022-01-12 12:09:47:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 12:09:47:INFO:	Num examples = 100
2022-01-12 12:09:47:INFO:	RMSE = 32.6393
2022-01-12 12:09:54:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 12:09:54:INFO:	Num examples = 100
2022-01-12 12:09:54:INFO:	RMSE = 19.8883
2022-01-12 12:09:56:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0002
2022-01-12 12:11:11:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0002
2022-01-12 12:12:27:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0001
2022-01-12 12:13:42:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-12 12:14:57:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0002
2022-01-12 12:16:13:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0001
2022-01-12 12:17:28:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0001
2022-01-12 12:18:44:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0001
2022-01-12 12:19:59:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0001
2022-01-12 12:21:14:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0001
2022-01-12 12:22:29:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0001
2022-01-12 12:23:45:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0001
2022-01-12 12:25:00:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0001
2022-01-12 12:26:15:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0002
2022-01-12 12:27:30:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0002
2022-01-12 12:28:50:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 12:28:50:INFO:	Num examples = 100
2022-01-12 12:28:50:INFO:	RMSE = 32.4241
2022-01-12 12:28:58:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 12:28:58:INFO:	Num examples = 100
2022-01-12 12:28:58:INFO:	RMSE = 21.4972
2022-01-12 12:28:59:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0001
2022-01-12 12:30:15:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0001
2022-01-12 12:31:30:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0002
2022-01-12 12:32:45:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0001
2022-01-12 12:34:01:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0002
2022-01-12 12:35:16:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0001
2022-01-12 12:36:33:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0001
2022-01-12 12:37:48:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0001
2022-01-12 12:39:04:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0001
2022-01-12 12:40:19:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0002
2022-01-12 12:41:34:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0002
2022-01-12 12:42:49:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0001
2022-01-12 12:44:05:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0001
2022-01-12 12:45:19:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0001
2022-01-12 12:46:35:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0002
2022-01-12 12:47:56:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 12:47:56:INFO:	Num examples = 100
2022-01-12 12:47:56:INFO:	RMSE = 32.2777
2022-01-12 12:48:04:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 12:48:04:INFO:	Num examples = 100
2022-01-12 12:48:04:INFO:	RMSE = 23.4205
2022-01-12 12:48:04:INFO:	Output TEST RMSE:	33.5932
2022-01-12 12:48:04:INFO:	VALID RMSEs:	24.9272	21.8471	21.0183	17.7502	18.3325	20.1117	17.6570	19.7463	19.9903	19.8883	21.4972	23.4205
2022-01-12 12:48:04:INFO:	TEST RMSEs:	32.8729	34.3853	33.4502	32.7893	33.9679	32.9830	33.5932	32.6725	32.5342	32.6393	32.4241	32.2777
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-12 12:48:09:INFO:Finish setting logger...
2022-01-12 12:48:09:INFO:==> Training/Evaluation parameters are:
2022-01-12 12:48:09:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42'
2022-01-12 12:48:09:INFO:	data_fn=1
2022-01-12 12:48:09:INFO:	datatest_fn=1
2022-01-12 12:48:09:INFO:	filter_kernel_size=1
2022-01-12 12:48:09:INFO:	override_data_cache=False
2022-01-12 12:48:09:INFO:	maxRUL=125
2022-01-12 12:48:09:INFO:	low_ratio=0.1
2022-01-12 12:48:09:INFO:	high_ratio=0.99
2022-01-12 12:48:09:INFO:	aug_ratio=150
2022-01-12 12:48:09:INFO:	noise_amplitude=0.01
2022-01-12 12:48:09:INFO:	modeltype='cnn1d'
2022-01-12 12:48:09:INFO:	max_seq_len=550
2022-01-12 12:48:09:INFO:	d_model=128
2022-01-12 12:48:09:INFO:	p_dropout=0.1
2022-01-12 12:48:09:INFO:	n_head=4
2022-01-12 12:48:09:INFO:	n_layer=2
2022-01-12 12:48:09:INFO:	dim_feedforward=512
2022-01-12 12:48:09:INFO:	e_dropout=0.1
2022-01-12 12:48:09:INFO:	activation='relu'
2022-01-12 12:48:09:INFO:	layer_norm=False
2022-01-12 12:48:09:INFO:	support_size=5
2022-01-12 12:48:09:INFO:	inner_steps=2
2022-01-12 12:48:09:INFO:	lr_inner=0.001
2022-01-12 12:48:09:INFO:	lr_meta=0.001
2022-01-12 12:48:09:INFO:	n_epochs=12
2022-01-12 12:48:09:INFO:	train_batch_size=20
2022-01-12 12:48:09:INFO:	eval_batch_size=1
2022-01-12 12:48:09:INFO:	lr=0.001
2022-01-12 12:48:09:INFO:	weight_decay=0.01
2022-01-12 12:48:09:INFO:	warmup_ratio=0.0
2022-01-12 12:48:09:INFO:	max_grad_norm=5.0
2022-01-12 12:48:09:INFO:	logging_steps=50
2022-01-12 12:48:09:INFO:	seed=42
2022-01-12 12:48:09:INFO:	gpu_id=0
2022-01-12 12:48:09:INFO:	do_train=True
2022-01-12 12:48:09:INFO:	do_eval=False
2022-01-12 12:48:09:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-12 12:48:09:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-12 12:48:09:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-12 12:48:09:INFO:	device=device(type='cuda'))
2022-01-12 12:48:09:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 12:48:09:INFO:==> Read data from data/train_FD001.txt...
2022-01-12 12:48:09:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 12:48:09:INFO:==> Min_max normalization...
2022-01-12 12:48:09:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 12:48:09:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 12:48:09:INFO:==> Read data from data/test_FD001.txt...
2022-01-12 12:48:09:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 12:48:09:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-12 12:48:09:INFO:	min_rul: 7, max_rul: 145
2022-01-12 12:48:09:INFO:==> Input length ratio of the [TEST] data:
2022-01-12 12:48:09:INFO:	min_ratio = 0.2067
2022-01-12 12:48:09:INFO:	max_ratio = 0.9667
2022-01-12 12:48:09:INFO:==> Min_max normalization...
2022-01-12 12:48:09:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 12:48:09:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 12:48:09:INFO:==> Computing Criterion...
2022-01-12 12:48:09:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-12 12:48:23:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-12 12:48:23:INFO:NumExpr defaulting to 8 threads.
2022-01-12 12:48:23:INFO:=============== Scheme: Meta Learning ===============
2022-01-12 12:48:23:INFO:	Num examples = 15000
2022-01-12 12:48:23:INFO:	Num epochs = 12
2022-01-12 12:48:23:INFO:	Batch size = 20
2022-01-12 12:48:23:INFO:	Total meta optimization steps = 9000
2022-01-12 12:48:23:INFO:	Total inner optimization steps = 18000
2022-01-12 12:48:32:INFO:==> Group parameters for optimization...
2022-01-12 12:48:32:INFO:    Parameters to update are:
2022-01-12 12:48:32:INFO:	conv1.0.weight
2022-01-12 12:48:32:INFO:	conv2.0.weight
2022-01-12 12:48:32:INFO:	conv3.0.weight
2022-01-12 12:48:32:INFO:	conv4.0.weight
2022-01-12 12:48:32:INFO:	conv5.0.weight
2022-01-12 12:48:32:INFO:	fc_1.0.weight
2022-01-12 12:48:32:INFO:	fc_1.0.bias
2022-01-12 12:48:32:INFO:	fc_2.weight
2022-01-12 12:48:32:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-12 12:48:35:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0010
2022-01-12 12:49:49:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0470
2022-01-12 12:51:03:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0257
2022-01-12 12:52:18:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0174
2022-01-12 12:53:33:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0090
2022-01-12 12:54:47:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0067
2022-01-12 12:56:00:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0060
2022-01-12 12:57:15:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0056
2022-01-12 12:58:28:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0045
2022-01-12 12:59:42:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0037
2022-01-12 13:00:55:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0034
2022-01-12 13:02:10:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0029
2022-01-12 13:03:24:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0024
2022-01-12 13:04:38:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0022
2022-01-12 13:05:53:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0019
2022-01-12 13:07:13:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 13:07:13:INFO:	Num examples = 100
2022-01-12 13:07:13:INFO:	RMSE = 32.8729
2022-01-12 13:07:21:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 13:07:21:INFO:	Num examples = 100
2022-01-12 13:07:21:INFO:	RMSE = 24.9272
2022-01-12 13:07:21:INFO:==> Minimal valid RMSE!
2022-01-12 13:07:21:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 13:07:22:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0021
2022-01-12 13:08:37:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0020
2022-01-12 13:09:51:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0020
2022-01-12 13:11:05:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0018
2022-01-12 13:12:19:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0017
2022-01-12 13:13:33:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0017
2022-01-12 13:14:47:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0018
2022-01-12 13:16:02:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0016
2022-01-12 13:17:17:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0015
2022-01-12 13:18:31:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0016
2022-01-12 13:19:45:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0015
2022-01-12 13:20:58:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0014
2022-01-12 13:22:11:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0014
2022-01-12 13:23:24:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0014
2022-01-12 13:24:38:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0013
2022-01-12 13:25:58:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 13:25:58:INFO:	Num examples = 100
2022-01-12 13:25:58:INFO:	RMSE = 34.3853
2022-01-12 13:26:06:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 13:26:06:INFO:	Num examples = 100
2022-01-12 13:26:06:INFO:	RMSE = 21.8471
2022-01-12 13:26:06:INFO:==> Minimal valid RMSE!
2022-01-12 13:26:06:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 13:26:07:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0014
2022-01-12 13:27:22:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0014
2022-01-12 13:28:35:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0013
2022-01-12 13:29:49:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0012
2022-01-12 13:31:03:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0013
2022-01-12 13:32:16:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0013
2022-01-12 13:33:30:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0011
2022-01-12 13:34:44:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0014
2022-01-12 13:35:58:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0011
2022-01-12 13:37:12:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0012
2022-01-12 13:38:26:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0012
2022-01-12 13:39:40:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0011
2022-01-12 13:40:55:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0011
2022-01-12 13:42:08:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0011
2022-01-12 13:43:22:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0011
2022-01-12 13:44:42:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 13:44:42:INFO:	Num examples = 100
2022-01-12 13:44:42:INFO:	RMSE = 33.4502
2022-01-12 13:44:50:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 13:44:50:INFO:	Num examples = 100
2022-01-12 13:44:50:INFO:	RMSE = 21.0183
2022-01-12 13:44:50:INFO:==> Minimal valid RMSE!
2022-01-12 13:44:50:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 13:44:51:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0010
2022-01-12 13:46:05:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0009
2022-01-12 13:47:19:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0010
2022-01-12 13:48:32:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0009
2022-01-12 13:49:46:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0009
2022-01-12 13:51:00:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0008
2022-01-12 13:52:14:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0010
2022-01-12 13:53:28:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0009
2022-01-12 13:54:42:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0008
2022-01-12 13:55:56:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0008
2022-01-12 13:57:10:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0008
2022-01-12 13:58:24:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0008
2022-01-12 13:59:38:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0007
2022-01-12 14:00:52:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0006
2022-01-12 14:02:05:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0007
2022-01-12 14:03:25:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 14:03:25:INFO:	Num examples = 100
2022-01-12 14:03:25:INFO:	RMSE = 32.7893
2022-01-12 14:03:32:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 14:03:32:INFO:	Num examples = 100
2022-01-12 14:03:32:INFO:	RMSE = 17.7502
2022-01-12 14:03:32:INFO:==> Minimal valid RMSE!
2022-01-12 14:03:32:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 14:03:33:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0007
2022-01-12 14:04:47:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0006
2022-01-12 14:06:01:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0006
2022-01-12 14:07:16:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0007
2022-01-12 14:08:30:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0006
2022-01-12 14:09:45:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0006
2022-01-12 14:10:59:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0006
2022-01-12 14:12:14:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0006
2022-01-12 14:13:28:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0006
2022-01-12 14:14:41:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0006
2022-01-12 14:15:55:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0005
2022-01-12 14:17:09:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0005
2022-01-12 14:18:24:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0005
2022-01-12 14:19:39:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0005
2022-01-12 14:20:53:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0005
2022-01-12 14:22:14:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 14:22:14:INFO:	Num examples = 100
2022-01-12 14:22:14:INFO:	RMSE = 33.9679
2022-01-12 14:22:21:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 14:22:21:INFO:	Num examples = 100
2022-01-12 14:22:21:INFO:	RMSE = 18.3325
2022-01-12 14:22:23:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0005
2022-01-12 14:23:37:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0004
2022-01-12 14:24:51:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0004
2022-01-12 14:26:04:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0004
2022-01-12 14:27:17:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0004
2022-01-12 14:28:31:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0004
2022-01-12 14:29:45:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0004
2022-01-12 14:31:00:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0004
2022-01-12 14:32:14:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0004
2022-01-12 14:33:29:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0003
2022-01-12 14:34:43:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0004
2022-01-12 14:35:58:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0003
2022-01-12 14:37:11:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0004
2022-01-12 14:38:25:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0004
2022-01-12 14:39:38:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0004
2022-01-12 14:40:59:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 14:40:59:INFO:	Num examples = 100
2022-01-12 14:40:59:INFO:	RMSE = 32.9830
2022-01-12 14:41:06:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 14:41:06:INFO:	Num examples = 100
2022-01-12 14:41:06:INFO:	RMSE = 20.1117
2022-01-12 14:41:08:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0004
2022-01-12 14:42:22:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0003
2022-01-12 14:43:37:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0003
2022-01-12 14:44:51:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0004
2022-01-12 14:46:05:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0003
2022-01-12 14:47:19:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0003
2022-01-12 14:48:33:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0003
2022-01-12 14:49:47:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0003
2022-01-12 14:51:01:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0003
2022-01-12 14:52:15:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0003
2022-01-12 14:53:28:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0003
2022-01-12 14:54:42:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0003
2022-01-12 14:55:55:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0003
2022-01-12 14:57:08:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0003
2022-01-12 14:58:23:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0003
2022-01-12 14:59:43:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 14:59:43:INFO:	Num examples = 100
2022-01-12 14:59:43:INFO:	RMSE = 33.5932
2022-01-12 14:59:50:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 14:59:50:INFO:	Num examples = 100
2022-01-12 14:59:50:INFO:	RMSE = 17.6570
2022-01-12 14:59:50:INFO:==> Minimal valid RMSE!
2022-01-12 14:59:50:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-42...
2022-01-12 14:59:52:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0003
2022-01-12 15:01:07:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0002
2022-01-12 15:02:22:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0002
2022-01-12 15:03:37:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0002
2022-01-12 15:04:52:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0002
2022-01-12 15:06:07:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0002
2022-01-12 15:07:21:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0002
2022-01-12 15:08:35:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0002
2022-01-12 15:09:50:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0002
2022-01-12 15:11:04:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0002
2022-01-12 15:12:18:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0002
2022-01-12 15:13:31:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0002
2022-01-12 15:14:51:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0002
2022-01-12 15:16:15:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0002
2022-01-12 15:17:28:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0002
2022-01-12 15:18:48:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 15:18:48:INFO:	Num examples = 100
2022-01-12 15:18:48:INFO:	RMSE = 32.6725
2022-01-12 15:18:56:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 15:18:56:INFO:	Num examples = 100
2022-01-12 15:18:56:INFO:	RMSE = 19.7463
2022-01-12 15:18:57:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 15:20:11:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 15:21:25:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 15:22:38:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 15:23:52:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 15:25:06:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 15:26:19:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0002
2022-01-12 15:27:32:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 15:28:45:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 15:29:59:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 15:31:13:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 15:32:28:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 15:33:45:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 15:34:58:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 15:36:12:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 15:37:32:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 15:37:32:INFO:	Num examples = 100
2022-01-12 15:37:32:INFO:	RMSE = 32.5342
2022-01-12 15:37:39:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 15:37:39:INFO:	Num examples = 100
2022-01-12 15:37:39:INFO:	RMSE = 19.9903
2022-01-12 15:37:41:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0002
2022-01-12 15:38:56:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 15:40:09:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 15:41:23:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 15:42:37:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 15:43:51:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 15:45:05:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 15:46:20:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 15:47:34:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 15:48:49:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0001
2022-01-12 15:50:05:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0002
2022-01-12 15:51:19:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0002
2022-01-12 15:52:34:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0002
2022-01-12 15:53:48:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0001
2022-01-12 15:55:01:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0002
2022-01-12 15:56:22:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 15:56:22:INFO:	Num examples = 100
2022-01-12 15:56:22:INFO:	RMSE = 32.6393
2022-01-12 15:56:29:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 15:56:29:INFO:	Num examples = 100
2022-01-12 15:56:29:INFO:	RMSE = 19.8883
2022-01-12 15:56:30:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0002
2022-01-12 15:57:44:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0002
2022-01-12 15:58:59:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0001
2022-01-12 16:00:14:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-12 16:01:28:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0002
2022-01-12 16:02:43:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0001
2022-01-12 16:04:01:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0001
2022-01-12 16:05:16:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0001
2022-01-12 16:06:31:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0001
2022-01-12 16:07:45:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0001
2022-01-12 16:09:00:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0001
2022-01-12 16:10:15:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0001
2022-01-12 16:11:30:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0001
2022-01-12 16:12:45:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0002
2022-01-12 16:13:59:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0002
2022-01-12 16:15:20:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 16:15:20:INFO:	Num examples = 100
2022-01-12 16:15:20:INFO:	RMSE = 32.4241
2022-01-12 16:15:27:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 16:15:27:INFO:	Num examples = 100
2022-01-12 16:15:27:INFO:	RMSE = 21.4972
2022-01-12 16:15:29:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0001
2022-01-12 16:16:43:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0001
2022-01-12 16:17:57:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0002
2022-01-12 16:19:11:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0001
2022-01-12 16:20:25:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0002
2022-01-12 16:21:41:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0001
2022-01-12 16:22:56:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0001
2022-01-12 16:24:10:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0001
2022-01-12 16:25:24:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0001
2022-01-12 16:26:38:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0002
2022-01-12 16:27:51:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0002
2022-01-12 16:29:05:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0001
2022-01-12 16:30:18:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0001
2022-01-12 16:31:32:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0001
2022-01-12 16:32:46:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0002
2022-01-12 16:34:05:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 16:34:05:INFO:	Num examples = 100
2022-01-12 16:34:05:INFO:	RMSE = 32.2777
2022-01-12 16:34:12:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 16:34:12:INFO:	Num examples = 100
2022-01-12 16:34:12:INFO:	RMSE = 23.4205
2022-01-12 16:34:12:INFO:	Output TEST RMSE:	33.5932
2022-01-12 16:34:12:INFO:	VALID RMSEs:	24.9272	21.8471	21.0183	17.7502	18.3325	20.1117	17.6570	19.7463	19.9903	19.8883	21.4972	23.4205
2022-01-12 16:34:12:INFO:	TEST RMSEs:	32.8729	34.3853	33.4502	32.7893	33.9679	32.9830	33.5932	32.6725	32.5342	32.6393	32.4241	32.2777
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-12 16:34:24:INFO:Finish setting logger...
2022-01-12 16:34:24:INFO:==> Training/Evaluation parameters are:
2022-01-12 16:34:24:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667'
2022-01-12 16:34:24:INFO:	data_fn=1
2022-01-12 16:34:24:INFO:	datatest_fn=1
2022-01-12 16:34:24:INFO:	filter_kernel_size=1
2022-01-12 16:34:24:INFO:	override_data_cache=False
2022-01-12 16:34:24:INFO:	maxRUL=125
2022-01-12 16:34:24:INFO:	low_ratio=0.1
2022-01-12 16:34:24:INFO:	high_ratio=0.99
2022-01-12 16:34:24:INFO:	aug_ratio=150
2022-01-12 16:34:24:INFO:	noise_amplitude=0.01
2022-01-12 16:34:24:INFO:	modeltype='cnn1d'
2022-01-12 16:34:24:INFO:	max_seq_len=550
2022-01-12 16:34:24:INFO:	d_model=128
2022-01-12 16:34:24:INFO:	p_dropout=0.1
2022-01-12 16:34:24:INFO:	n_head=4
2022-01-12 16:34:24:INFO:	n_layer=2
2022-01-12 16:34:24:INFO:	dim_feedforward=512
2022-01-12 16:34:24:INFO:	e_dropout=0.1
2022-01-12 16:34:24:INFO:	activation='relu'
2022-01-12 16:34:24:INFO:	layer_norm=False
2022-01-12 16:34:24:INFO:	support_size=2
2022-01-12 16:34:24:INFO:	inner_steps=2
2022-01-12 16:34:24:INFO:	lr_inner=0.0001
2022-01-12 16:34:24:INFO:	lr_meta=0.001
2022-01-12 16:34:24:INFO:	n_epochs=12
2022-01-12 16:34:24:INFO:	train_batch_size=20
2022-01-12 16:34:24:INFO:	eval_batch_size=1
2022-01-12 16:34:24:INFO:	lr=0.001
2022-01-12 16:34:24:INFO:	weight_decay=0.01
2022-01-12 16:34:24:INFO:	warmup_ratio=0.0
2022-01-12 16:34:24:INFO:	max_grad_norm=5.0
2022-01-12 16:34:24:INFO:	logging_steps=50
2022-01-12 16:34:24:INFO:	seed=667
2022-01-12 16:34:24:INFO:	gpu_id=0
2022-01-12 16:34:24:INFO:	do_train=True
2022-01-12 16:34:24:INFO:	do_eval=False
2022-01-12 16:34:24:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-12 16:34:24:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-12 16:34:24:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-12 16:34:24:INFO:	device=device(type='cuda'))
2022-01-12 16:34:24:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-12 16:34:24:INFO:==> Read data from data/train_FD001.txt...
2022-01-12 16:34:24:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 16:34:24:INFO:==> Min_max normalization...
2022-01-12 16:34:24:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 16:34:24:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 16:34:24:INFO:==> Read data from data/test_FD001.txt...
2022-01-12 16:34:24:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 16:34:24:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-12 16:34:24:INFO:	min_rul: 7, max_rul: 145
2022-01-12 16:34:24:INFO:==> Input length ratio of the [TEST] data:
2022-01-12 16:34:24:INFO:	min_ratio = 0.2067
2022-01-12 16:34:24:INFO:	max_ratio = 0.9667
2022-01-12 16:34:24:INFO:==> Min_max normalization...
2022-01-12 16:34:24:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 16:34:24:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 16:34:24:INFO:==> Computing Criterion...
2022-01-12 16:34:24:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-12 16:35:03:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-12 16:35:03:INFO:NumExpr defaulting to 8 threads.
2022-01-12 16:35:03:INFO:=============== Scheme: Meta Learning ===============
2022-01-12 16:35:03:INFO:	Num examples = 15000
2022-01-12 16:35:03:INFO:	Num epochs = 12
2022-01-12 16:35:03:INFO:	Batch size = 20
2022-01-12 16:35:03:INFO:	Total meta optimization steps = 9000
2022-01-12 16:35:03:INFO:	Total inner optimization steps = 18000
2022-01-12 16:35:12:INFO:==> Group parameters for optimization...
2022-01-12 16:35:12:INFO:    Parameters to update are:
2022-01-12 16:35:12:INFO:	conv1.0.weight
2022-01-12 16:35:12:INFO:	conv2.0.weight
2022-01-12 16:35:12:INFO:	conv3.0.weight
2022-01-12 16:35:12:INFO:	conv4.0.weight
2022-01-12 16:35:12:INFO:	conv5.0.weight
2022-01-12 16:35:12:INFO:	fc_1.0.weight
2022-01-12 16:35:12:INFO:	fc_1.0.bias
2022-01-12 16:35:12:INFO:	fc_2.weight
2022-01-12 16:35:12:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-12 16:35:15:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0007
2022-01-12 16:36:30:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0128
2022-01-12 16:37:45:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0079
2022-01-12 16:39:00:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0054
2022-01-12 16:40:16:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0046
2022-01-12 16:41:31:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0040
2022-01-12 16:42:47:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0036
2022-01-12 16:44:02:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0035
2022-01-12 16:45:17:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0030
2022-01-12 16:46:31:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0029
2022-01-12 16:47:45:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0027
2022-01-12 16:48:59:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0032
2022-01-12 16:50:14:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0030
2022-01-12 16:51:28:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0024
2022-01-12 16:52:41:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0022
2022-01-12 16:54:01:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 16:54:01:INFO:	Num examples = 100
2022-01-12 16:54:01:INFO:	RMSE = 32.4726
2022-01-12 16:54:08:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 16:54:08:INFO:	Num examples = 100
2022-01-12 16:54:08:INFO:	RMSE = 24.1175
2022-01-12 16:54:08:INFO:==> Minimal valid RMSE!
2022-01-12 16:54:08:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-12 16:54:10:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0023
2022-01-12 16:55:24:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0019
2022-01-12 16:56:37:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0020
2022-01-12 16:57:51:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0017
2022-01-12 16:59:05:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0016
2022-01-12 17:00:18:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0019
2022-01-12 17:01:31:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0016
2022-01-12 17:02:48:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0016
2022-01-12 17:04:01:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0016
2022-01-12 17:05:16:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0016
2022-01-12 17:06:29:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0014
2022-01-12 17:07:43:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0014
2022-01-12 17:08:56:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0013
2022-01-12 17:10:10:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0015
2022-01-12 17:11:24:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0014
2022-01-12 17:12:45:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 17:12:45:INFO:	Num examples = 100
2022-01-12 17:12:45:INFO:	RMSE = 32.3454
2022-01-12 17:12:53:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 17:12:53:INFO:	Num examples = 100
2022-01-12 17:12:53:INFO:	RMSE = 22.0564
2022-01-12 17:12:53:INFO:==> Minimal valid RMSE!
2022-01-12 17:12:53:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-12 17:12:55:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0014
2022-01-12 17:14:30:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0013
2022-01-12 17:16:06:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0012
2022-01-12 17:17:41:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0012
2022-01-12 17:19:17:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0011
2022-01-12 17:20:54:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0012
2022-01-12 17:22:36:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0011
2022-01-12 17:24:12:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0011
2022-01-12 17:25:48:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0011
2022-01-12 17:27:10:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0010
2022-01-12 17:28:22:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0011
2022-01-12 17:29:35:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0010
2022-01-12 17:30:48:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0010
2022-01-12 17:32:01:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0010
2022-01-12 17:33:15:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0009
2022-01-12 17:34:35:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 17:34:35:INFO:	Num examples = 100
2022-01-12 17:34:35:INFO:	RMSE = 34.1089
2022-01-12 17:34:42:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 17:34:42:INFO:	Num examples = 100
2022-01-12 17:34:42:INFO:	RMSE = 24.0981
2022-01-12 17:34:44:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0012
2022-01-12 17:35:58:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0010
2022-01-12 17:37:11:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0011
2022-01-12 17:38:26:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0010
2022-01-12 17:39:42:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0009
2022-01-12 17:40:56:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0009
2022-01-12 17:42:10:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0010
2022-01-12 17:43:25:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0010
2022-01-12 17:44:41:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0008
2022-01-12 17:45:56:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0008
2022-01-12 17:47:12:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0010
2022-01-12 17:48:28:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0009
2022-01-12 17:49:43:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0008
2022-01-12 17:50:58:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0008
2022-01-12 17:52:11:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0008
2022-01-12 17:53:31:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 17:53:31:INFO:	Num examples = 100
2022-01-12 17:53:31:INFO:	RMSE = 34.6450
2022-01-12 17:53:38:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 17:53:38:INFO:	Num examples = 100
2022-01-12 17:53:38:INFO:	RMSE = 19.8531
2022-01-12 17:53:38:INFO:==> Minimal valid RMSE!
2022-01-12 17:53:38:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-12 17:53:39:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0008
2022-01-12 17:54:53:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0008
2022-01-12 17:56:05:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0007
2022-01-12 17:57:19:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0008
2022-01-12 17:58:31:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0007
2022-01-12 17:59:45:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0008
2022-01-12 18:00:58:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0007
2022-01-12 18:02:11:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0007
2022-01-12 18:03:25:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0007
2022-01-12 18:04:39:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0007
2022-01-12 18:05:53:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0007
2022-01-12 18:07:09:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0007
2022-01-12 18:08:25:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0007
2022-01-12 18:09:40:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0007
2022-01-12 18:10:55:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0006
2022-01-12 18:12:17:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 18:12:17:INFO:	Num examples = 100
2022-01-12 18:12:17:INFO:	RMSE = 33.5648
2022-01-12 18:12:24:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 18:12:24:INFO:	Num examples = 100
2022-01-12 18:12:24:INFO:	RMSE = 18.1257
2022-01-12 18:12:24:INFO:==> Minimal valid RMSE!
2022-01-12 18:12:24:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-12 18:12:26:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0006
2022-01-12 18:13:42:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0005
2022-01-12 18:14:57:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0006
2022-01-12 18:16:12:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0006
2022-01-12 18:17:28:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0006
2022-01-12 18:18:44:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0005
2022-01-12 18:19:59:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0005
2022-01-12 18:21:15:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0005
2022-01-12 18:22:31:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0005
2022-01-12 18:23:45:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0005
2022-01-12 18:24:59:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0005
2022-01-12 18:26:15:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0005
2022-01-12 18:27:50:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0005
2022-01-12 18:29:50:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0005
2022-01-12 18:32:12:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0004
2022-01-12 18:34:46:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 18:34:46:INFO:	Num examples = 100
2022-01-12 18:34:46:INFO:	RMSE = 34.9426
2022-01-12 18:35:01:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 18:35:01:INFO:	Num examples = 100
2022-01-12 18:35:01:INFO:	RMSE = 20.8837
2022-01-12 18:35:04:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0005
2022-01-12 18:37:30:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0004
2022-01-12 18:39:52:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0004
2022-01-12 18:41:52:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0004
2022-01-12 18:43:53:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0004
2022-01-12 18:45:54:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0004
2022-01-12 18:47:55:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0004
2022-01-12 18:49:57:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0004
2022-01-12 18:51:57:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0004
2022-01-12 18:53:58:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0003
2022-01-12 18:55:59:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0004
2022-01-12 18:58:00:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0003
2022-01-12 19:00:03:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0004
2022-01-12 19:02:04:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0003
2022-01-12 19:03:43:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0003
2022-01-12 19:05:27:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 19:05:27:INFO:	Num examples = 100
2022-01-12 19:05:27:INFO:	RMSE = 34.6140
2022-01-12 19:05:37:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 19:05:37:INFO:	Num examples = 100
2022-01-12 19:05:37:INFO:	RMSE = 18.5714
2022-01-12 19:05:39:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0003
2022-01-12 19:07:16:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0003
2022-01-12 19:08:53:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0003
2022-01-12 19:10:30:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0003
2022-01-12 19:12:06:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0003
2022-01-12 19:13:43:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0003
2022-01-12 19:15:20:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0003
2022-01-12 19:16:57:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0003
2022-01-12 19:18:34:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0003
2022-01-12 19:20:11:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0003
2022-01-12 19:21:48:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0003
2022-01-12 19:23:25:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0003
2022-01-12 19:25:02:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0002
2022-01-12 19:26:40:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0003
2022-01-12 19:28:16:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0003
2022-01-12 19:30:00:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 19:30:00:INFO:	Num examples = 100
2022-01-12 19:30:00:INFO:	RMSE = 34.4787
2022-01-12 19:30:10:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 19:30:10:INFO:	Num examples = 100
2022-01-12 19:30:10:INFO:	RMSE = 19.8416
2022-01-12 19:30:12:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 19:31:48:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 19:33:25:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 19:35:01:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 19:36:39:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 19:38:16:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 19:39:52:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0002
2022-01-12 19:41:29:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 19:43:05:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 19:44:41:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 19:46:17:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 19:47:52:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 19:49:30:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 19:51:06:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 19:52:43:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 19:54:27:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 19:54:27:INFO:	Num examples = 100
2022-01-12 19:54:27:INFO:	RMSE = 34.2513
2022-01-12 19:54:36:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 19:54:36:INFO:	Num examples = 100
2022-01-12 19:54:36:INFO:	RMSE = 20.5156
2022-01-12 19:54:38:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0002
2022-01-12 19:56:15:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 19:57:51:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 19:59:28:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 20:01:04:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 20:02:42:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 20:04:18:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 20:05:54:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 20:07:30:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 20:09:08:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0002
2022-01-12 20:10:44:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0002
2022-01-12 20:12:20:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0002
2022-01-12 20:13:58:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0002
2022-01-12 20:15:34:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0002
2022-01-12 20:17:11:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0002
2022-01-12 20:18:56:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 20:18:56:INFO:	Num examples = 100
2022-01-12 20:18:56:INFO:	RMSE = 33.9713
2022-01-12 20:19:05:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 20:19:05:INFO:	Num examples = 100
2022-01-12 20:19:05:INFO:	RMSE = 22.7708
2022-01-12 20:19:07:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0001
2022-01-12 20:20:27:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0002
2022-01-12 20:21:42:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0001
2022-01-12 20:22:57:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-12 20:24:12:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0002
2022-01-12 20:25:27:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0001
2022-01-12 20:26:41:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0002
2022-01-12 20:27:56:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0001
2022-01-12 20:29:11:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0002
2022-01-12 20:30:26:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0002
2022-01-12 20:31:42:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0001
2022-01-12 20:32:58:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0001
2022-01-12 20:34:14:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0001
2022-01-12 20:35:29:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0002
2022-01-12 20:36:44:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0001
2022-01-12 20:38:06:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 20:38:06:INFO:	Num examples = 100
2022-01-12 20:38:06:INFO:	RMSE = 33.9150
2022-01-12 20:38:13:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 20:38:13:INFO:	Num examples = 100
2022-01-12 20:38:13:INFO:	RMSE = 25.0053
2022-01-12 20:38:14:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0001
2022-01-12 20:39:29:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0001
2022-01-12 20:40:44:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0001
2022-01-12 20:41:59:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0001
2022-01-12 20:43:14:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0001
2022-01-12 20:44:31:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0001
2022-01-12 20:45:46:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0002
2022-01-12 20:47:02:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0001
2022-01-12 20:48:17:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0001
2022-01-12 20:49:32:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0001
2022-01-12 20:50:48:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0001
2022-01-12 20:52:03:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0001
2022-01-12 20:53:19:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0001
2022-01-12 20:54:34:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0001
2022-01-12 20:55:49:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0001
2022-01-12 20:57:10:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 20:57:10:INFO:	Num examples = 100
2022-01-12 20:57:10:INFO:	RMSE = 33.8457
2022-01-12 20:57:18:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 20:57:18:INFO:	Num examples = 100
2022-01-12 20:57:18:INFO:	RMSE = 26.5859
2022-01-12 20:57:18:INFO:	Output TEST RMSE:	33.5648
2022-01-12 20:57:18:INFO:	VALID RMSEs:	24.1175	22.0564	24.0981	19.8531	18.1257	20.8837	18.5714	19.8416	20.5156	22.7708	25.0053	26.5859
2022-01-12 20:57:18:INFO:	TEST RMSEs:	32.4726	32.3454	34.1089	34.6450	33.5648	34.9426	34.6140	34.4787	34.2513	33.9713	33.9150	33.8457
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-12 20:57:22:INFO:Finish setting logger...
2022-01-12 20:57:22:INFO:==> Training/Evaluation parameters are:
2022-01-12 20:57:22:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667'
2022-01-12 20:57:22:INFO:	data_fn=1
2022-01-12 20:57:22:INFO:	datatest_fn=1
2022-01-12 20:57:22:INFO:	filter_kernel_size=1
2022-01-12 20:57:22:INFO:	override_data_cache=False
2022-01-12 20:57:22:INFO:	maxRUL=125
2022-01-12 20:57:22:INFO:	low_ratio=0.1
2022-01-12 20:57:22:INFO:	high_ratio=0.99
2022-01-12 20:57:22:INFO:	aug_ratio=150
2022-01-12 20:57:22:INFO:	noise_amplitude=0.01
2022-01-12 20:57:22:INFO:	modeltype='cnn1d'
2022-01-12 20:57:22:INFO:	max_seq_len=550
2022-01-12 20:57:22:INFO:	d_model=128
2022-01-12 20:57:22:INFO:	p_dropout=0.1
2022-01-12 20:57:22:INFO:	n_head=4
2022-01-12 20:57:22:INFO:	n_layer=2
2022-01-12 20:57:22:INFO:	dim_feedforward=512
2022-01-12 20:57:22:INFO:	e_dropout=0.1
2022-01-12 20:57:22:INFO:	activation='relu'
2022-01-12 20:57:22:INFO:	layer_norm=False
2022-01-12 20:57:22:INFO:	support_size=2
2022-01-12 20:57:22:INFO:	inner_steps=2
2022-01-12 20:57:22:INFO:	lr_inner=0.001
2022-01-12 20:57:22:INFO:	lr_meta=0.001
2022-01-12 20:57:22:INFO:	n_epochs=12
2022-01-12 20:57:22:INFO:	train_batch_size=20
2022-01-12 20:57:22:INFO:	eval_batch_size=1
2022-01-12 20:57:22:INFO:	lr=0.001
2022-01-12 20:57:22:INFO:	weight_decay=0.01
2022-01-12 20:57:22:INFO:	warmup_ratio=0.0
2022-01-12 20:57:22:INFO:	max_grad_norm=5.0
2022-01-12 20:57:22:INFO:	logging_steps=50
2022-01-12 20:57:22:INFO:	seed=667
2022-01-12 20:57:22:INFO:	gpu_id=0
2022-01-12 20:57:22:INFO:	do_train=True
2022-01-12 20:57:22:INFO:	do_eval=False
2022-01-12 20:57:22:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-12 20:57:22:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-12 20:57:22:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-12 20:57:22:INFO:	device=device(type='cuda'))
2022-01-12 20:57:22:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667...
2022-01-12 20:57:22:INFO:==> Read data from data/train_FD001.txt...
2022-01-12 20:57:22:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 20:57:22:INFO:==> Min_max normalization...
2022-01-12 20:57:22:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 20:57:22:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 20:57:22:INFO:==> Read data from data/test_FD001.txt...
2022-01-12 20:57:22:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-12 20:57:22:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-12 20:57:22:INFO:	min_rul: 7, max_rul: 145
2022-01-12 20:57:22:INFO:==> Input length ratio of the [TEST] data:
2022-01-12 20:57:22:INFO:	min_ratio = 0.2067
2022-01-12 20:57:22:INFO:	max_ratio = 0.9667
2022-01-12 20:57:22:INFO:==> Min_max normalization...
2022-01-12 20:57:22:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-12 20:57:22:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-12 20:57:22:INFO:==> Computing Criterion...
2022-01-12 20:57:22:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-12 20:57:30:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-12 20:57:30:INFO:NumExpr defaulting to 8 threads.
2022-01-12 20:57:30:INFO:=============== Scheme: Meta Learning ===============
2022-01-12 20:57:30:INFO:	Num examples = 15000
2022-01-12 20:57:30:INFO:	Num epochs = 12
2022-01-12 20:57:30:INFO:	Batch size = 20
2022-01-12 20:57:30:INFO:	Total meta optimization steps = 9000
2022-01-12 20:57:30:INFO:	Total inner optimization steps = 18000
2022-01-12 20:57:38:INFO:==> Group parameters for optimization...
2022-01-12 20:57:38:INFO:    Parameters to update are:
2022-01-12 20:57:38:INFO:	conv1.0.weight
2022-01-12 20:57:38:INFO:	conv2.0.weight
2022-01-12 20:57:38:INFO:	conv3.0.weight
2022-01-12 20:57:38:INFO:	conv4.0.weight
2022-01-12 20:57:38:INFO:	conv5.0.weight
2022-01-12 20:57:38:INFO:	fc_1.0.weight
2022-01-12 20:57:38:INFO:	fc_1.0.bias
2022-01-12 20:57:38:INFO:	fc_2.weight
2022-01-12 20:57:38:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-12 20:57:41:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0007
2022-01-12 20:58:56:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0128
2022-01-12 21:00:13:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0079
2022-01-12 21:01:28:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0054
2022-01-12 21:02:43:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0046
2022-01-12 21:03:58:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0040
2022-01-12 21:05:13:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0036
2022-01-12 21:06:28:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0035
2022-01-12 21:07:43:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0030
2022-01-12 21:08:59:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0029
2022-01-12 21:10:15:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0027
2022-01-12 21:11:30:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0032
2022-01-12 21:12:45:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0030
2022-01-12 21:14:02:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0024
2022-01-12 21:15:17:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0022
2022-01-12 21:16:39:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 21:16:39:INFO:	Num examples = 100
2022-01-12 21:16:39:INFO:	RMSE = 32.4726
2022-01-12 21:16:46:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 21:16:46:INFO:	Num examples = 100
2022-01-12 21:16:46:INFO:	RMSE = 24.1175
2022-01-12 21:16:46:INFO:==> Minimal valid RMSE!
2022-01-12 21:16:46:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667...
2022-01-12 21:16:47:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0023
2022-01-12 21:18:03:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0019
2022-01-12 21:19:18:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0020
2022-01-12 21:20:34:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0017
2022-01-12 21:21:49:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0016
2022-01-12 21:23:04:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0019
2022-01-12 21:24:20:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0016
2022-01-12 21:25:35:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0016
2022-01-12 21:26:51:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0016
2022-01-12 21:28:06:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0016
2022-01-12 21:29:21:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0014
2022-01-12 21:30:36:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0014
2022-01-12 21:31:52:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0013
2022-01-12 21:33:07:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0015
2022-01-12 21:34:23:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0014
2022-01-12 21:35:44:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 21:35:44:INFO:	Num examples = 100
2022-01-12 21:35:44:INFO:	RMSE = 32.3454
2022-01-12 21:35:51:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 21:35:51:INFO:	Num examples = 100
2022-01-12 21:35:51:INFO:	RMSE = 22.0564
2022-01-12 21:35:51:INFO:==> Minimal valid RMSE!
2022-01-12 21:35:51:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667...
2022-01-12 21:35:52:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0014
2022-01-12 21:37:08:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0013
2022-01-12 21:38:23:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0012
2022-01-12 21:39:38:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0012
2022-01-12 21:40:53:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0011
2022-01-12 21:42:08:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0012
2022-01-12 21:43:23:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0011
2022-01-12 21:44:39:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0011
2022-01-12 21:45:53:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0011
2022-01-12 21:47:09:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0010
2022-01-12 21:48:24:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0011
2022-01-12 21:49:40:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0010
2022-01-12 21:50:55:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0010
2022-01-12 21:52:10:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0010
2022-01-12 21:53:26:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0009
2022-01-12 21:54:47:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 21:54:47:INFO:	Num examples = 100
2022-01-12 21:54:47:INFO:	RMSE = 34.1089
2022-01-12 21:54:54:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 21:54:54:INFO:	Num examples = 100
2022-01-12 21:54:54:INFO:	RMSE = 24.0981
2022-01-12 21:54:55:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0012
2022-01-12 21:56:10:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0010
2022-01-12 21:57:26:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0011
2022-01-12 21:58:42:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0010
2022-01-12 21:59:57:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0009
2022-01-12 22:01:12:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0009
2022-01-12 22:02:28:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0010
2022-01-12 22:03:44:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0010
2022-01-12 22:04:59:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0008
2022-01-12 22:06:15:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0008
2022-01-12 22:07:30:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0010
2022-01-12 22:08:44:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0009
2022-01-12 22:09:59:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0008
2022-01-12 22:11:12:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0008
2022-01-12 22:12:25:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0008
2022-01-12 22:13:45:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 22:13:45:INFO:	Num examples = 100
2022-01-12 22:13:45:INFO:	RMSE = 34.6450
2022-01-12 22:13:52:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 22:13:52:INFO:	Num examples = 100
2022-01-12 22:13:52:INFO:	RMSE = 19.8531
2022-01-12 22:13:52:INFO:==> Minimal valid RMSE!
2022-01-12 22:13:52:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667...
2022-01-12 22:13:54:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0008
2022-01-12 22:15:08:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0008
2022-01-12 22:16:23:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0007
2022-01-12 22:17:36:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0008
2022-01-12 22:18:50:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0007
2022-01-12 22:20:04:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0008
2022-01-12 22:21:19:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0007
2022-01-12 22:22:34:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0007
2022-01-12 22:23:48:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0007
2022-01-12 22:25:02:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0007
2022-01-12 22:26:16:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0007
2022-01-12 22:27:31:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0007
2022-01-12 22:28:46:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0007
2022-01-12 22:30:00:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0007
2022-01-12 22:31:14:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0006
2022-01-12 22:32:34:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 22:32:34:INFO:	Num examples = 100
2022-01-12 22:32:34:INFO:	RMSE = 33.5648
2022-01-12 22:32:41:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 22:32:41:INFO:	Num examples = 100
2022-01-12 22:32:41:INFO:	RMSE = 18.1257
2022-01-12 22:32:41:INFO:==> Minimal valid RMSE!
2022-01-12 22:32:41:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667...
2022-01-12 22:32:43:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0006
2022-01-12 22:33:56:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0005
2022-01-12 22:35:11:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0006
2022-01-12 22:36:24:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0006
2022-01-12 22:37:38:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0006
2022-01-12 22:38:51:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0005
2022-01-12 22:40:06:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0005
2022-01-12 22:41:20:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0005
2022-01-12 22:42:34:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0005
2022-01-12 22:43:48:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0005
2022-01-12 22:45:02:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0005
2022-01-12 22:46:16:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0005
2022-01-12 22:47:30:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0005
2022-01-12 22:48:44:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0005
2022-01-12 22:49:58:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0004
2022-01-12 22:51:17:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 22:51:17:INFO:	Num examples = 100
2022-01-12 22:51:17:INFO:	RMSE = 34.9426
2022-01-12 22:51:25:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 22:51:25:INFO:	Num examples = 100
2022-01-12 22:51:25:INFO:	RMSE = 20.8837
2022-01-12 22:51:26:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0005
2022-01-12 22:52:40:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0004
2022-01-12 22:53:55:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0004
2022-01-12 22:55:09:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0004
2022-01-12 22:56:23:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0004
2022-01-12 22:57:38:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0004
2022-01-12 22:58:52:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0004
2022-01-12 23:00:06:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0004
2022-01-12 23:01:20:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0004
2022-01-12 23:02:35:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0003
2022-01-12 23:03:49:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0004
2022-01-12 23:05:03:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0003
2022-01-12 23:06:17:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0004
2022-01-12 23:07:30:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0003
2022-01-12 23:08:44:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0003
2022-01-12 23:10:03:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 23:10:03:INFO:	Num examples = 100
2022-01-12 23:10:03:INFO:	RMSE = 34.6140
2022-01-12 23:10:10:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 23:10:10:INFO:	Num examples = 100
2022-01-12 23:10:10:INFO:	RMSE = 18.5714
2022-01-12 23:10:12:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0003
2022-01-12 23:11:25:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0003
2022-01-12 23:12:39:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0003
2022-01-12 23:13:53:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0003
2022-01-12 23:15:07:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0003
2022-01-12 23:16:20:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0003
2022-01-12 23:17:33:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0003
2022-01-12 23:18:46:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0003
2022-01-12 23:19:59:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0003
2022-01-12 23:21:12:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0003
2022-01-12 23:22:25:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0003
2022-01-12 23:23:37:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0003
2022-01-12 23:24:50:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0002
2022-01-12 23:26:03:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0003
2022-01-12 23:27:16:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0003
2022-01-12 23:28:34:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 23:28:34:INFO:	Num examples = 100
2022-01-12 23:28:34:INFO:	RMSE = 34.4787
2022-01-12 23:28:41:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 23:28:41:INFO:	Num examples = 100
2022-01-12 23:28:41:INFO:	RMSE = 19.8416
2022-01-12 23:28:43:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 23:29:56:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0002
2022-01-12 23:31:09:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 23:32:22:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0002
2022-01-12 23:33:34:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 23:34:22:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0002
2022-01-12 23:35:09:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0002
2022-01-12 23:35:56:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 23:36:44:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0002
2022-01-12 23:37:31:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 23:38:19:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0002
2022-01-12 23:39:06:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 23:39:55:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0002
2022-01-12 23:40:43:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 23:41:31:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0002
2022-01-12 23:42:21:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 23:42:21:INFO:	Num examples = 100
2022-01-12 23:42:21:INFO:	RMSE = 34.2513
2022-01-12 23:42:26:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 23:42:26:INFO:	Num examples = 100
2022-01-12 23:42:26:INFO:	RMSE = 20.5156
2022-01-12 23:42:27:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0002
2022-01-12 23:43:14:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 23:44:01:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0002
2022-01-12 23:44:49:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 23:45:36:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0002
2022-01-12 23:46:24:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 23:47:11:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0002
2022-01-12 23:47:58:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 23:48:46:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0002
2022-01-12 23:49:33:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0002
2022-01-12 23:50:21:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0002
2022-01-12 23:51:09:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0002
2022-01-12 23:51:56:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0002
2022-01-12 23:52:43:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0002
2022-01-12 23:53:30:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0002
2022-01-12 23:54:21:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-12 23:54:21:INFO:	Num examples = 100
2022-01-12 23:54:21:INFO:	RMSE = 33.9713
2022-01-12 23:54:26:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-12 23:54:26:INFO:	Num examples = 100
2022-01-12 23:54:26:INFO:	RMSE = 22.7708
2022-01-12 23:54:27:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0001
2022-01-12 23:55:14:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0002
2022-01-12 23:56:01:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0001
2022-01-12 23:56:49:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-12 23:57:36:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0002
2022-01-12 23:58:23:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0001
2022-01-12 23:59:11:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0002
2022-01-12 23:59:58:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0001
2022-01-13 00:00:46:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 00:01:34:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 00:02:21:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0001
2022-01-13 00:03:08:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0001
2022-01-13 00:03:56:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0001
2022-01-13 00:04:43:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 00:05:31:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0001
2022-01-13 00:06:35:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 00:06:35:INFO:	Num examples = 100
2022-01-13 00:06:35:INFO:	RMSE = 33.9150
2022-01-13 00:06:43:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 00:06:43:INFO:	Num examples = 100
2022-01-13 00:06:43:INFO:	RMSE = 25.0053
2022-01-13 00:06:44:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0001
2022-01-13 00:07:59:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0001
2022-01-13 00:09:14:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0001
2022-01-13 00:10:28:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0001
2022-01-13 00:11:43:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0001
2022-01-13 00:12:57:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0001
2022-01-13 00:14:10:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0002
2022-01-13 00:15:23:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0001
2022-01-13 00:16:37:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0001
2022-01-13 00:17:51:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0001
2022-01-13 00:19:05:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0001
2022-01-13 00:20:20:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0001
2022-01-13 00:21:34:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0001
2022-01-13 00:22:48:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0001
2022-01-13 00:24:02:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0001
2022-01-13 00:25:21:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 00:25:21:INFO:	Num examples = 100
2022-01-13 00:25:21:INFO:	RMSE = 33.8457
2022-01-13 00:25:29:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 00:25:29:INFO:	Num examples = 100
2022-01-13 00:25:29:INFO:	RMSE = 26.5859
2022-01-13 00:25:29:INFO:	Output TEST RMSE:	33.5648
2022-01-13 00:25:29:INFO:	VALID RMSEs:	24.1175	22.0564	24.0981	19.8531	18.1257	20.8837	18.5714	19.8416	20.5156	22.7708	25.0053	26.5859
2022-01-13 00:25:29:INFO:	TEST RMSEs:	32.4726	32.3454	34.1089	34.6450	33.5648	34.9426	34.6140	34.4787	34.2513	33.9713	33.9150	33.8457
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-13 00:25:33:INFO:Finish setting logger...
2022-01-13 00:25:33:INFO:==> Training/Evaluation parameters are:
2022-01-13 00:25:33:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667'
2022-01-13 00:25:33:INFO:	data_fn=1
2022-01-13 00:25:33:INFO:	datatest_fn=1
2022-01-13 00:25:33:INFO:	filter_kernel_size=1
2022-01-13 00:25:33:INFO:	override_data_cache=False
2022-01-13 00:25:33:INFO:	maxRUL=125
2022-01-13 00:25:33:INFO:	low_ratio=0.1
2022-01-13 00:25:33:INFO:	high_ratio=0.99
2022-01-13 00:25:33:INFO:	aug_ratio=150
2022-01-13 00:25:33:INFO:	noise_amplitude=0.01
2022-01-13 00:25:33:INFO:	modeltype='cnn1d'
2022-01-13 00:25:33:INFO:	max_seq_len=550
2022-01-13 00:25:33:INFO:	d_model=128
2022-01-13 00:25:33:INFO:	p_dropout=0.1
2022-01-13 00:25:33:INFO:	n_head=4
2022-01-13 00:25:33:INFO:	n_layer=2
2022-01-13 00:25:33:INFO:	dim_feedforward=512
2022-01-13 00:25:33:INFO:	e_dropout=0.1
2022-01-13 00:25:33:INFO:	activation='relu'
2022-01-13 00:25:33:INFO:	layer_norm=False
2022-01-13 00:25:33:INFO:	support_size=5
2022-01-13 00:25:33:INFO:	inner_steps=2
2022-01-13 00:25:33:INFO:	lr_inner=0.0001
2022-01-13 00:25:33:INFO:	lr_meta=0.001
2022-01-13 00:25:33:INFO:	n_epochs=12
2022-01-13 00:25:33:INFO:	train_batch_size=20
2022-01-13 00:25:33:INFO:	eval_batch_size=1
2022-01-13 00:25:33:INFO:	lr=0.001
2022-01-13 00:25:33:INFO:	weight_decay=0.01
2022-01-13 00:25:33:INFO:	warmup_ratio=0.0
2022-01-13 00:25:33:INFO:	max_grad_norm=5.0
2022-01-13 00:25:33:INFO:	logging_steps=50
2022-01-13 00:25:33:INFO:	seed=667
2022-01-13 00:25:33:INFO:	gpu_id=0
2022-01-13 00:25:33:INFO:	do_train=True
2022-01-13 00:25:33:INFO:	do_eval=False
2022-01-13 00:25:33:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-13 00:25:33:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-13 00:25:33:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-13 00:25:33:INFO:	device=device(type='cuda'))
2022-01-13 00:25:33:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-13 00:25:33:INFO:==> Read data from data/train_FD001.txt...
2022-01-13 00:25:33:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 00:25:33:INFO:==> Min_max normalization...
2022-01-13 00:25:33:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 00:25:33:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 00:25:33:INFO:==> Read data from data/test_FD001.txt...
2022-01-13 00:25:33:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 00:25:33:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-13 00:25:33:INFO:	min_rul: 7, max_rul: 145
2022-01-13 00:25:33:INFO:==> Input length ratio of the [TEST] data:
2022-01-13 00:25:33:INFO:	min_ratio = 0.2067
2022-01-13 00:25:33:INFO:	max_ratio = 0.9667
2022-01-13 00:25:33:INFO:==> Min_max normalization...
2022-01-13 00:25:33:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 00:25:33:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 00:25:33:INFO:==> Computing Criterion...
2022-01-13 00:25:33:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-13 00:26:22:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-13 00:26:22:INFO:NumExpr defaulting to 8 threads.
2022-01-13 00:26:22:INFO:=============== Scheme: Meta Learning ===============
2022-01-13 00:26:22:INFO:	Num examples = 15000
2022-01-13 00:26:22:INFO:	Num epochs = 12
2022-01-13 00:26:22:INFO:	Batch size = 20
2022-01-13 00:26:22:INFO:	Total meta optimization steps = 9000
2022-01-13 00:26:22:INFO:	Total inner optimization steps = 18000
2022-01-13 00:26:30:INFO:==> Group parameters for optimization...
2022-01-13 00:26:30:INFO:    Parameters to update are:
2022-01-13 00:26:30:INFO:	conv1.0.weight
2022-01-13 00:26:30:INFO:	conv2.0.weight
2022-01-13 00:26:30:INFO:	conv3.0.weight
2022-01-13 00:26:30:INFO:	conv4.0.weight
2022-01-13 00:26:30:INFO:	conv5.0.weight
2022-01-13 00:26:30:INFO:	fc_1.0.weight
2022-01-13 00:26:30:INFO:	fc_1.0.bias
2022-01-13 00:26:30:INFO:	fc_2.weight
2022-01-13 00:26:30:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-13 00:26:34:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0007
2022-01-13 00:27:49:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0155
2022-01-13 00:29:04:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0087
2022-01-13 00:30:19:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0062
2022-01-13 00:31:34:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0047
2022-01-13 00:32:50:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0041
2022-01-13 00:34:06:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0037
2022-01-13 00:35:21:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0030
2022-01-13 00:36:36:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0031
2022-01-13 00:37:51:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0029
2022-01-13 00:39:06:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0030
2022-01-13 00:40:22:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0034
2022-01-13 00:41:37:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0027
2022-01-13 00:42:52:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0022
2022-01-13 00:44:08:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0023
2022-01-13 00:45:29:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 00:45:29:INFO:	Num examples = 100
2022-01-13 00:45:29:INFO:	RMSE = 31.4013
2022-01-13 00:45:37:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 00:45:37:INFO:	Num examples = 100
2022-01-13 00:45:37:INFO:	RMSE = 31.6758
2022-01-13 00:45:37:INFO:==> Minimal valid RMSE!
2022-01-13 00:45:37:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-13 00:45:38:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0025
2022-01-13 00:46:54:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0022
2022-01-13 00:48:09:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0025
2022-01-13 00:49:24:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0017
2022-01-13 00:50:40:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0017
2022-01-13 00:51:56:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0018
2022-01-13 00:53:11:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0018
2022-01-13 00:54:26:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0017
2022-01-13 00:55:41:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0015
2022-01-13 00:56:56:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0016
2022-01-13 00:58:12:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0015
2022-01-13 00:59:27:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0015
2022-01-13 01:00:41:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0014
2022-01-13 01:01:56:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0017
2022-01-13 01:03:12:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0014
2022-01-13 01:04:33:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 01:04:33:INFO:	Num examples = 100
2022-01-13 01:04:33:INFO:	RMSE = 31.9157
2022-01-13 01:04:41:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 01:04:41:INFO:	Num examples = 100
2022-01-13 01:04:41:INFO:	RMSE = 23.5055
2022-01-13 01:04:41:INFO:==> Minimal valid RMSE!
2022-01-13 01:04:41:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-13 01:04:42:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0017
2022-01-13 01:05:58:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0014
2022-01-13 01:07:13:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0014
2022-01-13 01:08:29:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0012
2022-01-13 01:09:44:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0012
2022-01-13 01:10:59:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0013
2022-01-13 01:12:14:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0012
2022-01-13 01:13:30:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0011
2022-01-13 01:14:46:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0012
2022-01-13 01:16:01:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0011
2022-01-13 01:17:16:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0011
2022-01-13 01:18:31:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0011
2022-01-13 01:19:47:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0011
2022-01-13 01:21:02:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0011
2022-01-13 01:22:17:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0009
2022-01-13 01:23:38:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 01:23:38:INFO:	Num examples = 100
2022-01-13 01:23:38:INFO:	RMSE = 32.5508
2022-01-13 01:23:45:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 01:23:45:INFO:	Num examples = 100
2022-01-13 01:23:45:INFO:	RMSE = 24.9193
2022-01-13 01:23:46:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0012
2022-01-13 01:25:02:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0011
2022-01-13 01:26:17:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0012
2022-01-13 01:27:32:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0010
2022-01-13 01:28:47:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0009
2022-01-13 01:30:02:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0009
2022-01-13 01:31:18:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0010
2022-01-13 01:32:33:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0010
2022-01-13 01:33:48:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0009
2022-01-13 01:35:03:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0009
2022-01-13 01:36:18:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0010
2022-01-13 01:37:33:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0009
2022-01-13 01:38:48:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0009
2022-01-13 01:40:03:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0008
2022-01-13 01:41:19:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 01:42:40:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 01:42:40:INFO:	Num examples = 100
2022-01-13 01:42:40:INFO:	RMSE = 33.5062
2022-01-13 01:42:48:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 01:42:48:INFO:	Num examples = 100
2022-01-13 01:42:48:INFO:	RMSE = 21.6722
2022-01-13 01:42:48:INFO:==> Minimal valid RMSE!
2022-01-13 01:42:48:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-13 01:42:50:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 01:44:04:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0009
2022-01-13 01:45:19:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0008
2022-01-13 01:46:34:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0009
2022-01-13 01:47:49:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0008
2022-01-13 01:49:04:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0009
2022-01-13 01:50:20:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0008
2022-01-13 01:51:35:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0008
2022-01-13 01:52:50:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0007
2022-01-13 01:54:06:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0008
2022-01-13 01:55:21:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0007
2022-01-13 01:56:36:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0007
2022-01-13 01:57:52:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0007
2022-01-13 01:59:07:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0008
2022-01-13 02:00:22:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0007
2022-01-13 02:01:43:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 02:01:43:INFO:	Num examples = 100
2022-01-13 02:01:43:INFO:	RMSE = 33.9334
2022-01-13 02:01:50:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 02:01:50:INFO:	Num examples = 100
2022-01-13 02:01:50:INFO:	RMSE = 19.6425
2022-01-13 02:01:50:INFO:==> Minimal valid RMSE!
2022-01-13 02:01:50:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-667...
2022-01-13 02:01:52:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0006
2022-01-13 02:03:07:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0006
2022-01-13 02:04:22:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 02:05:38:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 02:06:54:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0007
2022-01-13 02:08:10:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0006
2022-01-13 02:09:25:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0005
2022-01-13 02:10:40:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0005
2022-01-13 02:11:56:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0006
2022-01-13 02:13:11:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0005
2022-01-13 02:14:26:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0006
2022-01-13 02:15:42:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0005
2022-01-13 02:16:57:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0006
2022-01-13 02:18:12:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0006
2022-01-13 02:19:27:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0005
2022-01-13 02:20:48:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 02:20:48:INFO:	Num examples = 100
2022-01-13 02:20:48:INFO:	RMSE = 34.6535
2022-01-13 02:20:56:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 02:20:56:INFO:	Num examples = 100
2022-01-13 02:20:56:INFO:	RMSE = 22.3951
2022-01-13 02:20:57:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0005
2022-01-13 02:22:13:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0005
2022-01-13 02:23:28:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0004
2022-01-13 02:24:43:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0004
2022-01-13 02:25:59:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0005
2022-01-13 02:27:15:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0005
2022-01-13 02:28:30:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0004
2022-01-13 02:29:46:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0004
2022-01-13 02:31:01:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0004
2022-01-13 02:32:17:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0004
2022-01-13 02:33:32:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0004
2022-01-13 02:34:47:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0003
2022-01-13 02:36:03:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0004
2022-01-13 02:37:19:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0003
2022-01-13 02:38:34:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0004
2022-01-13 02:39:55:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 02:39:55:INFO:	Num examples = 100
2022-01-13 02:39:55:INFO:	RMSE = 34.4462
2022-01-13 02:40:02:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 02:40:02:INFO:	Num examples = 100
2022-01-13 02:40:02:INFO:	RMSE = 20.8269
2022-01-13 02:40:04:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0003
2022-01-13 02:41:19:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0004
2022-01-13 02:42:34:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0003
2022-01-13 02:43:49:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0003
2022-01-13 02:45:04:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0003
2022-01-13 02:46:19:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0003
2022-01-13 02:47:34:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0003
2022-01-13 02:48:50:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0003
2022-01-13 02:50:05:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0003
2022-01-13 02:51:20:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0003
2022-01-13 02:52:35:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0003
2022-01-13 02:53:50:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0003
2022-01-13 02:55:05:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0003
2022-01-13 02:56:20:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0003
2022-01-13 02:57:35:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0003
2022-01-13 02:58:56:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 02:58:56:INFO:	Num examples = 100
2022-01-13 02:58:56:INFO:	RMSE = 34.7142
2022-01-13 02:59:04:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 02:59:04:INFO:	Num examples = 100
2022-01-13 02:59:04:INFO:	RMSE = 21.4776
2022-01-13 02:59:05:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0003
2022-01-13 03:00:21:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0003
2022-01-13 03:01:36:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 03:02:50:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 03:04:06:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0002
2022-01-13 03:05:21:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0002
2022-01-13 03:06:36:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0002
2022-01-13 03:07:52:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0002
2022-01-13 03:09:08:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0002
2022-01-13 03:10:23:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0003
2022-01-13 03:11:38:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0003
2022-01-13 03:12:54:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0002
2022-01-13 03:14:09:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0002
2022-01-13 03:15:24:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0002
2022-01-13 03:16:40:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0002
2022-01-13 03:18:00:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 03:18:00:INFO:	Num examples = 100
2022-01-13 03:18:00:INFO:	RMSE = 34.6746
2022-01-13 03:18:08:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 03:18:08:INFO:	Num examples = 100
2022-01-13 03:18:08:INFO:	RMSE = 21.9742
2022-01-13 03:18:09:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0002
2022-01-13 03:19:24:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0002
2022-01-13 03:20:39:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0002
2022-01-13 03:21:55:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0002
2022-01-13 03:23:10:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0002
2022-01-13 03:24:25:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0002
2022-01-13 03:25:40:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0002
2022-01-13 03:26:55:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-13 03:28:10:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0002
2022-01-13 03:29:26:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0002
2022-01-13 03:30:42:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0002
2022-01-13 03:31:57:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0002
2022-01-13 03:33:13:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0002
2022-01-13 03:34:28:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0002
2022-01-13 03:35:43:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0002
2022-01-13 03:37:04:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 03:37:04:INFO:	Num examples = 100
2022-01-13 03:37:05:INFO:	RMSE = 34.1529
2022-01-13 03:37:12:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 03:37:12:INFO:	Num examples = 100
2022-01-13 03:37:12:INFO:	RMSE = 22.8696
2022-01-13 03:37:13:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0002
2022-01-13 03:38:27:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0002
2022-01-13 03:39:41:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0002
2022-01-13 03:40:56:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-13 03:42:11:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0002
2022-01-13 03:43:26:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0002
2022-01-13 03:44:40:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0002
2022-01-13 03:45:55:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0002
2022-01-13 03:47:11:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 03:48:26:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 03:49:42:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0002
2022-01-13 03:50:57:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0002
2022-01-13 03:52:12:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0002
2022-01-13 03:53:27:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 03:54:42:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 03:56:02:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 03:56:02:INFO:	Num examples = 100
2022-01-13 03:56:02:INFO:	RMSE = 33.8974
2022-01-13 03:56:09:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 03:56:09:INFO:	Num examples = 100
2022-01-13 03:56:09:INFO:	RMSE = 24.8732
2022-01-13 03:56:11:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0002
2022-01-13 03:57:25:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0002
2022-01-13 03:58:40:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0002
2022-01-13 03:59:55:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0002
2022-01-13 04:01:09:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0002
2022-01-13 04:02:24:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0002
2022-01-13 04:03:39:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0002
2022-01-13 04:04:55:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0002
2022-01-13 04:06:11:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0002
2022-01-13 04:07:26:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0001
2022-01-13 04:08:41:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0002
2022-01-13 04:09:57:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0001
2022-01-13 04:11:12:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0001
2022-01-13 04:12:27:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0002
2022-01-13 04:13:43:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0002
2022-01-13 04:15:04:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 04:15:04:INFO:	Num examples = 100
2022-01-13 04:15:04:INFO:	RMSE = 33.8904
2022-01-13 04:15:12:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 04:15:12:INFO:	Num examples = 100
2022-01-13 04:15:12:INFO:	RMSE = 26.8391
2022-01-13 04:15:12:INFO:	Output TEST RMSE:	33.9334
2022-01-13 04:15:12:INFO:	VALID RMSEs:	31.6758	23.5055	24.9193	21.6722	19.6425	22.3951	20.8269	21.4776	21.9742	22.8696	24.8732	26.8391
2022-01-13 04:15:12:INFO:	TEST RMSEs:	31.4013	31.9157	32.5508	33.5062	33.9334	34.6535	34.4462	34.7142	34.6746	34.1529	33.8974	33.8904
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-13 04:15:16:INFO:Finish setting logger...
2022-01-13 04:15:16:INFO:==> Training/Evaluation parameters are:
2022-01-13 04:15:16:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667'
2022-01-13 04:15:16:INFO:	data_fn=1
2022-01-13 04:15:16:INFO:	datatest_fn=1
2022-01-13 04:15:16:INFO:	filter_kernel_size=1
2022-01-13 04:15:16:INFO:	override_data_cache=False
2022-01-13 04:15:16:INFO:	maxRUL=125
2022-01-13 04:15:16:INFO:	low_ratio=0.1
2022-01-13 04:15:16:INFO:	high_ratio=0.99
2022-01-13 04:15:16:INFO:	aug_ratio=150
2022-01-13 04:15:16:INFO:	noise_amplitude=0.01
2022-01-13 04:15:16:INFO:	modeltype='cnn1d'
2022-01-13 04:15:16:INFO:	max_seq_len=550
2022-01-13 04:15:16:INFO:	d_model=128
2022-01-13 04:15:16:INFO:	p_dropout=0.1
2022-01-13 04:15:16:INFO:	n_head=4
2022-01-13 04:15:16:INFO:	n_layer=2
2022-01-13 04:15:16:INFO:	dim_feedforward=512
2022-01-13 04:15:16:INFO:	e_dropout=0.1
2022-01-13 04:15:16:INFO:	activation='relu'
2022-01-13 04:15:16:INFO:	layer_norm=False
2022-01-13 04:15:16:INFO:	support_size=5
2022-01-13 04:15:16:INFO:	inner_steps=2
2022-01-13 04:15:16:INFO:	lr_inner=0.001
2022-01-13 04:15:16:INFO:	lr_meta=0.001
2022-01-13 04:15:16:INFO:	n_epochs=12
2022-01-13 04:15:16:INFO:	train_batch_size=20
2022-01-13 04:15:16:INFO:	eval_batch_size=1
2022-01-13 04:15:16:INFO:	lr=0.001
2022-01-13 04:15:16:INFO:	weight_decay=0.01
2022-01-13 04:15:16:INFO:	warmup_ratio=0.0
2022-01-13 04:15:16:INFO:	max_grad_norm=5.0
2022-01-13 04:15:16:INFO:	logging_steps=50
2022-01-13 04:15:16:INFO:	seed=667
2022-01-13 04:15:16:INFO:	gpu_id=0
2022-01-13 04:15:16:INFO:	do_train=True
2022-01-13 04:15:16:INFO:	do_eval=False
2022-01-13 04:15:16:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-13 04:15:16:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-13 04:15:16:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-13 04:15:16:INFO:	device=device(type='cuda'))
2022-01-13 04:15:16:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667...
2022-01-13 04:15:16:INFO:==> Read data from data/train_FD001.txt...
2022-01-13 04:15:16:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 04:15:16:INFO:==> Min_max normalization...
2022-01-13 04:15:16:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 04:15:16:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 04:15:16:INFO:==> Read data from data/test_FD001.txt...
2022-01-13 04:15:16:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 04:15:16:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-13 04:15:16:INFO:	min_rul: 7, max_rul: 145
2022-01-13 04:15:16:INFO:==> Input length ratio of the [TEST] data:
2022-01-13 04:15:16:INFO:	min_ratio = 0.2067
2022-01-13 04:15:16:INFO:	max_ratio = 0.9667
2022-01-13 04:15:16:INFO:==> Min_max normalization...
2022-01-13 04:15:16:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 04:15:16:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 04:15:16:INFO:==> Computing Criterion...
2022-01-13 04:15:16:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-13 04:15:29:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-13 04:15:29:INFO:NumExpr defaulting to 8 threads.
2022-01-13 04:15:29:INFO:=============== Scheme: Meta Learning ===============
2022-01-13 04:15:29:INFO:	Num examples = 15000
2022-01-13 04:15:29:INFO:	Num epochs = 12
2022-01-13 04:15:29:INFO:	Batch size = 20
2022-01-13 04:15:29:INFO:	Total meta optimization steps = 9000
2022-01-13 04:15:29:INFO:	Total inner optimization steps = 18000
2022-01-13 04:15:37:INFO:==> Group parameters for optimization...
2022-01-13 04:15:37:INFO:    Parameters to update are:
2022-01-13 04:15:37:INFO:	conv1.0.weight
2022-01-13 04:15:37:INFO:	conv2.0.weight
2022-01-13 04:15:37:INFO:	conv3.0.weight
2022-01-13 04:15:37:INFO:	conv4.0.weight
2022-01-13 04:15:37:INFO:	conv5.0.weight
2022-01-13 04:15:37:INFO:	fc_1.0.weight
2022-01-13 04:15:37:INFO:	fc_1.0.bias
2022-01-13 04:15:37:INFO:	fc_2.weight
2022-01-13 04:15:37:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-13 04:15:40:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0007
2022-01-13 04:16:56:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0155
2022-01-13 04:18:11:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0087
2022-01-13 04:19:25:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0062
2022-01-13 04:20:40:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0047
2022-01-13 04:21:54:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0041
2022-01-13 04:23:10:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0037
2022-01-13 04:24:25:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0030
2022-01-13 04:25:40:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0031
2022-01-13 04:26:56:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0029
2022-01-13 04:28:11:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0030
2022-01-13 04:29:27:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0034
2022-01-13 04:30:42:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0027
2022-01-13 04:31:58:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0022
2022-01-13 04:33:13:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0023
2022-01-13 04:34:35:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 04:34:35:INFO:	Num examples = 100
2022-01-13 04:34:35:INFO:	RMSE = 31.4013
2022-01-13 04:34:43:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 04:34:43:INFO:	Num examples = 100
2022-01-13 04:34:43:INFO:	RMSE = 31.6758
2022-01-13 04:34:43:INFO:==> Minimal valid RMSE!
2022-01-13 04:34:43:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667...
2022-01-13 04:34:44:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0025
2022-01-13 04:35:59:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0022
2022-01-13 04:37:15:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0025
2022-01-13 04:38:30:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0017
2022-01-13 04:39:44:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0017
2022-01-13 04:40:59:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0018
2022-01-13 04:42:14:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0018
2022-01-13 04:43:29:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0017
2022-01-13 04:44:44:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0015
2022-01-13 04:45:59:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0016
2022-01-13 04:47:15:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0015
2022-01-13 04:48:30:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0015
2022-01-13 04:49:45:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0014
2022-01-13 04:51:00:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0017
2022-01-13 04:52:15:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0014
2022-01-13 04:53:35:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 04:53:35:INFO:	Num examples = 100
2022-01-13 04:53:35:INFO:	RMSE = 31.9157
2022-01-13 04:53:43:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 04:53:43:INFO:	Num examples = 100
2022-01-13 04:53:43:INFO:	RMSE = 23.5055
2022-01-13 04:53:43:INFO:==> Minimal valid RMSE!
2022-01-13 04:53:43:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667...
2022-01-13 04:53:44:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0017
2022-01-13 04:54:59:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0014
2022-01-13 04:56:13:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0014
2022-01-13 04:57:28:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0012
2022-01-13 04:58:43:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0012
2022-01-13 04:59:57:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0013
2022-01-13 05:01:12:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0012
2022-01-13 05:02:27:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0011
2022-01-13 05:03:42:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0012
2022-01-13 05:04:57:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0011
2022-01-13 05:06:12:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0011
2022-01-13 05:07:26:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0011
2022-01-13 05:08:41:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0011
2022-01-13 05:09:56:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0011
2022-01-13 05:11:10:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0009
2022-01-13 05:12:32:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 05:12:32:INFO:	Num examples = 100
2022-01-13 05:12:32:INFO:	RMSE = 32.5508
2022-01-13 05:12:40:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 05:12:40:INFO:	Num examples = 100
2022-01-13 05:12:40:INFO:	RMSE = 24.9193
2022-01-13 05:12:41:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0012
2022-01-13 05:13:56:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0011
2022-01-13 05:15:11:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0012
2022-01-13 05:16:26:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0010
2022-01-13 05:17:42:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0009
2022-01-13 05:18:57:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0009
2022-01-13 05:20:13:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0010
2022-01-13 05:21:27:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0010
2022-01-13 05:22:42:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0009
2022-01-13 05:23:57:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0009
2022-01-13 05:25:12:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0010
2022-01-13 05:26:27:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0009
2022-01-13 05:27:41:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0009
2022-01-13 05:28:56:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0008
2022-01-13 05:30:12:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 05:31:34:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 05:31:34:INFO:	Num examples = 100
2022-01-13 05:31:34:INFO:	RMSE = 33.5062
2022-01-13 05:31:41:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 05:31:41:INFO:	Num examples = 100
2022-01-13 05:31:41:INFO:	RMSE = 21.6722
2022-01-13 05:31:41:INFO:==> Minimal valid RMSE!
2022-01-13 05:31:41:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667...
2022-01-13 05:31:43:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 05:32:58:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0009
2022-01-13 05:34:13:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0008
2022-01-13 05:35:28:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0009
2022-01-13 05:36:44:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0008
2022-01-13 05:37:59:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0009
2022-01-13 05:39:14:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0008
2022-01-13 05:40:29:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0008
2022-01-13 05:41:44:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0007
2022-01-13 05:43:00:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0008
2022-01-13 05:44:14:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0007
2022-01-13 05:45:29:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0007
2022-01-13 05:46:45:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0007
2022-01-13 05:48:00:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0008
2022-01-13 05:49:15:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0007
2022-01-13 05:50:35:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 05:50:35:INFO:	Num examples = 100
2022-01-13 05:50:35:INFO:	RMSE = 33.9334
2022-01-13 05:50:42:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 05:50:42:INFO:	Num examples = 100
2022-01-13 05:50:42:INFO:	RMSE = 19.6425
2022-01-13 05:50:42:INFO:==> Minimal valid RMSE!
2022-01-13 05:50:42:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-667...
2022-01-13 05:50:43:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0006
2022-01-13 05:51:58:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0006
2022-01-13 05:53:14:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 05:54:30:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 05:55:45:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0007
2022-01-13 05:57:01:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0006
2022-01-13 05:58:16:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0005
2022-01-13 05:59:31:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0005
2022-01-13 06:00:46:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0006
2022-01-13 06:02:02:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0005
2022-01-13 06:03:17:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0006
2022-01-13 06:04:32:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0005
2022-01-13 06:05:47:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0006
2022-01-13 06:07:03:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0006
2022-01-13 06:08:18:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0005
2022-01-13 06:09:39:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 06:09:39:INFO:	Num examples = 100
2022-01-13 06:09:39:INFO:	RMSE = 34.6535
2022-01-13 06:09:47:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 06:09:47:INFO:	Num examples = 100
2022-01-13 06:09:47:INFO:	RMSE = 22.3951
2022-01-13 06:09:48:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0005
2022-01-13 06:11:03:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0005
2022-01-13 06:12:19:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0004
2022-01-13 06:13:33:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0004
2022-01-13 06:14:48:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0005
2022-01-13 06:16:03:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0005
2022-01-13 06:17:18:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0004
2022-01-13 06:18:33:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0004
2022-01-13 06:19:49:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0004
2022-01-13 06:21:05:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0004
2022-01-13 06:22:19:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0004
2022-01-13 06:23:34:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0003
2022-01-13 06:24:50:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0004
2022-01-13 06:26:05:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0003
2022-01-13 06:27:20:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0004
2022-01-13 06:28:41:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 06:28:41:INFO:	Num examples = 100
2022-01-13 06:28:41:INFO:	RMSE = 34.4462
2022-01-13 06:28:48:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 06:28:48:INFO:	Num examples = 100
2022-01-13 06:28:48:INFO:	RMSE = 20.8269
2022-01-13 06:28:50:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0003
2022-01-13 06:30:05:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0004
2022-01-13 06:31:20:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0003
2022-01-13 06:32:35:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0003
2022-01-13 06:33:50:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0003
2022-01-13 06:35:05:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0003
2022-01-13 06:36:21:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0003
2022-01-13 06:37:37:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0003
2022-01-13 06:38:52:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0003
2022-01-13 06:40:07:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0003
2022-01-13 06:41:22:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0003
2022-01-13 06:42:37:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0003
2022-01-13 06:43:52:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0003
2022-01-13 06:45:07:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0003
2022-01-13 06:46:22:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0003
2022-01-13 06:47:44:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 06:47:44:INFO:	Num examples = 100
2022-01-13 06:47:44:INFO:	RMSE = 34.7142
2022-01-13 06:47:51:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 06:47:51:INFO:	Num examples = 100
2022-01-13 06:47:51:INFO:	RMSE = 21.4776
2022-01-13 06:47:53:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0003
2022-01-13 06:49:08:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0003
2022-01-13 06:50:23:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 06:51:38:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 06:52:54:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0002
2022-01-13 06:54:09:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0002
2022-01-13 06:55:24:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0002
2022-01-13 06:56:39:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0002
2022-01-13 06:57:55:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0002
2022-01-13 06:59:10:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0003
2022-01-13 07:00:25:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0003
2022-01-13 07:01:41:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0002
2022-01-13 07:02:57:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0002
2022-01-13 07:04:11:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0002
2022-01-13 07:05:27:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0002
2022-01-13 07:06:48:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 07:06:48:INFO:	Num examples = 100
2022-01-13 07:06:48:INFO:	RMSE = 34.6746
2022-01-13 07:06:56:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 07:06:56:INFO:	Num examples = 100
2022-01-13 07:06:56:INFO:	RMSE = 21.9742
2022-01-13 07:06:57:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0002
2022-01-13 07:08:12:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0002
2022-01-13 07:09:27:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0002
2022-01-13 07:10:43:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0002
2022-01-13 07:11:59:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0002
2022-01-13 07:13:14:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0002
2022-01-13 07:14:30:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0002
2022-01-13 07:15:46:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-13 07:17:00:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0002
2022-01-13 07:18:16:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0002
2022-01-13 07:19:32:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0002
2022-01-13 07:20:47:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0002
2022-01-13 07:22:03:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0002
2022-01-13 07:23:18:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0002
2022-01-13 07:24:34:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0002
2022-01-13 07:25:55:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 07:25:55:INFO:	Num examples = 100
2022-01-13 07:25:55:INFO:	RMSE = 34.1529
2022-01-13 07:26:02:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 07:26:02:INFO:	Num examples = 100
2022-01-13 07:26:02:INFO:	RMSE = 22.8696
2022-01-13 07:26:04:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0002
2022-01-13 07:27:19:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0002
2022-01-13 07:28:34:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0002
2022-01-13 07:29:49:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-13 07:31:04:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0002
2022-01-13 07:32:19:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0002
2022-01-13 07:33:34:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0002
2022-01-13 07:34:50:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0002
2022-01-13 07:36:05:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 07:37:21:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 07:38:36:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0002
2022-01-13 07:39:51:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0002
2022-01-13 07:41:07:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0002
2022-01-13 07:42:22:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 07:43:37:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 07:44:58:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 07:44:58:INFO:	Num examples = 100
2022-01-13 07:44:58:INFO:	RMSE = 33.8974
2022-01-13 07:45:05:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 07:45:05:INFO:	Num examples = 100
2022-01-13 07:45:05:INFO:	RMSE = 24.8732
2022-01-13 07:45:07:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0002
2022-01-13 07:46:21:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0002
2022-01-13 07:47:36:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0002
2022-01-13 07:48:50:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0002
2022-01-13 07:50:05:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0002
2022-01-13 07:51:20:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0002
2022-01-13 07:52:35:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0002
2022-01-13 07:53:50:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0002
2022-01-13 07:55:05:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0002
2022-01-13 07:56:20:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0001
2022-01-13 07:57:35:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0002
2022-01-13 07:58:51:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0001
2022-01-13 08:00:06:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0001
2022-01-13 08:01:21:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0002
2022-01-13 08:02:36:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0002
2022-01-13 08:03:57:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 08:03:57:INFO:	Num examples = 100
2022-01-13 08:03:57:INFO:	RMSE = 33.8904
2022-01-13 08:04:05:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 08:04:05:INFO:	Num examples = 100
2022-01-13 08:04:05:INFO:	RMSE = 26.8391
2022-01-13 08:04:05:INFO:	Output TEST RMSE:	33.9334
2022-01-13 08:04:05:INFO:	VALID RMSEs:	31.6758	23.5055	24.9193	21.6722	19.6425	22.3951	20.8269	21.4776	21.9742	22.8696	24.8732	26.8391
2022-01-13 08:04:05:INFO:	TEST RMSEs:	31.4013	31.9157	32.5508	33.5062	33.9334	34.6535	34.4462	34.7142	34.6746	34.1529	33.8974	33.8904
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-13 08:04:09:INFO:Finish setting logger...
2022-01-13 08:04:09:INFO:==> Training/Evaluation parameters are:
2022-01-13 08:04:09:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128'
2022-01-13 08:04:09:INFO:	data_fn=1
2022-01-13 08:04:09:INFO:	datatest_fn=1
2022-01-13 08:04:09:INFO:	filter_kernel_size=1
2022-01-13 08:04:09:INFO:	override_data_cache=False
2022-01-13 08:04:09:INFO:	maxRUL=125
2022-01-13 08:04:09:INFO:	low_ratio=0.1
2022-01-13 08:04:09:INFO:	high_ratio=0.99
2022-01-13 08:04:09:INFO:	aug_ratio=150
2022-01-13 08:04:09:INFO:	noise_amplitude=0.01
2022-01-13 08:04:09:INFO:	modeltype='cnn1d'
2022-01-13 08:04:09:INFO:	max_seq_len=550
2022-01-13 08:04:09:INFO:	d_model=128
2022-01-13 08:04:09:INFO:	p_dropout=0.1
2022-01-13 08:04:09:INFO:	n_head=4
2022-01-13 08:04:09:INFO:	n_layer=2
2022-01-13 08:04:09:INFO:	dim_feedforward=512
2022-01-13 08:04:09:INFO:	e_dropout=0.1
2022-01-13 08:04:09:INFO:	activation='relu'
2022-01-13 08:04:09:INFO:	layer_norm=False
2022-01-13 08:04:09:INFO:	support_size=2
2022-01-13 08:04:09:INFO:	inner_steps=2
2022-01-13 08:04:09:INFO:	lr_inner=0.0001
2022-01-13 08:04:09:INFO:	lr_meta=0.001
2022-01-13 08:04:09:INFO:	n_epochs=12
2022-01-13 08:04:09:INFO:	train_batch_size=20
2022-01-13 08:04:09:INFO:	eval_batch_size=1
2022-01-13 08:04:09:INFO:	lr=0.001
2022-01-13 08:04:09:INFO:	weight_decay=0.01
2022-01-13 08:04:09:INFO:	warmup_ratio=0.0
2022-01-13 08:04:09:INFO:	max_grad_norm=5.0
2022-01-13 08:04:09:INFO:	logging_steps=50
2022-01-13 08:04:09:INFO:	seed=128
2022-01-13 08:04:09:INFO:	gpu_id=0
2022-01-13 08:04:09:INFO:	do_train=True
2022-01-13 08:04:09:INFO:	do_eval=False
2022-01-13 08:04:09:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-13 08:04:09:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-13 08:04:09:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-13 08:04:09:INFO:	device=device(type='cuda'))
2022-01-13 08:04:09:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-13 08:04:09:INFO:==> Read data from data/train_FD001.txt...
2022-01-13 08:04:09:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 08:04:09:INFO:==> Min_max normalization...
2022-01-13 08:04:09:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 08:04:09:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 08:04:09:INFO:==> Read data from data/test_FD001.txt...
2022-01-13 08:04:09:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 08:04:09:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-13 08:04:09:INFO:	min_rul: 7, max_rul: 145
2022-01-13 08:04:09:INFO:==> Input length ratio of the [TEST] data:
2022-01-13 08:04:09:INFO:	min_ratio = 0.2067
2022-01-13 08:04:09:INFO:	max_ratio = 0.9667
2022-01-13 08:04:09:INFO:==> Min_max normalization...
2022-01-13 08:04:09:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 08:04:09:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 08:04:09:INFO:==> Computing Criterion...
2022-01-13 08:04:10:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-13 08:04:17:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-13 08:04:17:INFO:NumExpr defaulting to 8 threads.
2022-01-13 08:04:18:INFO:=============== Scheme: Meta Learning ===============
2022-01-13 08:04:18:INFO:	Num examples = 15000
2022-01-13 08:04:18:INFO:	Num epochs = 12
2022-01-13 08:04:18:INFO:	Batch size = 20
2022-01-13 08:04:18:INFO:	Total meta optimization steps = 9000
2022-01-13 08:04:18:INFO:	Total inner optimization steps = 18000
2022-01-13 08:04:26:INFO:==> Group parameters for optimization...
2022-01-13 08:04:26:INFO:    Parameters to update are:
2022-01-13 08:04:26:INFO:	conv1.0.weight
2022-01-13 08:04:26:INFO:	conv2.0.weight
2022-01-13 08:04:26:INFO:	conv3.0.weight
2022-01-13 08:04:26:INFO:	conv4.0.weight
2022-01-13 08:04:26:INFO:	conv5.0.weight
2022-01-13 08:04:26:INFO:	fc_1.0.weight
2022-01-13 08:04:26:INFO:	fc_1.0.bias
2022-01-13 08:04:26:INFO:	fc_2.weight
2022-01-13 08:04:26:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-13 08:04:29:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0012
2022-01-13 08:05:45:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0130
2022-01-13 08:07:00:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0066
2022-01-13 08:08:15:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0039
2022-01-13 08:09:31:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0035
2022-01-13 08:10:47:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0033
2022-01-13 08:12:01:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0034
2022-01-13 08:13:17:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0030
2022-01-13 08:14:32:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0027
2022-01-13 08:15:48:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0030
2022-01-13 08:17:04:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0031
2022-01-13 08:18:20:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0028
2022-01-13 08:19:36:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0025
2022-01-13 08:20:52:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0026
2022-01-13 08:22:07:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0027
2022-01-13 08:23:28:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 08:23:28:INFO:	Num examples = 100
2022-01-13 08:23:28:INFO:	RMSE = 33.7433
2022-01-13 08:23:35:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 08:23:35:INFO:	Num examples = 100
2022-01-13 08:23:35:INFO:	RMSE = 26.5313
2022-01-13 08:23:35:INFO:==> Minimal valid RMSE!
2022-01-13 08:23:36:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-13 08:23:37:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0021
2022-01-13 08:24:52:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0021
2022-01-13 08:26:08:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0019
2022-01-13 08:27:25:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0019
2022-01-13 08:28:40:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0020
2022-01-13 08:29:56:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0020
2022-01-13 08:31:11:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0018
2022-01-13 08:32:27:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0018
2022-01-13 08:33:42:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0018
2022-01-13 08:34:57:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0018
2022-01-13 08:36:12:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0017
2022-01-13 08:37:27:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0018
2022-01-13 08:38:42:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0015
2022-01-13 08:39:58:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0015
2022-01-13 08:41:13:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0015
2022-01-13 08:42:34:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 08:42:34:INFO:	Num examples = 100
2022-01-13 08:42:34:INFO:	RMSE = 32.6902
2022-01-13 08:42:42:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 08:42:42:INFO:	Num examples = 100
2022-01-13 08:42:42:INFO:	RMSE = 31.7063
2022-01-13 08:42:43:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0015
2022-01-13 08:43:58:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0013
2022-01-13 08:45:14:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0013
2022-01-13 08:46:30:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0012
2022-01-13 08:47:46:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0011
2022-01-13 08:49:02:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0012
2022-01-13 08:50:18:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0012
2022-01-13 08:51:33:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0012
2022-01-13 08:52:48:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0013
2022-01-13 08:54:03:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0011
2022-01-13 08:55:19:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0012
2022-01-13 08:56:34:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0011
2022-01-13 08:57:50:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0012
2022-01-13 08:59:05:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0011
2022-01-13 09:00:20:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0011
2022-01-13 09:01:42:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 09:01:42:INFO:	Num examples = 100
2022-01-13 09:01:42:INFO:	RMSE = 32.6343
2022-01-13 09:01:49:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 09:01:49:INFO:	Num examples = 100
2022-01-13 09:01:49:INFO:	RMSE = 27.9264
2022-01-13 09:01:51:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0009
2022-01-13 09:03:06:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0010
2022-01-13 09:04:22:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0010
2022-01-13 09:05:37:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0009
2022-01-13 09:06:53:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0008
2022-01-13 09:08:09:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0010
2022-01-13 09:09:24:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0011
2022-01-13 09:10:39:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0010
2022-01-13 09:11:55:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0009
2022-01-13 09:13:10:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0009
2022-01-13 09:14:25:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0009
2022-01-13 09:15:41:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0010
2022-01-13 09:16:56:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0008
2022-01-13 09:18:12:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0008
2022-01-13 09:19:27:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 09:20:48:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 09:20:48:INFO:	Num examples = 100
2022-01-13 09:20:48:INFO:	RMSE = 33.6193
2022-01-13 09:20:55:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 09:20:55:INFO:	Num examples = 100
2022-01-13 09:20:55:INFO:	RMSE = 22.4685
2022-01-13 09:20:55:INFO:==> Minimal valid RMSE!
2022-01-13 09:20:55:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-13 09:20:57:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 09:22:12:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0008
2022-01-13 09:23:27:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0008
2022-01-13 09:24:43:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0009
2022-01-13 09:25:58:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0008
2022-01-13 09:27:13:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0008
2022-01-13 09:28:28:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0008
2022-01-13 09:29:43:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0007
2022-01-13 09:30:58:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0008
2022-01-13 09:32:13:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0008
2022-01-13 09:33:27:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0008
2022-01-13 09:34:42:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0007
2022-01-13 09:35:57:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0008
2022-01-13 09:37:13:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0007
2022-01-13 09:38:29:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0008
2022-01-13 09:39:50:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 09:39:50:INFO:	Num examples = 100
2022-01-13 09:39:50:INFO:	RMSE = 33.3207
2022-01-13 09:39:57:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 09:39:57:INFO:	Num examples = 100
2022-01-13 09:39:57:INFO:	RMSE = 24.7134
2022-01-13 09:39:59:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0007
2022-01-13 09:41:14:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0007
2022-01-13 09:42:31:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 09:43:46:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 09:45:02:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0006
2022-01-13 09:46:18:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0006
2022-01-13 09:47:34:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0006
2022-01-13 09:48:49:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0006
2022-01-13 09:50:04:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0006
2022-01-13 09:51:19:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0006
2022-01-13 09:52:34:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0006
2022-01-13 09:53:49:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0006
2022-01-13 09:55:04:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0005
2022-01-13 09:56:20:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0005
2022-01-13 09:57:36:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0005
2022-01-13 09:58:57:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 09:58:57:INFO:	Num examples = 100
2022-01-13 09:58:57:INFO:	RMSE = 33.1443
2022-01-13 09:59:04:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 09:59:04:INFO:	Num examples = 100
2022-01-13 09:59:04:INFO:	RMSE = 26.0841
2022-01-13 09:59:06:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0006
2022-01-13 10:00:21:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0005
2022-01-13 10:01:37:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0005
2022-01-13 10:02:53:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0005
2022-01-13 10:04:08:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0005
2022-01-13 10:05:23:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0005
2022-01-13 10:06:39:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0004
2022-01-13 10:07:54:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0005
2022-01-13 10:09:10:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0004
2022-01-13 10:10:25:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0004
2022-01-13 10:11:41:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0005
2022-01-13 10:12:56:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0004
2022-01-13 10:14:11:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0004
2022-01-13 10:15:26:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0004
2022-01-13 10:16:41:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0004
2022-01-13 10:18:02:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 10:18:02:INFO:	Num examples = 100
2022-01-13 10:18:02:INFO:	RMSE = 35.2370
2022-01-13 10:18:09:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 10:18:09:INFO:	Num examples = 100
2022-01-13 10:18:09:INFO:	RMSE = 26.5164
2022-01-13 10:18:11:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0004
2022-01-13 10:19:26:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0004
2022-01-13 10:20:41:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0004
2022-01-13 10:21:56:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0004
2022-01-13 10:23:12:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0004
2022-01-13 10:24:27:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0004
2022-01-13 10:25:43:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0004
2022-01-13 10:26:58:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0003
2022-01-13 10:28:13:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0004
2022-01-13 10:29:29:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0003
2022-01-13 10:30:45:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0003
2022-01-13 10:32:00:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0004
2022-01-13 10:33:16:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0003
2022-01-13 10:34:32:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0003
2022-01-13 10:35:47:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0003
2022-01-13 10:37:08:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 10:37:08:INFO:	Num examples = 100
2022-01-13 10:37:08:INFO:	RMSE = 34.6862
2022-01-13 10:37:16:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 10:37:16:INFO:	Num examples = 100
2022-01-13 10:37:16:INFO:	RMSE = 24.6210
2022-01-13 10:37:17:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0003
2022-01-13 10:38:32:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0003
2022-01-13 10:39:48:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 10:41:03:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 10:42:18:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0003
2022-01-13 10:43:34:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0002
2022-01-13 10:44:50:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0003
2022-01-13 10:46:06:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0003
2022-01-13 10:47:22:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0003
2022-01-13 10:48:37:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0003
2022-01-13 10:49:53:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0003
2022-01-13 10:51:09:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0003
2022-01-13 10:52:24:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0003
2022-01-13 10:53:40:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0002
2022-01-13 10:54:56:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0003
2022-01-13 10:56:17:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 10:56:17:INFO:	Num examples = 100
2022-01-13 10:56:17:INFO:	RMSE = 34.8416
2022-01-13 10:56:25:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 10:56:25:INFO:	Num examples = 100
2022-01-13 10:56:25:INFO:	RMSE = 24.9056
2022-01-13 10:56:26:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0003
2022-01-13 10:57:42:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0002
2022-01-13 10:58:57:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0003
2022-01-13 11:00:13:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0002
2022-01-13 11:01:29:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0002
2022-01-13 11:02:45:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0002
2022-01-13 11:04:01:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0002
2022-01-13 11:05:16:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-13 11:06:32:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0002
2022-01-13 11:07:49:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0002
2022-01-13 11:09:04:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0002
2022-01-13 11:10:20:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0002
2022-01-13 11:11:35:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0002
2022-01-13 11:12:51:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0002
2022-01-13 11:14:07:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0002
2022-01-13 11:15:28:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 11:15:28:INFO:	Num examples = 100
2022-01-13 11:15:28:INFO:	RMSE = 35.1669
2022-01-13 11:15:36:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 11:15:36:INFO:	Num examples = 100
2022-01-13 11:15:36:INFO:	RMSE = 27.1607
2022-01-13 11:15:37:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0002
2022-01-13 11:16:53:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0002
2022-01-13 11:18:10:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0002
2022-01-13 11:19:25:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-13 11:20:41:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0002
2022-01-13 11:21:57:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0002
2022-01-13 11:23:13:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0002
2022-01-13 11:24:29:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0002
2022-01-13 11:25:44:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 11:27:00:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 11:28:16:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0002
2022-01-13 11:29:32:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0002
2022-01-13 11:30:48:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0002
2022-01-13 11:32:04:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 11:33:20:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 11:34:41:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 11:34:41:INFO:	Num examples = 100
2022-01-13 11:34:41:INFO:	RMSE = 34.6945
2022-01-13 11:34:49:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 11:34:49:INFO:	Num examples = 100
2022-01-13 11:34:49:INFO:	RMSE = 29.2404
2022-01-13 11:34:50:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0002
2022-01-13 11:36:06:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0002
2022-01-13 11:37:21:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0002
2022-01-13 11:38:37:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0002
2022-01-13 11:39:53:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0001
2022-01-13 11:41:09:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0002
2022-01-13 11:42:24:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0002
2022-01-13 11:43:40:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0002
2022-01-13 11:44:55:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0002
2022-01-13 11:46:10:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0002
2022-01-13 11:47:25:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0002
2022-01-13 11:48:41:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0002
2022-01-13 11:49:56:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0002
2022-01-13 11:51:12:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0002
2022-01-13 11:52:28:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0002
2022-01-13 11:53:50:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 11:53:50:INFO:	Num examples = 100
2022-01-13 11:53:50:INFO:	RMSE = 34.7926
2022-01-13 11:53:57:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 11:53:57:INFO:	Num examples = 100
2022-01-13 11:53:57:INFO:	RMSE = 30.7539
2022-01-13 11:53:57:INFO:	Output TEST RMSE:	33.6193
2022-01-13 11:53:57:INFO:	VALID RMSEs:	26.5313	31.7063	27.9264	22.4685	24.7134	26.0841	26.5164	24.6210	24.9056	27.1607	29.2404	30.7539
2022-01-13 11:53:57:INFO:	TEST RMSEs:	33.7433	32.6902	32.6343	33.6193	33.3207	33.1443	35.2370	34.6862	34.8416	35.1669	34.6945	34.7926
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-13 11:54:00:INFO:Finish setting logger...
2022-01-13 11:54:00:INFO:==> Training/Evaluation parameters are:
2022-01-13 11:54:00:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-128'
2022-01-13 11:54:00:INFO:	data_fn=1
2022-01-13 11:54:00:INFO:	datatest_fn=1
2022-01-13 11:54:00:INFO:	filter_kernel_size=1
2022-01-13 11:54:00:INFO:	override_data_cache=False
2022-01-13 11:54:00:INFO:	maxRUL=125
2022-01-13 11:54:00:INFO:	low_ratio=0.1
2022-01-13 11:54:00:INFO:	high_ratio=0.99
2022-01-13 11:54:00:INFO:	aug_ratio=150
2022-01-13 11:54:00:INFO:	noise_amplitude=0.01
2022-01-13 11:54:00:INFO:	modeltype='cnn1d'
2022-01-13 11:54:00:INFO:	max_seq_len=550
2022-01-13 11:54:00:INFO:	d_model=128
2022-01-13 11:54:00:INFO:	p_dropout=0.1
2022-01-13 11:54:00:INFO:	n_head=4
2022-01-13 11:54:00:INFO:	n_layer=2
2022-01-13 11:54:00:INFO:	dim_feedforward=512
2022-01-13 11:54:00:INFO:	e_dropout=0.1
2022-01-13 11:54:00:INFO:	activation='relu'
2022-01-13 11:54:00:INFO:	layer_norm=False
2022-01-13 11:54:00:INFO:	support_size=2
2022-01-13 11:54:00:INFO:	inner_steps=2
2022-01-13 11:54:00:INFO:	lr_inner=0.001
2022-01-13 11:54:00:INFO:	lr_meta=0.001
2022-01-13 11:54:00:INFO:	n_epochs=12
2022-01-13 11:54:00:INFO:	train_batch_size=20
2022-01-13 11:54:00:INFO:	eval_batch_size=1
2022-01-13 11:54:00:INFO:	lr=0.001
2022-01-13 11:54:00:INFO:	weight_decay=0.01
2022-01-13 11:54:00:INFO:	warmup_ratio=0.0
2022-01-13 11:54:00:INFO:	max_grad_norm=5.0
2022-01-13 11:54:00:INFO:	logging_steps=50
2022-01-13 11:54:00:INFO:	seed=128
2022-01-13 11:54:00:INFO:	gpu_id=0
2022-01-13 11:54:00:INFO:	do_train=True
2022-01-13 11:54:00:INFO:	do_eval=False
2022-01-13 11:54:00:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-13 11:54:00:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-13 11:54:00:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-13 11:54:00:INFO:	device=device(type='cuda'))
2022-01-13 11:54:00:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-128...
2022-01-13 11:54:00:INFO:==> Read data from data/train_FD001.txt...
2022-01-13 11:54:00:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 11:54:01:INFO:==> Min_max normalization...
2022-01-13 11:54:01:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 11:54:01:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 11:54:01:INFO:==> Read data from data/test_FD001.txt...
2022-01-13 11:54:01:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 11:54:01:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-13 11:54:01:INFO:	min_rul: 7, max_rul: 145
2022-01-13 11:54:01:INFO:==> Input length ratio of the [TEST] data:
2022-01-13 11:54:01:INFO:	min_ratio = 0.2067
2022-01-13 11:54:01:INFO:	max_ratio = 0.9667
2022-01-13 11:54:01:INFO:==> Min_max normalization...
2022-01-13 11:54:01:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 11:54:01:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 11:54:01:INFO:==> Computing Criterion...
2022-01-13 11:54:01:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-13 11:54:09:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-13 11:54:09:INFO:NumExpr defaulting to 8 threads.
2022-01-13 11:54:10:INFO:=============== Scheme: Meta Learning ===============
2022-01-13 11:54:10:INFO:	Num examples = 15000
2022-01-13 11:54:10:INFO:	Num epochs = 12
2022-01-13 11:54:10:INFO:	Batch size = 20
2022-01-13 11:54:10:INFO:	Total meta optimization steps = 9000
2022-01-13 11:54:10:INFO:	Total inner optimization steps = 18000
2022-01-13 11:54:18:INFO:==> Group parameters for optimization...
2022-01-13 11:54:18:INFO:    Parameters to update are:
2022-01-13 11:54:18:INFO:	conv1.0.weight
2022-01-13 11:54:18:INFO:	conv2.0.weight
2022-01-13 11:54:18:INFO:	conv3.0.weight
2022-01-13 11:54:18:INFO:	conv4.0.weight
2022-01-13 11:54:18:INFO:	conv5.0.weight
2022-01-13 11:54:18:INFO:	fc_1.0.weight
2022-01-13 11:54:18:INFO:	fc_1.0.bias
2022-01-13 11:54:18:INFO:	fc_2.weight
2022-01-13 11:54:18:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-13 11:54:21:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0012
2022-01-13 11:55:36:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0130
2022-01-13 11:56:52:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0066
2022-01-13 11:58:07:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0039
2022-01-13 11:59:22:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0035
2022-01-13 12:00:38:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0033
2022-01-13 12:01:54:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0034
2022-01-13 12:03:10:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0030
2022-01-13 12:04:26:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0027
2022-01-13 12:05:41:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0030
2022-01-13 12:06:58:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0031
2022-01-13 12:08:14:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0028
2022-01-13 12:09:30:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0025
2022-01-13 12:10:46:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0026
2022-01-13 12:12:02:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0027
2022-01-13 12:13:24:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 12:13:24:INFO:	Num examples = 100
2022-01-13 12:13:24:INFO:	RMSE = 33.7433
2022-01-13 12:13:32:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 12:13:32:INFO:	Num examples = 100
2022-01-13 12:13:32:INFO:	RMSE = 26.5313
2022-01-13 12:13:32:INFO:==> Minimal valid RMSE!
2022-01-13 12:13:32:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-128...
2022-01-13 12:13:33:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0021
2022-01-13 12:14:49:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0021
2022-01-13 12:16:05:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0019
2022-01-13 12:17:21:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0019
2022-01-13 12:18:37:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0020
2022-01-13 12:19:52:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0020
2022-01-13 12:21:08:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0018
2022-01-13 12:22:24:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0018
2022-01-13 12:23:41:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0018
2022-01-13 12:24:58:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0018
2022-01-13 12:26:37:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0017
2022-01-13 12:28:39:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0018
2022-01-13 12:30:43:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0015
2022-01-13 12:32:46:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0015
2022-01-13 12:34:52:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0015
2022-01-13 12:37:08:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 12:37:08:INFO:	Num examples = 100
2022-01-13 12:37:08:INFO:	RMSE = 32.6902
2022-01-13 12:37:20:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 12:37:20:INFO:	Num examples = 100
2022-01-13 12:37:20:INFO:	RMSE = 31.7063
2022-01-13 12:37:22:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0015
2022-01-13 12:39:27:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0013
2022-01-13 12:41:32:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0013
2022-01-13 12:43:39:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0012
2022-01-13 12:45:45:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0011
2022-01-13 12:47:51:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0012
2022-01-13 12:49:56:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0012
2022-01-13 12:52:02:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0012
2022-01-13 12:54:07:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0013
2022-01-13 12:56:13:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0011
2022-01-13 12:58:16:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0012
2022-01-13 13:00:42:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0011
2022-01-13 13:03:11:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0012
2022-01-13 13:05:40:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0011
2022-01-13 13:08:05:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0011
2022-01-13 13:10:49:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 13:10:49:INFO:	Num examples = 100
2022-01-13 13:10:49:INFO:	RMSE = 32.6343
2022-01-13 13:11:04:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 13:11:04:INFO:	Num examples = 100
2022-01-13 13:11:04:INFO:	RMSE = 27.9264
2022-01-13 13:11:07:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0009
2022-01-13 13:13:29:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0010
2022-01-13 13:16:00:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0010
2022-01-13 13:18:32:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0009
2022-01-13 13:21:03:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0008
2022-01-13 13:23:25:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0010
2022-01-13 13:25:52:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0011
2022-01-13 13:28:23:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0010
2022-01-13 13:30:53:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0009
2022-01-13 13:33:25:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0009
2022-01-13 13:35:54:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0009
2022-01-13 13:38:26:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0010
2022-01-13 13:40:56:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0008
2022-01-13 13:43:29:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0008
2022-01-13 13:46:02:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 13:48:46:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 13:48:46:INFO:	Num examples = 100
2022-01-13 13:48:46:INFO:	RMSE = 33.6193
2022-01-13 13:49:01:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 13:49:01:INFO:	Num examples = 100
2022-01-13 13:49:01:INFO:	RMSE = 22.4685
2022-01-13 13:49:01:INFO:==> Minimal valid RMSE!
2022-01-13 13:49:01:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-2_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-128...
2022-01-13 13:49:05:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 13:51:35:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0008
2022-01-13 13:54:08:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0008
2022-01-13 13:56:41:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0009
2022-01-13 13:59:14:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0008
2022-01-13 14:01:48:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0008
2022-01-13 14:04:20:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0008
2022-01-13 14:06:53:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0007
2022-01-13 14:09:26:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0008
2022-01-13 14:11:57:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0008
2022-01-13 14:14:30:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0008
2022-01-13 14:17:04:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0007
2022-01-13 14:19:37:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0008
2022-01-13 14:22:11:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0007
2022-01-13 14:24:44:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0008
2022-01-13 14:27:27:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 14:27:27:INFO:	Num examples = 100
2022-01-13 14:27:27:INFO:	RMSE = 33.3207
2022-01-13 14:27:42:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 14:27:42:INFO:	Num examples = 100
2022-01-13 14:27:42:INFO:	RMSE = 24.7134
2022-01-13 14:27:45:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0007
2022-01-13 14:30:17:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0007
2022-01-13 14:32:47:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 14:35:22:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 14:37:53:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0006
2022-01-13 14:40:26:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0006
2022-01-13 14:42:59:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0006
2022-01-13 14:45:34:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0006
2022-01-13 14:48:08:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0006
2022-01-13 14:50:42:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0006
2022-01-13 14:53:17:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0006
2022-01-13 14:55:50:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0006
2022-01-13 14:58:25:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0005
2022-01-13 15:01:01:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0005
2022-01-13 15:03:35:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0005
2022-01-13 15:06:22:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 15:06:22:INFO:	Num examples = 100
2022-01-13 15:06:22:INFO:	RMSE = 33.1443
2022-01-13 15:06:37:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 15:06:37:INFO:	Num examples = 100
2022-01-13 15:06:37:INFO:	RMSE = 26.0841
2022-01-13 15:06:40:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0006
2022-01-13 15:09:14:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0005
2022-01-13 15:11:48:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0005
2022-01-13 15:14:21:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0005
2022-01-13 15:16:55:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0005
2022-01-13 15:19:31:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0005
2022-01-13 15:22:06:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0004
2022-01-13 15:24:37:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0005
2022-01-13 15:27:12:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0004
2022-01-13 15:29:46:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0004
2022-01-13 15:32:20:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0005
2022-01-13 15:34:54:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0004
2022-01-13 15:37:00:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0004
2022-01-13 15:38:17:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0004
2022-01-13 15:39:34:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0004
2022-01-13 15:40:55:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 15:40:55:INFO:	Num examples = 100
2022-01-13 15:40:55:INFO:	RMSE = 35.2370
2022-01-13 15:41:03:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 15:41:03:INFO:	Num examples = 100
2022-01-13 15:41:03:INFO:	RMSE = 26.5164
2022-01-13 15:41:04:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0004
2022-01-13 15:42:20:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0004
2022-01-13 15:43:36:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0004
2022-01-13 15:44:51:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0004
2022-01-13 15:46:07:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0004
2022-01-13 15:47:22:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0004
2022-01-13 15:48:38:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0004
2022-01-13 15:49:55:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0003
2022-01-13 15:51:11:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0004
2022-01-13 15:52:27:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0003
2022-01-13 15:53:43:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0003
2022-01-13 15:54:59:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0004
2022-01-13 15:56:15:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0003
2022-01-13 15:57:30:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0003
2022-01-13 15:58:46:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0003
2022-01-13 16:00:07:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 16:00:07:INFO:	Num examples = 100
2022-01-13 16:00:07:INFO:	RMSE = 34.6862
2022-01-13 16:00:14:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 16:00:14:INFO:	Num examples = 100
2022-01-13 16:00:14:INFO:	RMSE = 24.6210
2022-01-13 16:00:16:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0003
2022-01-13 16:01:32:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0003
2022-01-13 16:02:48:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 16:04:02:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 16:05:19:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0003
2022-01-13 16:06:34:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0002
2022-01-13 16:07:49:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0003
2022-01-13 16:09:05:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0003
2022-01-13 16:10:20:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0003
2022-01-13 16:11:36:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0003
2022-01-13 16:12:52:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0003
2022-01-13 16:14:08:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0003
2022-01-13 16:15:24:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0003
2022-01-13 16:16:40:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0002
2022-01-13 16:17:56:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0003
2022-01-13 16:19:19:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 16:19:19:INFO:	Num examples = 100
2022-01-13 16:19:19:INFO:	RMSE = 34.8416
2022-01-13 16:19:26:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 16:19:26:INFO:	Num examples = 100
2022-01-13 16:19:26:INFO:	RMSE = 24.9056
2022-01-13 16:19:28:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0003
2022-01-13 16:20:45:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0002
2022-01-13 16:22:03:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0003
2022-01-13 16:23:20:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0002
2022-01-13 16:24:36:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0002
2022-01-13 16:25:52:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0002
2022-01-13 16:27:08:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0002
2022-01-13 16:28:24:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-13 16:29:40:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0002
2022-01-13 16:30:57:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0002
2022-01-13 16:32:13:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0002
2022-01-13 16:33:29:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0002
2022-01-13 16:34:44:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0002
2022-01-13 16:36:01:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0002
2022-01-13 16:37:17:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0002
2022-01-13 16:38:38:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 16:38:38:INFO:	Num examples = 100
2022-01-13 16:38:38:INFO:	RMSE = 35.1669
2022-01-13 16:38:46:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 16:38:46:INFO:	Num examples = 100
2022-01-13 16:38:46:INFO:	RMSE = 27.1607
2022-01-13 16:38:47:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0002
2022-01-13 16:40:03:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0002
2022-01-13 16:41:19:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0002
2022-01-13 16:42:34:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-13 16:43:50:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0002
2022-01-13 16:45:05:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0002
2022-01-13 16:46:21:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0002
2022-01-13 16:47:37:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0002
2022-01-13 16:48:52:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 16:50:08:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 16:51:23:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0002
2022-01-13 16:52:39:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0002
2022-01-13 16:53:55:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0002
2022-01-13 16:55:10:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 16:56:26:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 16:57:49:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 16:57:49:INFO:	Num examples = 100
2022-01-13 16:57:49:INFO:	RMSE = 34.6945
2022-01-13 16:57:56:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 16:57:56:INFO:	Num examples = 100
2022-01-13 16:57:56:INFO:	RMSE = 29.2404
2022-01-13 16:57:58:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0002
2022-01-13 16:59:14:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0002
2022-01-13 17:00:29:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0002
2022-01-13 17:01:45:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0002
2022-01-13 17:03:01:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0001
2022-01-13 17:04:17:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0002
2022-01-13 17:05:32:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0002
2022-01-13 17:06:47:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0002
2022-01-13 17:08:03:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0002
2022-01-13 17:09:18:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0002
2022-01-13 17:10:33:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0002
2022-01-13 17:11:49:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0002
2022-01-13 17:13:05:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0002
2022-01-13 17:14:22:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0002
2022-01-13 17:15:38:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0002
2022-01-13 17:16:59:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 17:16:59:INFO:	Num examples = 100
2022-01-13 17:16:59:INFO:	RMSE = 34.7926
2022-01-13 17:17:06:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 17:17:06:INFO:	Num examples = 100
2022-01-13 17:17:06:INFO:	RMSE = 30.7539
2022-01-13 17:17:06:INFO:	Output TEST RMSE:	33.6193
2022-01-13 17:17:06:INFO:	VALID RMSEs:	26.5313	31.7063	27.9264	22.4685	24.7134	26.0841	26.5164	24.6210	24.9056	27.1607	29.2404	30.7539
2022-01-13 17:17:06:INFO:	TEST RMSEs:	33.7433	32.6902	32.6343	33.6193	33.3207	33.1443	35.2370	34.6862	34.8416	35.1669	34.6945	34.7926
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-13 17:17:10:INFO:Finish setting logger...
2022-01-13 17:17:10:INFO:==> Training/Evaluation parameters are:
2022-01-13 17:17:10:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128'
2022-01-13 17:17:10:INFO:	data_fn=1
2022-01-13 17:17:10:INFO:	datatest_fn=1
2022-01-13 17:17:10:INFO:	filter_kernel_size=1
2022-01-13 17:17:10:INFO:	override_data_cache=False
2022-01-13 17:17:10:INFO:	maxRUL=125
2022-01-13 17:17:10:INFO:	low_ratio=0.1
2022-01-13 17:17:10:INFO:	high_ratio=0.99
2022-01-13 17:17:10:INFO:	aug_ratio=150
2022-01-13 17:17:10:INFO:	noise_amplitude=0.01
2022-01-13 17:17:10:INFO:	modeltype='cnn1d'
2022-01-13 17:17:10:INFO:	max_seq_len=550
2022-01-13 17:17:10:INFO:	d_model=128
2022-01-13 17:17:10:INFO:	p_dropout=0.1
2022-01-13 17:17:10:INFO:	n_head=4
2022-01-13 17:17:10:INFO:	n_layer=2
2022-01-13 17:17:10:INFO:	dim_feedforward=512
2022-01-13 17:17:10:INFO:	e_dropout=0.1
2022-01-13 17:17:10:INFO:	activation='relu'
2022-01-13 17:17:10:INFO:	layer_norm=False
2022-01-13 17:17:10:INFO:	support_size=5
2022-01-13 17:17:10:INFO:	inner_steps=2
2022-01-13 17:17:10:INFO:	lr_inner=0.0001
2022-01-13 17:17:10:INFO:	lr_meta=0.001
2022-01-13 17:17:10:INFO:	n_epochs=12
2022-01-13 17:17:10:INFO:	train_batch_size=20
2022-01-13 17:17:10:INFO:	eval_batch_size=1
2022-01-13 17:17:10:INFO:	lr=0.001
2022-01-13 17:17:10:INFO:	weight_decay=0.01
2022-01-13 17:17:10:INFO:	warmup_ratio=0.0
2022-01-13 17:17:10:INFO:	max_grad_norm=5.0
2022-01-13 17:17:10:INFO:	logging_steps=50
2022-01-13 17:17:10:INFO:	seed=128
2022-01-13 17:17:10:INFO:	gpu_id=0
2022-01-13 17:17:10:INFO:	do_train=True
2022-01-13 17:17:10:INFO:	do_eval=False
2022-01-13 17:17:10:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-13 17:17:10:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-13 17:17:10:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-13 17:17:10:INFO:	device=device(type='cuda'))
2022-01-13 17:17:10:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-13 17:17:10:INFO:==> Read data from data/train_FD001.txt...
2022-01-13 17:17:10:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 17:17:10:INFO:==> Min_max normalization...
2022-01-13 17:17:10:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 17:17:10:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 17:17:10:INFO:==> Read data from data/test_FD001.txt...
2022-01-13 17:17:10:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 17:17:10:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-13 17:17:10:INFO:	min_rul: 7, max_rul: 145
2022-01-13 17:17:10:INFO:==> Input length ratio of the [TEST] data:
2022-01-13 17:17:10:INFO:	min_ratio = 0.2067
2022-01-13 17:17:10:INFO:	max_ratio = 0.9667
2022-01-13 17:17:10:INFO:==> Min_max normalization...
2022-01-13 17:17:10:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 17:17:10:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 17:17:10:INFO:==> Computing Criterion...
2022-01-13 17:17:10:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-13 17:17:23:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-13 17:17:23:INFO:NumExpr defaulting to 8 threads.
2022-01-13 17:17:23:INFO:=============== Scheme: Meta Learning ===============
2022-01-13 17:17:23:INFO:	Num examples = 15000
2022-01-13 17:17:23:INFO:	Num epochs = 12
2022-01-13 17:17:23:INFO:	Batch size = 20
2022-01-13 17:17:23:INFO:	Total meta optimization steps = 9000
2022-01-13 17:17:23:INFO:	Total inner optimization steps = 18000
2022-01-13 17:17:31:INFO:==> Group parameters for optimization...
2022-01-13 17:17:31:INFO:    Parameters to update are:
2022-01-13 17:17:31:INFO:	conv1.0.weight
2022-01-13 17:17:31:INFO:	conv2.0.weight
2022-01-13 17:17:31:INFO:	conv3.0.weight
2022-01-13 17:17:31:INFO:	conv4.0.weight
2022-01-13 17:17:31:INFO:	conv5.0.weight
2022-01-13 17:17:31:INFO:	fc_1.0.weight
2022-01-13 17:17:31:INFO:	fc_1.0.bias
2022-01-13 17:17:31:INFO:	fc_2.weight
2022-01-13 17:17:31:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-13 17:17:35:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0012
2022-01-13 17:18:50:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0139
2022-01-13 17:20:06:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0072
2022-01-13 17:21:21:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0052
2022-01-13 17:22:38:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0035
2022-01-13 17:23:54:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0034
2022-01-13 17:25:10:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0035
2022-01-13 17:26:27:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0032
2022-01-13 17:27:43:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0028
2022-01-13 17:28:59:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0030
2022-01-13 17:30:15:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0033
2022-01-13 17:31:30:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0030
2022-01-13 17:32:45:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0028
2022-01-13 17:34:00:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0027
2022-01-13 17:35:14:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0029
2022-01-13 17:36:35:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 17:36:35:INFO:	Num examples = 100
2022-01-13 17:36:35:INFO:	RMSE = 31.4966
2022-01-13 17:36:43:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 17:36:43:INFO:	Num examples = 100
2022-01-13 17:36:43:INFO:	RMSE = 28.9535
2022-01-13 17:36:43:INFO:==> Minimal valid RMSE!
2022-01-13 17:36:43:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-13 17:36:44:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0021
2022-01-13 17:37:59:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0029
2022-01-13 17:39:14:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0022
2022-01-13 17:40:30:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0022
2022-01-13 17:41:45:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0023
2022-01-13 17:43:01:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0023
2022-01-13 17:44:16:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0019
2022-01-13 17:45:32:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0019
2022-01-13 17:46:46:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0021
2022-01-13 17:48:02:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0020
2022-01-13 17:49:16:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0017
2022-01-13 17:50:31:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0018
2022-01-13 17:51:47:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0018
2022-01-13 17:53:03:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0016
2022-01-13 17:54:19:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0016
2022-01-13 17:55:41:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 17:55:41:INFO:	Num examples = 100
2022-01-13 17:55:41:INFO:	RMSE = 30.8395
2022-01-13 17:55:48:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 17:55:48:INFO:	Num examples = 100
2022-01-13 17:55:48:INFO:	RMSE = 32.3615
2022-01-13 17:55:50:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0015
2022-01-13 17:57:05:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0015
2022-01-13 17:58:20:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0013
2022-01-13 17:59:35:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0013
2022-01-13 18:00:49:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0011
2022-01-13 18:02:04:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0013
2022-01-13 18:03:20:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0013
2022-01-13 18:04:35:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0013
2022-01-13 18:05:50:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0014
2022-01-13 18:07:05:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0011
2022-01-13 18:08:20:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0013
2022-01-13 18:09:34:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0012
2022-01-13 18:10:49:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0013
2022-01-13 18:12:04:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0012
2022-01-13 18:13:18:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0011
2022-01-13 18:14:39:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 18:14:39:INFO:	Num examples = 100
2022-01-13 18:14:39:INFO:	RMSE = 31.7485
2022-01-13 18:14:46:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 18:14:46:INFO:	Num examples = 100
2022-01-13 18:14:46:INFO:	RMSE = 28.9199
2022-01-13 18:14:46:INFO:==> Minimal valid RMSE!
2022-01-13 18:14:46:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-13 18:14:48:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0011
2022-01-13 18:16:02:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0011
2022-01-13 18:17:18:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0011
2022-01-13 18:18:33:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0010
2022-01-13 18:19:48:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0009
2022-01-13 18:21:04:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0011
2022-01-13 18:22:18:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0011
2022-01-13 18:23:34:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0011
2022-01-13 18:24:50:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0010
2022-01-13 18:26:06:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0010
2022-01-13 18:27:21:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0010
2022-01-13 18:28:36:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0010
2022-01-13 18:29:52:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0008
2022-01-13 18:31:07:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0010
2022-01-13 18:32:24:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 18:33:45:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 18:33:45:INFO:	Num examples = 100
2022-01-13 18:33:45:INFO:	RMSE = 34.4198
2022-01-13 18:33:53:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 18:33:53:INFO:	Num examples = 100
2022-01-13 18:33:53:INFO:	RMSE = 24.6706
2022-01-13 18:33:53:INFO:==> Minimal valid RMSE!
2022-01-13 18:33:53:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-13 18:33:54:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 18:35:09:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0009
2022-01-13 18:36:25:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0008
2022-01-13 18:37:42:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0009
2022-01-13 18:38:58:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0008
2022-01-13 18:40:13:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0008
2022-01-13 18:41:29:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0009
2022-01-13 18:42:45:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0008
2022-01-13 18:44:01:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0009
2022-01-13 18:45:15:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0009
2022-01-13 18:46:31:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0009
2022-01-13 18:47:48:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0008
2022-01-13 18:49:04:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0008
2022-01-13 18:50:20:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0008
2022-01-13 18:51:37:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0008
2022-01-13 18:52:57:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 18:52:57:INFO:	Num examples = 100
2022-01-13 18:52:57:INFO:	RMSE = 33.8437
2022-01-13 18:53:05:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 18:53:05:INFO:	Num examples = 100
2022-01-13 18:53:05:INFO:	RMSE = 24.4790
2022-01-13 18:53:05:INFO:==> Minimal valid RMSE!
2022-01-13 18:53:05:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.0001_warmUp-0.0_seed-128...
2022-01-13 18:53:06:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0007
2022-01-13 18:54:23:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0007
2022-01-13 18:55:37:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 18:56:53:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 18:58:09:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0007
2022-01-13 18:59:25:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0007
2022-01-13 19:00:41:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0007
2022-01-13 19:01:57:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0006
2022-01-13 19:03:12:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0007
2022-01-13 19:04:28:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0007
2022-01-13 19:05:44:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0006
2022-01-13 19:06:59:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0007
2022-01-13 19:08:16:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0006
2022-01-13 19:09:31:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0006
2022-01-13 19:10:47:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0006
2022-01-13 19:12:10:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 19:12:10:INFO:	Num examples = 100
2022-01-13 19:12:10:INFO:	RMSE = 32.8354
2022-01-13 19:12:18:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 19:12:18:INFO:	Num examples = 100
2022-01-13 19:12:18:INFO:	RMSE = 26.3607
2022-01-13 19:12:19:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0006
2022-01-13 19:13:35:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0006
2022-01-13 19:14:52:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0006
2022-01-13 19:16:09:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0005
2022-01-13 19:17:25:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0005
2022-01-13 19:18:40:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0006
2022-01-13 19:19:56:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0005
2022-01-13 19:21:12:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0005
2022-01-13 19:22:28:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0005
2022-01-13 19:23:43:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0005
2022-01-13 19:24:58:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0005
2022-01-13 19:26:15:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0005
2022-01-13 19:27:32:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0005
2022-01-13 19:28:49:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0005
2022-01-13 19:30:06:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0005
2022-01-13 19:31:29:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 19:31:29:INFO:	Num examples = 100
2022-01-13 19:31:29:INFO:	RMSE = 34.7453
2022-01-13 19:31:36:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 19:31:36:INFO:	Num examples = 100
2022-01-13 19:31:36:INFO:	RMSE = 26.4906
2022-01-13 19:31:38:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0005
2022-01-13 19:32:53:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0005
2022-01-13 19:34:08:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0004
2022-01-13 19:35:23:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0005
2022-01-13 19:36:39:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0005
2022-01-13 19:37:55:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0004
2022-01-13 19:39:09:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0004
2022-01-13 19:40:25:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0004
2022-01-13 19:41:39:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0005
2022-01-13 19:42:54:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0004
2022-01-13 19:44:09:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0004
2022-01-13 19:45:24:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0004
2022-01-13 19:46:40:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0004
2022-01-13 19:47:55:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0004
2022-01-13 19:49:10:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0004
2022-01-13 19:50:32:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 19:50:32:INFO:	Num examples = 100
2022-01-13 19:50:32:INFO:	RMSE = 34.4220
2022-01-13 19:50:39:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 19:50:39:INFO:	Num examples = 100
2022-01-13 19:50:39:INFO:	RMSE = 26.7755
2022-01-13 19:50:41:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0004
2022-01-13 19:51:56:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0004
2022-01-13 19:53:11:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 19:54:26:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 19:55:42:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0003
2022-01-13 19:56:57:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0003
2022-01-13 19:58:12:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0004
2022-01-13 19:59:27:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0003
2022-01-13 20:00:42:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0004
2022-01-13 20:01:57:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0003
2022-01-13 20:03:12:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0004
2022-01-13 20:04:27:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0003
2022-01-13 20:05:42:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0003
2022-01-13 20:06:56:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0003
2022-01-13 20:08:12:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0003
2022-01-13 20:09:32:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 20:09:32:INFO:	Num examples = 100
2022-01-13 20:09:32:INFO:	RMSE = 34.6341
2022-01-13 20:09:40:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 20:09:40:INFO:	Num examples = 100
2022-01-13 20:09:40:INFO:	RMSE = 24.9412
2022-01-13 20:09:41:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0003
2022-01-13 20:10:56:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0003
2022-01-13 20:12:11:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0003
2022-01-13 20:13:27:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0003
2022-01-13 20:14:42:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0003
2022-01-13 20:15:57:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0003
2022-01-13 20:17:13:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0003
2022-01-13 20:18:28:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-13 20:19:44:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0003
2022-01-13 20:20:59:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0003
2022-01-13 20:22:15:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0003
2022-01-13 20:23:30:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0003
2022-01-13 20:24:45:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0003
2022-01-13 20:26:01:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0003
2022-01-13 20:27:16:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0003
2022-01-13 20:28:38:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 20:28:38:INFO:	Num examples = 100
2022-01-13 20:28:38:INFO:	RMSE = 34.4942
2022-01-13 20:28:45:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 20:28:45:INFO:	Num examples = 100
2022-01-13 20:28:45:INFO:	RMSE = 27.3982
2022-01-13 20:28:46:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0003
2022-01-13 20:30:02:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0003
2022-01-13 20:31:17:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0002
2022-01-13 20:32:33:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-13 20:33:49:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0002
2022-01-13 20:35:06:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0003
2022-01-13 20:36:22:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0003
2022-01-13 20:37:38:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0002
2022-01-13 20:38:54:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 20:40:09:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0002
2022-01-13 20:41:25:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0002
2022-01-13 20:42:40:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0002
2022-01-13 20:43:55:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0002
2022-01-13 20:45:10:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 20:46:26:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0002
2022-01-13 20:47:47:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 20:47:47:INFO:	Num examples = 100
2022-01-13 20:47:47:INFO:	RMSE = 34.2181
2022-01-13 20:47:55:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 20:47:55:INFO:	Num examples = 100
2022-01-13 20:47:55:INFO:	RMSE = 28.5587
2022-01-13 20:47:56:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0002
2022-01-13 20:49:12:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0002
2022-01-13 20:50:28:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0002
2022-01-13 20:51:43:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0002
2022-01-13 20:52:58:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0002
2022-01-13 20:54:14:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0002
2022-01-13 20:55:29:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0002
2022-01-13 20:56:45:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0002
2022-01-13 20:58:00:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0002
2022-01-13 20:59:16:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0002
2022-01-13 21:00:32:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0002
2022-01-13 21:01:47:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0002
2022-01-13 21:03:02:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0002
2022-01-13 21:04:17:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0002
2022-01-13 21:05:33:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0002
2022-01-13 21:06:54:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 21:06:54:INFO:	Num examples = 100
2022-01-13 21:06:54:INFO:	RMSE = 34.4434
2022-01-13 21:07:02:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 21:07:02:INFO:	Num examples = 100
2022-01-13 21:07:02:INFO:	RMSE = 30.6725
2022-01-13 21:07:02:INFO:	Output TEST RMSE:	33.8437
2022-01-13 21:07:02:INFO:	VALID RMSEs:	28.9535	32.3615	28.9199	24.6706	24.4790	26.3607	26.4906	26.7755	24.9412	27.3982	28.5587	30.6725
2022-01-13 21:07:02:INFO:	TEST RMSEs:	31.4966	30.8395	31.7485	34.4198	33.8437	32.8354	34.7453	34.4220	34.6341	34.4942	34.2181	34.4434
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
2022-01-13 21:07:06:INFO:Finish setting logger...
2022-01-13 21:07:06:INFO:==> Training/Evaluation parameters are:
2022-01-13 21:07:06:INFO:	Namespace(model_dir='cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-128'
2022-01-13 21:07:06:INFO:	data_fn=1
2022-01-13 21:07:06:INFO:	datatest_fn=1
2022-01-13 21:07:06:INFO:	filter_kernel_size=1
2022-01-13 21:07:06:INFO:	override_data_cache=False
2022-01-13 21:07:06:INFO:	maxRUL=125
2022-01-13 21:07:06:INFO:	low_ratio=0.1
2022-01-13 21:07:06:INFO:	high_ratio=0.99
2022-01-13 21:07:06:INFO:	aug_ratio=150
2022-01-13 21:07:06:INFO:	noise_amplitude=0.01
2022-01-13 21:07:06:INFO:	modeltype='cnn1d'
2022-01-13 21:07:06:INFO:	max_seq_len=550
2022-01-13 21:07:06:INFO:	d_model=128
2022-01-13 21:07:06:INFO:	p_dropout=0.1
2022-01-13 21:07:06:INFO:	n_head=4
2022-01-13 21:07:06:INFO:	n_layer=2
2022-01-13 21:07:06:INFO:	dim_feedforward=512
2022-01-13 21:07:06:INFO:	e_dropout=0.1
2022-01-13 21:07:06:INFO:	activation='relu'
2022-01-13 21:07:06:INFO:	layer_norm=False
2022-01-13 21:07:06:INFO:	support_size=5
2022-01-13 21:07:06:INFO:	inner_steps=2
2022-01-13 21:07:06:INFO:	lr_inner=0.001
2022-01-13 21:07:06:INFO:	lr_meta=0.001
2022-01-13 21:07:06:INFO:	n_epochs=12
2022-01-13 21:07:06:INFO:	train_batch_size=20
2022-01-13 21:07:06:INFO:	eval_batch_size=1
2022-01-13 21:07:06:INFO:	lr=0.001
2022-01-13 21:07:06:INFO:	weight_decay=0.01
2022-01-13 21:07:06:INFO:	warmup_ratio=0.0
2022-01-13 21:07:06:INFO:	max_grad_norm=5.0
2022-01-13 21:07:06:INFO:	logging_steps=50
2022-01-13 21:07:06:INFO:	seed=128
2022-01-13 21:07:06:INFO:	gpu_id=0
2022-01-13 21:07:06:INFO:	do_train=True
2022-01-13 21:07:06:INFO:	do_eval=False
2022-01-13 21:07:06:INFO:	train_data_fn='data/train_FD001.txt'
2022-01-13 21:07:06:INFO:	test_data_fn='data/test_FD001.txt'
2022-01-13 21:07:06:INFO:	target_ruls_fn='data/RUL_FD001.txt'
2022-01-13 21:07:06:INFO:	device=device(type='cuda'))
2022-01-13 21:07:06:INFO:Dump arguments to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-128...
2022-01-13 21:07:06:INFO:==> Read data from data/train_FD001.txt...
2022-01-13 21:07:06:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 21:07:06:INFO:==> Min_max normalization...
2022-01-13 21:07:06:INFO:	The min value is [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 21:07:06:INFO:	The max value is [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 21:07:06:INFO:==> Read data from data/test_FD001.txt...
2022-01-13 21:07:06:INFO:	The selected feature idxs are: 0, 1, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 17, 19, 22, 23
2022-01-13 21:07:06:INFO:==> Read RULsfrom data/RUL_FD001.txt...
2022-01-13 21:07:06:INFO:	min_rul: 7, max_rul: 145
2022-01-13 21:07:06:INFO:==> Input length ratio of the [TEST] data:
2022-01-13 21:07:06:INFO:	min_ratio = 0.2067
2022-01-13 21:07:06:INFO:	max_ratio = 0.9667
2022-01-13 21:07:06:INFO:==> Min_max normalization...
2022-01-13 21:07:06:INFO:	With given min value [-0.008700000122189522, -0.0006000000284984708, 641.2100219726562, 1571.0400390625, 1382.25, 549.8499755859375, 2387.89990234375, 9021.73046875, 46.849998474121094, 518.6900024414062, 2387.8798828125, 8099.93994140625, 8.324899673461914, 388.0, 38.13999938964844, 22.89419937133789]
2022-01-13 21:07:06:INFO:	With given max value [0.008700000122189522, 0.0006000000284984708, 644.530029296875, 1616.9100341796875, 1441.489990234375, 556.0599975585938, 2388.56005859375, 9244.58984375, 48.529998779296875, 523.3800048828125, 2388.56005859375, 8293.7197265625, 8.584799766540527, 400.0, 39.43000030517578, 23.61840057373047]
2022-01-13 21:07:06:INFO:==> Computing Criterion...
2022-01-13 21:07:07:INFO:	The weights are: 0.007887763902544975, 0.008001004345715046, 0.06667434424161911, 0.0634712353348732, 0.07656104862689972, 0.0755249634385109, 0.06726357340812683, 0.0644979178905487, 0.0795108750462532, 0.07743842899799347, 0.0671684592962265, 0.06869389116764069, 0.07147877663373947, 0.06516212970018387, 0.07012488692998886, 0.07054071873426437
2022-01-13 21:07:19:INFO:Note: NumExpr detected 40 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2022-01-13 21:07:19:INFO:NumExpr defaulting to 8 threads.
2022-01-13 21:07:20:INFO:=============== Scheme: Meta Learning ===============
2022-01-13 21:07:20:INFO:	Num examples = 15000
2022-01-13 21:07:20:INFO:	Num epochs = 12
2022-01-13 21:07:20:INFO:	Batch size = 20
2022-01-13 21:07:20:INFO:	Total meta optimization steps = 9000
2022-01-13 21:07:20:INFO:	Total inner optimization steps = 18000
2022-01-13 21:07:28:INFO:==> Group parameters for optimization...
2022-01-13 21:07:28:INFO:    Parameters to update are:
2022-01-13 21:07:28:INFO:	conv1.0.weight
2022-01-13 21:07:28:INFO:	conv2.0.weight
2022-01-13 21:07:28:INFO:	conv3.0.weight
2022-01-13 21:07:28:INFO:	conv4.0.weight
2022-01-13 21:07:28:INFO:	conv5.0.weight
2022-01-13 21:07:28:INFO:	fc_1.0.weight
2022-01-13 21:07:28:INFO:	fc_1.0.bias
2022-01-13 21:07:28:INFO:	fc_2.weight
2022-01-13 21:07:28:INFO:	fc_2.bias
/data/moy20/Meta-Learning/Meta-prognosis-main/optimizer.py:78: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/python_arg_parser.cpp:1025.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/data/moy20/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2022-01-13 21:07:31:INFO:Epoch: 0	 global_step: 0/9000	 lr: 0.00100	 loss: 0.0012
2022-01-13 21:08:46:INFO:Epoch: 0	 global_step: 50/9000	 lr: 0.00099	 loss: 0.0139
2022-01-13 21:10:02:INFO:Epoch: 0	 global_step: 100/9000	 lr: 0.00099	 loss: 0.0072
2022-01-13 21:11:17:INFO:Epoch: 0	 global_step: 150/9000	 lr: 0.00098	 loss: 0.0052
2022-01-13 21:12:33:INFO:Epoch: 0	 global_step: 200/9000	 lr: 0.00098	 loss: 0.0035
2022-01-13 21:13:48:INFO:Epoch: 0	 global_step: 250/9000	 lr: 0.00097	 loss: 0.0034
2022-01-13 21:15:03:INFO:Epoch: 0	 global_step: 300/9000	 lr: 0.00097	 loss: 0.0035
2022-01-13 21:16:19:INFO:Epoch: 0	 global_step: 350/9000	 lr: 0.00096	 loss: 0.0032
2022-01-13 21:17:34:INFO:Epoch: 0	 global_step: 400/9000	 lr: 0.00096	 loss: 0.0028
2022-01-13 21:18:49:INFO:Epoch: 0	 global_step: 450/9000	 lr: 0.00095	 loss: 0.0030
2022-01-13 21:20:05:INFO:Epoch: 0	 global_step: 500/9000	 lr: 0.00094	 loss: 0.0033
2022-01-13 21:21:20:INFO:Epoch: 0	 global_step: 550/9000	 lr: 0.00094	 loss: 0.0030
2022-01-13 21:22:36:INFO:Epoch: 0	 global_step: 600/9000	 lr: 0.00093	 loss: 0.0028
2022-01-13 21:23:51:INFO:Epoch: 0	 global_step: 650/9000	 lr: 0.00093	 loss: 0.0027
2022-01-13 21:25:07:INFO:Epoch: 0	 global_step: 700/9000	 lr: 0.00092	 loss: 0.0029
2022-01-13 21:26:28:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 21:26:28:INFO:	Num examples = 100
2022-01-13 21:26:28:INFO:	RMSE = 31.4966
2022-01-13 21:26:35:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 21:26:35:INFO:	Num examples = 100
2022-01-13 21:26:35:INFO:	RMSE = 28.9535
2022-01-13 21:26:35:INFO:==> Minimal valid RMSE!
2022-01-13 21:26:35:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-128...
2022-01-13 21:26:37:INFO:Epoch: 1	 global_step: 750/9000	 lr: 0.00092	 loss: 0.0021
2022-01-13 21:27:52:INFO:Epoch: 1	 global_step: 800/9000	 lr: 0.00091	 loss: 0.0029
2022-01-13 21:29:07:INFO:Epoch: 1	 global_step: 850/9000	 lr: 0.00091	 loss: 0.0022
2022-01-13 21:30:23:INFO:Epoch: 1	 global_step: 900/9000	 lr: 0.00090	 loss: 0.0022
2022-01-13 21:31:38:INFO:Epoch: 1	 global_step: 950/9000	 lr: 0.00089	 loss: 0.0023
2022-01-13 21:32:53:INFO:Epoch: 1	 global_step: 1000/9000	 lr: 0.00089	 loss: 0.0023
2022-01-13 21:34:09:INFO:Epoch: 1	 global_step: 1050/9000	 lr: 0.00088	 loss: 0.0019
2022-01-13 21:35:24:INFO:Epoch: 1	 global_step: 1100/9000	 lr: 0.00088	 loss: 0.0019
2022-01-13 21:36:40:INFO:Epoch: 1	 global_step: 1150/9000	 lr: 0.00087	 loss: 0.0021
2022-01-13 21:37:55:INFO:Epoch: 1	 global_step: 1200/9000	 lr: 0.00087	 loss: 0.0020
2022-01-13 21:39:10:INFO:Epoch: 1	 global_step: 1250/9000	 lr: 0.00086	 loss: 0.0017
2022-01-13 21:40:25:INFO:Epoch: 1	 global_step: 1300/9000	 lr: 0.00086	 loss: 0.0018
2022-01-13 21:41:41:INFO:Epoch: 1	 global_step: 1350/9000	 lr: 0.00085	 loss: 0.0018
2022-01-13 21:42:57:INFO:Epoch: 1	 global_step: 1400/9000	 lr: 0.00084	 loss: 0.0016
2022-01-13 21:44:12:INFO:Epoch: 1	 global_step: 1450/9000	 lr: 0.00084	 loss: 0.0016
2022-01-13 21:45:33:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 21:45:33:INFO:	Num examples = 100
2022-01-13 21:45:33:INFO:	RMSE = 30.8395
2022-01-13 21:45:41:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 21:45:41:INFO:	Num examples = 100
2022-01-13 21:45:41:INFO:	RMSE = 32.3615
2022-01-13 21:45:42:INFO:Epoch: 2	 global_step: 1500/9000	 lr: 0.00083	 loss: 0.0015
2022-01-13 21:46:58:INFO:Epoch: 2	 global_step: 1550/9000	 lr: 0.00083	 loss: 0.0015
2022-01-13 21:48:13:INFO:Epoch: 2	 global_step: 1600/9000	 lr: 0.00082	 loss: 0.0013
2022-01-13 21:49:28:INFO:Epoch: 2	 global_step: 1650/9000	 lr: 0.00082	 loss: 0.0013
2022-01-13 21:50:43:INFO:Epoch: 2	 global_step: 1700/9000	 lr: 0.00081	 loss: 0.0011
2022-01-13 21:51:59:INFO:Epoch: 2	 global_step: 1750/9000	 lr: 0.00081	 loss: 0.0013
2022-01-13 21:53:14:INFO:Epoch: 2	 global_step: 1800/9000	 lr: 0.00080	 loss: 0.0013
2022-01-13 21:54:29:INFO:Epoch: 2	 global_step: 1850/9000	 lr: 0.00079	 loss: 0.0013
2022-01-13 21:55:44:INFO:Epoch: 2	 global_step: 1900/9000	 lr: 0.00079	 loss: 0.0014
2022-01-13 21:56:59:INFO:Epoch: 2	 global_step: 1950/9000	 lr: 0.00078	 loss: 0.0011
2022-01-13 21:58:14:INFO:Epoch: 2	 global_step: 2000/9000	 lr: 0.00078	 loss: 0.0013
2022-01-13 21:59:29:INFO:Epoch: 2	 global_step: 2050/9000	 lr: 0.00077	 loss: 0.0012
2022-01-13 22:00:44:INFO:Epoch: 2	 global_step: 2100/9000	 lr: 0.00077	 loss: 0.0013
2022-01-13 22:02:00:INFO:Epoch: 2	 global_step: 2150/9000	 lr: 0.00076	 loss: 0.0012
2022-01-13 22:03:16:INFO:Epoch: 2	 global_step: 2200/9000	 lr: 0.00076	 loss: 0.0011
2022-01-13 22:04:37:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 22:04:37:INFO:	Num examples = 100
2022-01-13 22:04:37:INFO:	RMSE = 31.7485
2022-01-13 22:04:45:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 22:04:45:INFO:	Num examples = 100
2022-01-13 22:04:45:INFO:	RMSE = 28.9199
2022-01-13 22:04:45:INFO:==> Minimal valid RMSE!
2022-01-13 22:04:45:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-128...
2022-01-13 22:04:46:INFO:Epoch: 3	 global_step: 2250/9000	 lr: 0.00075	 loss: 0.0011
2022-01-13 22:06:01:INFO:Epoch: 3	 global_step: 2300/9000	 lr: 0.00074	 loss: 0.0011
2022-01-13 22:07:16:INFO:Epoch: 3	 global_step: 2350/9000	 lr: 0.00074	 loss: 0.0011
2022-01-13 22:08:31:INFO:Epoch: 3	 global_step: 2400/9000	 lr: 0.00073	 loss: 0.0010
2022-01-13 22:09:47:INFO:Epoch: 3	 global_step: 2450/9000	 lr: 0.00073	 loss: 0.0009
2022-01-13 22:11:02:INFO:Epoch: 3	 global_step: 2500/9000	 lr: 0.00072	 loss: 0.0011
2022-01-13 22:12:17:INFO:Epoch: 3	 global_step: 2550/9000	 lr: 0.00072	 loss: 0.0011
2022-01-13 22:13:32:INFO:Epoch: 3	 global_step: 2600/9000	 lr: 0.00071	 loss: 0.0011
2022-01-13 22:14:47:INFO:Epoch: 3	 global_step: 2650/9000	 lr: 0.00071	 loss: 0.0010
2022-01-13 22:16:03:INFO:Epoch: 3	 global_step: 2700/9000	 lr: 0.00070	 loss: 0.0010
2022-01-13 22:17:18:INFO:Epoch: 3	 global_step: 2750/9000	 lr: 0.00069	 loss: 0.0010
2022-01-13 22:18:34:INFO:Epoch: 3	 global_step: 2800/9000	 lr: 0.00069	 loss: 0.0010
2022-01-13 22:19:49:INFO:Epoch: 3	 global_step: 2850/9000	 lr: 0.00068	 loss: 0.0008
2022-01-13 22:21:04:INFO:Epoch: 3	 global_step: 2900/9000	 lr: 0.00068	 loss: 0.0010
2022-01-13 22:22:19:INFO:Epoch: 3	 global_step: 2950/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 22:23:40:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 22:23:40:INFO:	Num examples = 100
2022-01-13 22:23:40:INFO:	RMSE = 34.4198
2022-01-13 22:23:48:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 22:23:48:INFO:	Num examples = 100
2022-01-13 22:23:48:INFO:	RMSE = 24.6706
2022-01-13 22:23:48:INFO:==> Minimal valid RMSE!
2022-01-13 22:23:48:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-128...
2022-01-13 22:23:50:INFO:Epoch: 4	 global_step: 3000/9000	 lr: 0.00067	 loss: 0.0009
2022-01-13 22:25:05:INFO:Epoch: 4	 global_step: 3050/9000	 lr: 0.00066	 loss: 0.0009
2022-01-13 22:26:20:INFO:Epoch: 4	 global_step: 3100/9000	 lr: 0.00066	 loss: 0.0008
2022-01-13 22:27:35:INFO:Epoch: 4	 global_step: 3150/9000	 lr: 0.00065	 loss: 0.0009
2022-01-13 22:28:50:INFO:Epoch: 4	 global_step: 3200/9000	 lr: 0.00064	 loss: 0.0008
2022-01-13 22:30:05:INFO:Epoch: 4	 global_step: 3250/9000	 lr: 0.00064	 loss: 0.0008
2022-01-13 22:31:21:INFO:Epoch: 4	 global_step: 3300/9000	 lr: 0.00063	 loss: 0.0009
2022-01-13 22:32:36:INFO:Epoch: 4	 global_step: 3350/9000	 lr: 0.00063	 loss: 0.0008
2022-01-13 22:33:51:INFO:Epoch: 4	 global_step: 3400/9000	 lr: 0.00062	 loss: 0.0009
2022-01-13 22:35:07:INFO:Epoch: 4	 global_step: 3450/9000	 lr: 0.00062	 loss: 0.0009
2022-01-13 22:36:23:INFO:Epoch: 4	 global_step: 3500/9000	 lr: 0.00061	 loss: 0.0009
2022-01-13 22:37:39:INFO:Epoch: 4	 global_step: 3550/9000	 lr: 0.00061	 loss: 0.0008
2022-01-13 22:38:54:INFO:Epoch: 4	 global_step: 3600/9000	 lr: 0.00060	 loss: 0.0008
2022-01-13 22:40:09:INFO:Epoch: 4	 global_step: 3650/9000	 lr: 0.00059	 loss: 0.0008
2022-01-13 22:41:25:INFO:Epoch: 4	 global_step: 3700/9000	 lr: 0.00059	 loss: 0.0008
2022-01-13 22:42:46:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 22:42:46:INFO:	Num examples = 100
2022-01-13 22:42:46:INFO:	RMSE = 33.8437
2022-01-13 22:42:53:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 22:42:53:INFO:	Num examples = 100
2022-01-13 22:42:53:INFO:	RMSE = 24.4790
2022-01-13 22:42:53:INFO:==> Minimal valid RMSE!
2022-01-13 22:42:53:INFO:Save model to cnn1dmodels/data-1_n_epochs-12_aug-150_noise-0.01_supportSize-5_innerSteps-2_lrMeta-0.001_lrInner-0.001_warmUp-0.0_seed-128...
2022-01-13 22:42:55:INFO:Epoch: 5	 global_step: 3750/9000	 lr: 0.00058	 loss: 0.0007
2022-01-13 22:44:11:INFO:Epoch: 5	 global_step: 3800/9000	 lr: 0.00058	 loss: 0.0007
2022-01-13 22:45:27:INFO:Epoch: 5	 global_step: 3850/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 22:46:42:INFO:Epoch: 5	 global_step: 3900/9000	 lr: 0.00057	 loss: 0.0007
2022-01-13 22:47:57:INFO:Epoch: 5	 global_step: 3950/9000	 lr: 0.00056	 loss: 0.0007
2022-01-13 22:49:12:INFO:Epoch: 5	 global_step: 4000/9000	 lr: 0.00056	 loss: 0.0007
2022-01-13 22:50:28:INFO:Epoch: 5	 global_step: 4050/9000	 lr: 0.00055	 loss: 0.0007
2022-01-13 22:51:43:INFO:Epoch: 5	 global_step: 4100/9000	 lr: 0.00054	 loss: 0.0006
2022-01-13 22:52:58:INFO:Epoch: 5	 global_step: 4150/9000	 lr: 0.00054	 loss: 0.0007
2022-01-13 22:54:14:INFO:Epoch: 5	 global_step: 4200/9000	 lr: 0.00053	 loss: 0.0007
2022-01-13 22:55:29:INFO:Epoch: 5	 global_step: 4250/9000	 lr: 0.00053	 loss: 0.0006
2022-01-13 22:56:45:INFO:Epoch: 5	 global_step: 4300/9000	 lr: 0.00052	 loss: 0.0007
2022-01-13 22:58:00:INFO:Epoch: 5	 global_step: 4350/9000	 lr: 0.00052	 loss: 0.0006
2022-01-13 22:59:15:INFO:Epoch: 5	 global_step: 4400/9000	 lr: 0.00051	 loss: 0.0006
2022-01-13 23:00:30:INFO:Epoch: 5	 global_step: 4450/9000	 lr: 0.00051	 loss: 0.0006
2022-01-13 23:01:51:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 23:01:51:INFO:	Num examples = 100
2022-01-13 23:01:51:INFO:	RMSE = 32.8354
2022-01-13 23:01:59:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 23:01:59:INFO:	Num examples = 100
2022-01-13 23:01:59:INFO:	RMSE = 26.3607
2022-01-13 23:02:00:INFO:Epoch: 6	 global_step: 4500/9000	 lr: 0.00050	 loss: 0.0006
2022-01-13 23:03:15:INFO:Epoch: 6	 global_step: 4550/9000	 lr: 0.00049	 loss: 0.0006
2022-01-13 23:04:30:INFO:Epoch: 6	 global_step: 4600/9000	 lr: 0.00049	 loss: 0.0006
2022-01-13 23:05:45:INFO:Epoch: 6	 global_step: 4650/9000	 lr: 0.00048	 loss: 0.0005
2022-01-13 23:07:00:INFO:Epoch: 6	 global_step: 4700/9000	 lr: 0.00048	 loss: 0.0005
2022-01-13 23:08:15:INFO:Epoch: 6	 global_step: 4750/9000	 lr: 0.00047	 loss: 0.0006
2022-01-13 23:09:31:INFO:Epoch: 6	 global_step: 4800/9000	 lr: 0.00047	 loss: 0.0005
2022-01-13 23:10:46:INFO:Epoch: 6	 global_step: 4850/9000	 lr: 0.00046	 loss: 0.0005
2022-01-13 23:12:01:INFO:Epoch: 6	 global_step: 4900/9000	 lr: 0.00046	 loss: 0.0005
2022-01-13 23:13:16:INFO:Epoch: 6	 global_step: 4950/9000	 lr: 0.00045	 loss: 0.0005
2022-01-13 23:14:32:INFO:Epoch: 6	 global_step: 5000/9000	 lr: 0.00044	 loss: 0.0005
2022-01-13 23:15:47:INFO:Epoch: 6	 global_step: 5050/9000	 lr: 0.00044	 loss: 0.0005
2022-01-13 23:17:02:INFO:Epoch: 6	 global_step: 5100/9000	 lr: 0.00043	 loss: 0.0005
2022-01-13 23:18:18:INFO:Epoch: 6	 global_step: 5150/9000	 lr: 0.00043	 loss: 0.0005
2022-01-13 23:19:34:INFO:Epoch: 6	 global_step: 5200/9000	 lr: 0.00042	 loss: 0.0005
2022-01-13 23:20:55:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 23:20:55:INFO:	Num examples = 100
2022-01-13 23:20:55:INFO:	RMSE = 34.7453
2022-01-13 23:21:03:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 23:21:03:INFO:	Num examples = 100
2022-01-13 23:21:03:INFO:	RMSE = 26.4906
2022-01-13 23:21:04:INFO:Epoch: 7	 global_step: 5250/9000	 lr: 0.00042	 loss: 0.0005
2022-01-13 23:22:19:INFO:Epoch: 7	 global_step: 5300/9000	 lr: 0.00041	 loss: 0.0005
2022-01-13 23:23:35:INFO:Epoch: 7	 global_step: 5350/9000	 lr: 0.00041	 loss: 0.0004
2022-01-13 23:24:51:INFO:Epoch: 7	 global_step: 5400/9000	 lr: 0.00040	 loss: 0.0005
2022-01-13 23:26:06:INFO:Epoch: 7	 global_step: 5450/9000	 lr: 0.00039	 loss: 0.0005
2022-01-13 23:27:22:INFO:Epoch: 7	 global_step: 5500/9000	 lr: 0.00039	 loss: 0.0004
2022-01-13 23:28:37:INFO:Epoch: 7	 global_step: 5550/9000	 lr: 0.00038	 loss: 0.0004
2022-01-13 23:29:53:INFO:Epoch: 7	 global_step: 5600/9000	 lr: 0.00038	 loss: 0.0004
2022-01-13 23:31:08:INFO:Epoch: 7	 global_step: 5650/9000	 lr: 0.00037	 loss: 0.0005
2022-01-13 23:32:23:INFO:Epoch: 7	 global_step: 5700/9000	 lr: 0.00037	 loss: 0.0004
2022-01-13 23:33:38:INFO:Epoch: 7	 global_step: 5750/9000	 lr: 0.00036	 loss: 0.0004
2022-01-13 23:34:54:INFO:Epoch: 7	 global_step: 5800/9000	 lr: 0.00036	 loss: 0.0004
2022-01-13 23:36:10:INFO:Epoch: 7	 global_step: 5850/9000	 lr: 0.00035	 loss: 0.0004
2022-01-13 23:37:26:INFO:Epoch: 7	 global_step: 5900/9000	 lr: 0.00034	 loss: 0.0004
2022-01-13 23:38:41:INFO:Epoch: 7	 global_step: 5950/9000	 lr: 0.00034	 loss: 0.0004
2022-01-13 23:40:02:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 23:40:02:INFO:	Num examples = 100
2022-01-13 23:40:02:INFO:	RMSE = 34.4220
2022-01-13 23:40:10:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 23:40:10:INFO:	Num examples = 100
2022-01-13 23:40:10:INFO:	RMSE = 26.7755
2022-01-13 23:40:11:INFO:Epoch: 8	 global_step: 6000/9000	 lr: 0.00033	 loss: 0.0004
2022-01-13 23:41:26:INFO:Epoch: 8	 global_step: 6050/9000	 lr: 0.00033	 loss: 0.0004
2022-01-13 23:42:42:INFO:Epoch: 8	 global_step: 6100/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 23:43:58:INFO:Epoch: 8	 global_step: 6150/9000	 lr: 0.00032	 loss: 0.0003
2022-01-13 23:45:13:INFO:Epoch: 8	 global_step: 6200/9000	 lr: 0.00031	 loss: 0.0003
2022-01-13 23:46:28:INFO:Epoch: 8	 global_step: 6250/9000	 lr: 0.00031	 loss: 0.0003
2022-01-13 23:47:43:INFO:Epoch: 8	 global_step: 6300/9000	 lr: 0.00030	 loss: 0.0004
2022-01-13 23:48:58:INFO:Epoch: 8	 global_step: 6350/9000	 lr: 0.00029	 loss: 0.0003
2022-01-13 23:50:14:INFO:Epoch: 8	 global_step: 6400/9000	 lr: 0.00029	 loss: 0.0004
2022-01-13 23:51:29:INFO:Epoch: 8	 global_step: 6450/9000	 lr: 0.00028	 loss: 0.0003
2022-01-13 23:52:44:INFO:Epoch: 8	 global_step: 6500/9000	 lr: 0.00028	 loss: 0.0004
2022-01-13 23:54:00:INFO:Epoch: 8	 global_step: 6550/9000	 lr: 0.00027	 loss: 0.0003
2022-01-13 23:55:15:INFO:Epoch: 8	 global_step: 6600/9000	 lr: 0.00027	 loss: 0.0003
2022-01-13 23:56:31:INFO:Epoch: 8	 global_step: 6650/9000	 lr: 0.00026	 loss: 0.0003
2022-01-13 23:57:46:INFO:Epoch: 8	 global_step: 6700/9000	 lr: 0.00026	 loss: 0.0003
2022-01-13 23:59:07:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-13 23:59:07:INFO:	Num examples = 100
2022-01-13 23:59:07:INFO:	RMSE = 34.6341
2022-01-13 23:59:15:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-13 23:59:15:INFO:	Num examples = 100
2022-01-13 23:59:15:INFO:	RMSE = 24.9412
2022-01-13 23:59:16:INFO:Epoch: 9	 global_step: 6750/9000	 lr: 0.00025	 loss: 0.0003
2022-01-14 00:00:32:INFO:Epoch: 9	 global_step: 6800/9000	 lr: 0.00024	 loss: 0.0003
2022-01-14 00:01:47:INFO:Epoch: 9	 global_step: 6850/9000	 lr: 0.00024	 loss: 0.0003
2022-01-14 00:03:02:INFO:Epoch: 9	 global_step: 6900/9000	 lr: 0.00023	 loss: 0.0003
2022-01-14 00:04:17:INFO:Epoch: 9	 global_step: 6950/9000	 lr: 0.00023	 loss: 0.0003
2022-01-14 00:05:33:INFO:Epoch: 9	 global_step: 7000/9000	 lr: 0.00022	 loss: 0.0003
2022-01-14 00:06:49:INFO:Epoch: 9	 global_step: 7050/9000	 lr: 0.00022	 loss: 0.0003
2022-01-14 00:08:05:INFO:Epoch: 9	 global_step: 7100/9000	 lr: 0.00021	 loss: 0.0002
2022-01-14 00:09:20:INFO:Epoch: 9	 global_step: 7150/9000	 lr: 0.00021	 loss: 0.0003
2022-01-14 00:10:35:INFO:Epoch: 9	 global_step: 7200/9000	 lr: 0.00020	 loss: 0.0003
2022-01-14 00:11:51:INFO:Epoch: 9	 global_step: 7250/9000	 lr: 0.00019	 loss: 0.0003
2022-01-14 00:13:06:INFO:Epoch: 9	 global_step: 7300/9000	 lr: 0.00019	 loss: 0.0003
2022-01-14 00:14:21:INFO:Epoch: 9	 global_step: 7350/9000	 lr: 0.00018	 loss: 0.0003
2022-01-14 00:15:37:INFO:Epoch: 9	 global_step: 7400/9000	 lr: 0.00018	 loss: 0.0003
2022-01-14 00:16:52:INFO:Epoch: 9	 global_step: 7450/9000	 lr: 0.00017	 loss: 0.0003
2022-01-14 00:18:13:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-14 00:18:13:INFO:	Num examples = 100
2022-01-14 00:18:13:INFO:	RMSE = 34.4942
2022-01-14 00:18:20:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-14 00:18:20:INFO:	Num examples = 100
2022-01-14 00:18:20:INFO:	RMSE = 27.3982
2022-01-14 00:18:22:INFO:Epoch: 10	 global_step: 7500/9000	 lr: 0.00017	 loss: 0.0003
2022-01-14 00:19:37:INFO:Epoch: 10	 global_step: 7550/9000	 lr: 0.00016	 loss: 0.0003
2022-01-14 00:20:53:INFO:Epoch: 10	 global_step: 7600/9000	 lr: 0.00016	 loss: 0.0002
2022-01-14 00:22:07:INFO:Epoch: 10	 global_step: 7650/9000	 lr: 0.00015	 loss: 0.0002
2022-01-14 00:23:23:INFO:Epoch: 10	 global_step: 7700/9000	 lr: 0.00014	 loss: 0.0002
2022-01-14 00:24:39:INFO:Epoch: 10	 global_step: 7750/9000	 lr: 0.00014	 loss: 0.0003
2022-01-14 00:25:54:INFO:Epoch: 10	 global_step: 7800/9000	 lr: 0.00013	 loss: 0.0003
2022-01-14 00:27:09:INFO:Epoch: 10	 global_step: 7850/9000	 lr: 0.00013	 loss: 0.0002
2022-01-14 00:28:24:INFO:Epoch: 10	 global_step: 7900/9000	 lr: 0.00012	 loss: 0.0002
2022-01-14 00:29:39:INFO:Epoch: 10	 global_step: 7950/9000	 lr: 0.00012	 loss: 0.0002
2022-01-14 00:30:55:INFO:Epoch: 10	 global_step: 8000/9000	 lr: 0.00011	 loss: 0.0002
2022-01-14 00:32:10:INFO:Epoch: 10	 global_step: 8050/9000	 lr: 0.00011	 loss: 0.0002
2022-01-14 00:33:26:INFO:Epoch: 10	 global_step: 8100/9000	 lr: 0.00010	 loss: 0.0002
2022-01-14 00:34:41:INFO:Epoch: 10	 global_step: 8150/9000	 lr: 0.00009	 loss: 0.0002
2022-01-14 00:35:56:INFO:Epoch: 10	 global_step: 8200/9000	 lr: 0.00009	 loss: 0.0002
2022-01-14 00:37:17:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-14 00:37:17:INFO:	Num examples = 100
2022-01-14 00:37:17:INFO:	RMSE = 34.2181
2022-01-14 00:37:25:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-14 00:37:25:INFO:	Num examples = 100
2022-01-14 00:37:25:INFO:	RMSE = 28.5587
2022-01-14 00:37:26:INFO:Epoch: 11	 global_step: 8250/9000	 lr: 0.00008	 loss: 0.0002
2022-01-14 00:38:42:INFO:Epoch: 11	 global_step: 8300/9000	 lr: 0.00008	 loss: 0.0002
2022-01-14 00:39:57:INFO:Epoch: 11	 global_step: 8350/9000	 lr: 0.00007	 loss: 0.0002
2022-01-14 00:41:13:INFO:Epoch: 11	 global_step: 8400/9000	 lr: 0.00007	 loss: 0.0002
2022-01-14 00:42:28:INFO:Epoch: 11	 global_step: 8450/9000	 lr: 0.00006	 loss: 0.0002
2022-01-14 00:43:44:INFO:Epoch: 11	 global_step: 8500/9000	 lr: 0.00006	 loss: 0.0002
2022-01-14 00:44:59:INFO:Epoch: 11	 global_step: 8550/9000	 lr: 0.00005	 loss: 0.0002
2022-01-14 00:46:14:INFO:Epoch: 11	 global_step: 8600/9000	 lr: 0.00004	 loss: 0.0002
2022-01-14 00:47:29:INFO:Epoch: 11	 global_step: 8650/9000	 lr: 0.00004	 loss: 0.0002
2022-01-14 00:48:45:INFO:Epoch: 11	 global_step: 8700/9000	 lr: 0.00003	 loss: 0.0002
2022-01-14 00:49:59:INFO:Epoch: 11	 global_step: 8750/9000	 lr: 0.00003	 loss: 0.0002
2022-01-14 00:51:15:INFO:Epoch: 11	 global_step: 8800/9000	 lr: 0.00002	 loss: 0.0002
2022-01-14 00:52:30:INFO:Epoch: 11	 global_step: 8850/9000	 lr: 0.00002	 loss: 0.0002
2022-01-14 00:53:46:INFO:Epoch: 11	 global_step: 8900/9000	 lr: 0.00001	 loss: 0.0002
2022-01-14 00:55:01:INFO:Epoch: 11	 global_step: 8950/9000	 lr: 0.00001	 loss: 0.0002
2022-01-14 00:56:22:INFO:############### Compute RMSEs @ mode [TEST] ###############
2022-01-14 00:56:22:INFO:	Num examples = 100
2022-01-14 00:56:22:INFO:	RMSE = 34.4434
2022-01-14 00:56:29:INFO:############### Compute RMSEs @ mode [VALID] ###############
2022-01-14 00:56:29:INFO:	Num examples = 100
2022-01-14 00:56:29:INFO:	RMSE = 30.6725
2022-01-14 00:56:29:INFO:	Output TEST RMSE:	33.8437
2022-01-14 00:56:29:INFO:	VALID RMSEs:	28.9535	32.3615	28.9199	24.6706	24.4790	26.3607	26.4906	26.7755	24.9412	27.3982	28.5587	30.6725
2022-01-14 00:56:29:INFO:	TEST RMSEs:	31.4966	30.8395	31.7485	34.4198	33.8437	32.8354	34.7453	34.4220	34.6341	34.4942	34.2181	34.4434
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
!!! Reset batch info !!! mode: [TRAIN]
!!! Reset batch info !!! mode: [TEST]
!!! Reset batch info !!! mode: [VALID]
